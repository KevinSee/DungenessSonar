---
title: "Estimates of Steelhead in Dungeness River"
subtitle: "Using Sonar"
author:
  - Kevin See:
      email: Kevin.See@dfw.wa.gov
      institute: [wdfw]
      correspondence: true
  - Bethany Craig:
      email: Bethany.Craig@dfw.wa.gov
      institute: [wdfw]
      correspondence: false
institute:
  - wdfw: Washington Department of Fish & Wildlife
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
    wdfwTemplates::wdfw_html_format2:
      fig_caption: yes
      fig_height: 4
      fig_width: 6
      toc: yes
      toc_depth: 3
      toc_float:
        collapsed: yes
        smooth_scroll: yes
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::pdf_document2:
      fig_caption: yes
      fig_height: 5
      fig_width: 6
      toc: yes
      includes:
        in_header: ../templates/header_WDFW.tex
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks2.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::word_document2:
      fig_caption: yes
      fig_height: 4
      fig_width: 6
      toc: yes
      reference_docx: "../templates/ReportTemplate.docx" # Insert path for the DOCX file
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
bibliography:
  # - AUC.bib
  - references.bib
csl: "../templates/american-fisheries-society.csl" # Insert path for the bib-style
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/",
  dpi = 300
)
```

```{r packages}
# load these packages
library(tidyverse)
library(here)
library(magrittr)
library(janitor)
library(lubridate)
library(readxl)
library(splines)
library(mgcv)
library(ggfortify)
library(ggpubr)
library(scales)
library(kableExtra)

theme_set(theme_bw())

# knitr options
options(knitr.kable.NA = '-')

# when knitting to Word, use this
# what kind of document is being created?
doc.type <- knitr::opts_knit$get('rmarkdown.pandoc.to')

if(doc.type == 'docx') {
  options(knitr.table.format = "pandoc")
}

```


```{r read-data}
sonar_raw <- read_csv(here("analysis/data/raw_data",
                           "2019 sonar.csv")) %>%
  mutate(across(Hour,
                hms)) %>%
  bind_rows(read_csv(here("analysis/data/raw_data",
                          "2020 sonar.csv")) %>%
              mutate(across(Hour,
                            ~ str_pad(.,
                                      width = 5,
                                      side = "left",
                                      pad = "0"))) %>%
              mutate(across(Hour,
                            hm)) %>%
              rename(comments = `Comments/Notes`) %>%
              clean_names("upper_camel")) %>%
  bind_rows(read_csv(here("analysis/data/raw_data",
                          "2021 sonar.csv")) %>%
              mutate(across(Hour,
                            ~ hm(paste(str_sub(., 1,2),
                                       str_sub(., 3, 4),
                                       sep = ":")))) %>%
              rename(comments = `Comments/Notes`) %>%
              clean_names("upper_camel")) %>%
  bind_rows(read_csv(here("analysis/data/raw_data",
                          "2022 sonar.csv")) %>%
              mutate(across(
                Hour,
                ~ str_remove(., ":")
              )) |> 
              mutate(across(
                Hour,
                ~ hm(paste(str_sub(., 1,2),
                           str_sub(., 3,4),
                           sep = ":")))) %>%
              clean_names("upper_camel")) %>%
  filter(!is.na(Year)) %>%
  mutate(across(Date,
                mdy)) %>%
  mutate(across(DataReviewed,
                ~ recode(.,
                         "No Data" = "No data"))) %>%
  mutate(across(DataRecorded,
                ~ recode(.,
                         "No Data" = "No data"))) %>%
  mutate(across(Time,
                ~ recode(.,
                         "No Data" = "No data",
                         "no fish" = "No fish",
                         "No Fish" = "No fish")),
         Time = if_else(str_detect(Comments, "No data"),
                        "No data",
                        Time),
         across(Direction,
                ~ recode(.,
                         "Downstream" = "downstream",
                         "Upstream" = "upstream"))) %>%
  clean_names() %>%
  mutate(date_time = date + hour) %>%
  arrange(date_time) %>%
  # drop any detections after June 1
  filter(month(date) < 6)

# drop a couple records because they don't match
# one upstream and one downstream fish recorded during the 2nd half hour,
# but no data in the first half hour, at all
sonar_raw %<>%
  filter(!(date == ymd(20200322) &
             hour == "30M 0S"))

sonar_raw %<>%
  filter(!(date == ymd(20200301) &
             (str_detect(hour, "^0S") |
                str_detect(hour, "^30M") |
                str_detect(hour, "^1H"))))


# add a couple missing row (assuming no fish detected)
sonar_raw %<>%
  bind_rows(sonar_raw %>%
              filter(date == ymd(20190320),
                     str_detect(hour, "15H")) %>%
              mutate(hour = hour + minutes(30),
                     date_time = date + hour,
                     data_reviewed = "Second 30",
                     across(c(direction),
                            ~ NA),
                     across(c(length:confidence),
                            ~ NA_real_))) %>%
  bind_rows(sonar_raw |> 
  filter(date==ymd(20190420), 
         str_detect(hour, "23H")) |> 
  mutate(hour = hour - minutes(30),
         date_time = date + hour,
         data_reviewed = "First 30")) %>%
  arrange(date_time)



# correct some columns in one row
sonar_raw %>%
  filter(!(date == ymd(20200218) &
             data_recorded == "No data" &
             data_reviewed == "No data")) %>%
  bind_rows(
    sonar_raw %>%
      filter(date == ymd(20200218),
             data_recorded == "No data",
             data_reviewed == "No data") %>%
      mutate(data_recorded = "Full",
             data_reviewed = "Second 30")) %>%
  arrange(date_time) -> sonar_raw

# fix a couple times
sonar_raw <- sonar_raw |> 
  mutate(n_colon = str_count(time, ":")) |> 
  # filter(n_colon == 1) |> 
  mutate(time = if_else(n_colon == 1 & !is.na(time),
                        paste0(time, ":00"),
                        time)) |> 
  select(-n_colon)



# pull out records of fish detections
sonar_fish <- sonar_raw %>%
  filter(data_recorded != "Partial",
         confidence == 1) %>%
  filter(!is.na(frame)) %>%
  mutate(sthd_length = if_else(length > 67, T, F)) %>%
  mutate(across(time,
                hms))

#--------------------------------------------------
# which hours do we want to group together?
#--------------------------------------------------
# 6 hour blocks
hrs_fct_grp <- rep(1:4, each = 6) |>
  set_names(0:23)

# set up tibble containing all half hour periods that sonar was operating
hr_periods <- sonar_raw %>%
  group_by(year) %>%
  summarize(across(date_time,
                   list(min = min,
                        max = max),
                   na.rm = T,
                   .names = "{.fn}"),
            .groups = "drop") %>%
  mutate(date_time = map2(min,
                          max,
                          .f = function(x, y) seq(x, y, by = 30*60))) %>%
  select(-min, -max) %>%
  unnest(date_time) %>%
  mutate(date = floor_date(date_time,
                           unit = "day"),
         hour = floor_date(date_time,
                           unit = "hour"),
         time = difftime(date_time, date,
                         units = "mins"),
         hour = difftime(hour, date,
                         units = "mins"),
         across(c(time,
                  hour),
                as.period)) %>%
  left_join(sonar_raw %>%
              mutate(data_reviewed = if_else(data_recorded %in% c("None", "Partial"),
                                             "Not reviewed",
                                             data_reviewed)) %>%
              select(year, date, time = hour,
                     data_recorded,
                     data_reviewed) %>%
              distinct() %>%
              mutate(reviewed = if_else(data_reviewed == "Not reviewed",
                                        FALSE,
                                        TRUE))) %>%
  mutate(
    across(
      reviewed,
      ~ replace_na(.,
                   FALSE)
    )) %>%
  mutate(reviewed = if_else(!is.na(data_reviewed) & data_reviewed == "No data",
                            F, reviewed)) %>%
  mutate(data_recorded = if_else(is.na(data_recorded) &
                                   lag(data_recorded == "Full") &
                                   lag(data_reviewed == "First 30"),
                                 "Full",
                                 data_recorded)) %>%
  mutate(operational = if_else(reviewed |
                                 data_recorded == "Full",
                               T, F)) %>%
  mutate(hr_fct = hrs_fct_grp[as.character(hour(date_time))]) %>%
  relocate(hour, hr_fct, .after = "time") %>%
  # fix one data_reviewed entry
  mutate(data_reviewed = if_else(str_detect(time, "30M") & 
                                   data_recorded == "Full" &
                                   data_reviewed == "First 30",
                                 "Second 30",
                                 data_reviewed)) %>%
  mutate(data_reviewed = if_else(operational &
                                   data_recorded == "Full" &
                                   is.na(data_reviewed),
                                 "Not reviewed",
                                 data_reviewed)) %>%
  mutate(across(
    operational,
    ~ replace_na(., F)
  ))
  # # fix one other row
  # mutate(data_recorded = if_else(data_reviewed == "No data" &
  #                                  is.na(data_recorded),
  #                                "No data",
  #                                data_recorded)) %>%
  # mutate(operational = if_else(data_reviewed == "No data" &
  #                                !is.na(data_reviewed) &
  #                                data_recorded == "No data",
  #                              T, operational))

#-----------------------------------
# combine fish and operational data
#-----------------------------------

# which hours had the 2nd 30 min reviewed?
full_hrs <- hr_periods %>%
  filter(data_reviewed %in% c("First 30",
                              "Second 30")) %>%
  group_by(date, hour) %>%
  mutate(n_pers = n(),
         n_first = sum(str_detect(data_reviewed, "First")),
         n_second = sum(str_detect(data_reviewed, "Second"))) %>%
  # filter(year == 2022) |>
  # as.data.frame() |> head(10)
  filter(n_pers == 2,
         n_first == 1,
         n_second == 1) %>%
  arrange(date_time) %>%
  ungroup() %>%
  select(all_of(names(hr_periods)))

# add column indicating if full hour was reviewed
hr_periods <- hr_periods %>%
  left_join(full_hrs |> 
              mutate(full_hr = T)) |> 
  mutate(across(
    full_hr,
    ~ replace_na(., F)
  ))

# add counts of upstream, downstream and net large fish to hr_periods
ts_df <- hr_periods %>%
  left_join(sonar_fish %>%
              filter(confidence == 1) %>%
              group_by(year,
                       date,
                       time = hour,
                       sthd_length) %>%
              count(direction) %>%
              ungroup() %>%
              filter(direction %in% c("upstream",
                                      "downstream")) %>%
              mutate(across(direction,
                            ~ recode(.,
                                     "upstream" = "up",
                                     "downstream" = "down")),
                     across(sthd_length,
                            as.character),
                     across(sthd_length,
                            ~ recode(.,
                                     "TRUE" = "sthd",
                                     "FALSE" = "small"))) %>%
              pivot_wider(names_from = c(direction, sthd_length),
                          values_from = n,
                          values_fill = 0)) %>%
  mutate(across(c(starts_with("up"), 
                  starts_with("down")),
                ~ if_else(is.na(.) & reviewed, as.integer(0), .))) %>%
  # mutate(across(
  #   sthd_length,
  #   ~ if_else(is.na(.) & reviewed, T, .)
  # )) %>%
  # mutate(net = up - down)
  mutate(net_sthd = up_sthd - down_sthd,
         net_small = up_small - down_small)

ts_long <- ts_df %>%
  pivot_longer(c(starts_with("up"),
                 starts_with("down"),
                 starts_with("net")),
               values_to = "n_fish") %>%
  mutate(direction = str_split(name, "_", simplify = T)[,1],
         size = str_split(name, "_", simplify = T)[,2]) %>%
  select(-name) %>%
  relocate(n_fish, 
           .after = last_col())

# split data by direction, steelhead length (and also missing data)
analysis_grps <- ts_long |> 
  filter(direction != "net") |> 
  mutate(missing = if_else(is.na(n_fish), T, F)) |> 
  nest(data = -c(direction,
                 size,
                 missing)) |> 
  arrange(missing,
          size,
          direction)

#--------------------------------------------------
# species composition data
#--------------------------------------------------
spp_comp_2021 <- read_excel(here("analysis/data/raw_data",
                                 "Species Comp ALL.xlsx"),
                            "2021 lengths") %>%
  clean_names() %>%
  mutate(spp = recode(species,
                      "Resident rainbow" = "Resident RB")) %>%
  rename(date = survey_date,
         rml = rm_lower,
         rmu = rm_upper,
         age = scale_age,
         mark_status = mark,
         gear = survey_type) |> 
  mutate(fork_length_mm = fork_length * 10,
         poh_length_mm = poh * 10)

spp_comp_2022 <- read_excel(here("analysis/data/raw_data",
                                 "Dungeness_sppcomp_data_2022_FINAL.xlsx"),
                            "BullTrout",
                            skip = 2) %>%
  mutate(across(
    contains("mm"),
    as.numeric
  )) |> 
  bind_rows(read_excel(here("analysis/data/raw_data",
                            "Dungeness_sppcomp_data_2022_FINAL.xlsx"),
                       "Steelhead",
                       skip = 2) |> 
              mutate(across(
                contains("mm"),
                as.numeric
              ))) %>%
  bind_rows(read_excel(here("analysis/data/raw_data",
                            "Dungeness_sppcomp_data_2022_FINAL.xlsx"),
                       "Other",
                       skip = 2) |> 
              mutate(across(
                contains("mm"),
                as.numeric
              ))) %>%
  clean_names()

spp_comp <- spp_comp_2021 |> 
  bind_rows(spp_comp_2022 |> 
              filter(!is.na(as.numeric(count)))) |> 
  select(all_of(intersect(names(spp_comp_2022), names(spp_comp_2021)))) |> 
  mutate(year = year(date)) |> 
  relocate(year, .before = 1) |> 
  mutate(across(gear,
                ~ fct_relabel(.,
                              ~ if_else(str_detect(., "Hook"),
                                        "hook and line",
                                        .))),
         across(gear,
                ~ fct_relabel(.,
                              ~ str_to_lower(.))),
         across(species,
                ~ recode(.,
                         "RAINBOW" = "Resident rainbow",
                         "Bull Trout" = "Bull trout")))

spp_fl <- spp_comp |> 
  filter(rml < 12) |> 
  select(date,
         species,
         fork_length_mm) |> 
  filter(!is.na(fork_length_mm)) |> 
  mutate(spp_fct = if_else(species == "Steelhead",
                           1, 0)) %>%
  mutate(across(c(species,
                  spp_fct),
                as_factor)) %>%
  mutate(jday = yday(date),
         fl_mean = mean(fork_length_mm),
         fl_sd = sd(fork_length_mm),
         fl_z = (fork_length_mm - fl_mean) / fl_sd)

```


```{r operational-time}
# generate percent of each time step sonar was operational
half_hr_op = hr_periods %>%
  group_by(year, 
           date,
           time) %>%
  summarize(across(date_time,
                   min),
            tot_pers = n(),
            op_pers = sum(operational),
            op_perc = op_pers / tot_pers,
            .groups = "drop")

hr_op <- hr_periods %>%
  group_by(year, 
           date,
           hour,
           hr_fct) %>%
  summarize(across(date_time,
                   min),
            n_hrs = n_distinct(hour),
            tot_pers = n(),
            op_pers = sum(operational),
            op_perc = op_pers / tot_pers,
            .groups = "drop")

hrs_op <- hr_periods %>%
  group_by(year, 
           date,
           hr_fct) %>%
  summarize(across(date_time,
                   min),
            n_hrs = n_distinct(hour),
            tot_pers = n(),
            op_pers = sum(operational),
            op_perc = op_pers / tot_pers,
            .groups = "drop") %>%
  left_join(hrs_fct_grp %>%
              enframe(name = "hour",
                      value = "hr_fct") %>%
              mutate(across(hour,
                            as.numeric)) %>%
              group_by(hr_fct) %>%
              summarize(across(hour,
                               min))) %>%
  mutate(across(hour,
                as.period,
                unit = "hour"),
         date_time = date + hour) %>%
  relocate(hour,
           .after = "date")

day_op <- hr_periods %>%
  group_by(year, date) %>%
  summarize(
    across(
      date_time,
      ~ min(.)),
    n_hrs = n_distinct(hour),
    tot_pers = n(),
    op_pers = sum(operational),
    op_perc = op_pers / tot_pers,
    .groups = "drop") %>%
  mutate(hour = 0,
         across(hour,
                as.period,
                unit = "hour"),
         hr_fct = as.integer(1)) %>%
  relocate(hour, hr_fct,
           .after = "date")

# fix the hour of date-time to be 0
hour(day_op$date_time) = 0

# wk_op <- hr_periods %>%
#   mutate(wk_grp = week(date_time)) %>%
#   group_by(year, wk_grp) %>%
#   summarize(first_day = min(date),
#             tot_hrs = n(),
#             op_hrs = sum(operational),
#             op_perc = op_hrs / tot_hrs,
#             .groups = "drop")


ops_df <- tibble(time_scale = as_factor(c('Hour',
                                          paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                          'Day')),
                 ops = list(hr_op,
                            hrs_op,
                            day_op))

```



\newpage
# Introduction

In 2019 the Washington Department of Fish and Wildlife (WDFW) installed and operated a stationary multi-beam SONAR unit in the lower Dungeness River to enumerate and gather run-timing information on winter steelhead (*Onchorhynchus mykiss*). Steelhead spawning ground surveys in the Dungeness River basin are inherently challenging due to springtime snow melt and rain events which can lead to high, turbid water and dangerous survey conditions. In most years it is not possible to survey for steelhead through the entirety of the spawning season, and in some years poor survey conditions prevent an adequate number of surveys to estimate redd-based escapement. SONAR may provide an alternative method for steelhead enumeration and run timing in a dynamic, turbid snow-melt system like the Dungeness watershed.
<!-- This involves several sub-tasks: -->

<!-- 1. Expand 30 min observations into 60 min estimates. Most hours only the first 30 min of video was analyzed, but for a subset of hours the entire hour was analyzed. Develop a crosswalk between the first 30 min and the entire hour, and then predict the total counts from all the hours when only the first 30 min are available, with uncertainty from this crosswalk. -->
<!-- 1. Exclude bull trout from images based on species composition data.  -->
<!-- 1. Account for outages. There are periods when the sonar unit was not functioning for a variety of reasons. Develop a method to interpolate across those periods of missing data. -->


# Methods

## SONAR operation
In 2019, the SONAR unit was deployed at approximately river mile (RM) 0.3, below the majority of steelhead spawning habitat (Figure \@ref(fig:site-map)). 

```{r site-map, fig.width = 5, fig.height = 5, fig.cap = "Location of the SONAR site (white X) in the lower Dungeness River in 2019."}
knitr::include_graphics(here("analysis/figures",
                             "sonar_site_map.png"))
```

The SONAR was mounted to a pole mount and attached to a reinforced ladder, secured to the river bottom by rebar (Figure \@ref(fig:sonar-pic)). The SONAR was placed along the hardened left bank, in a spot that was protected, but that retained adequate depth so that the SONAR unit and ladder did not need to be shifted laterally to accommodate changing water levels. The SONAR site ensonified an approximately 20 meter (m) wide run in the river that fish actively migrate past, reducing the possibility of milling or holding fish. This site was easily accessible from the field trailer site, which enabled the unit to be directly connected and powered by trailer power, and any adjustments to the SONAR settings to be accomplished in the dry, safe comfort of the trailer. 

A picket weir was constructed approximately 1 m above the SONAR unit from the bank to approximately 1 m past the SONAR to deflect debris and help direct migrating fish out in front of the unit. 

```{r sonar-pic, fig.cap = "ARIS 1800 SONAR unit deployment in the Dungeness River."}
knitr::include_graphics(here("analysis/figures",
                             "sonar_pic.jpg"))
```

We deployed the ARIS 1800 Explorer, manufactured by Sound Metrics, of Bellevue, Washington. The ARIS 1800 uses 96 beams to project an acoustic wedge that ensonifies the water column. The SONAR unit was adjusted to have a pitch of 3.5 degrees to -8 degrees to ensonify the entire water column and was checked daily and adjusted as necessary to maintain full ensonification of the channel. Imagery was continuously recorded 24 hours a day, and saved in 30-minute files, so that 48 individual files were recorded for each full day of operation.  

The SONAR was operated continuously from March 5, 2019 until October 10, 2019. This report focuses on the steelhead migration period between March 5 and June 24th, 2019. From March 5th to June 24th, there were 8 days with partial data recording gaps (7% of 112 days). Four of these gaps were 1 hour or less; the other 2 data gaps ranged from approximately 18 hours (March 9 and 10) to 20 hours (April 9 and 10, Figure 3a). 

## Species Composition

## Data Processing

The first 30 minutes of each hour were processed and reviewed for fish migration. This subsampling scheme enabled the SONAR project team to keep up with data review throughout the season, and to complete a review of the entire steelhead migration period. Fourteen days were fully reviewed (60 minutes of each hour) to compare fish migration with subsampled (30 minutes of each hour) data (Figure 3b). Each reviewed imagery file was processed using Sound Metric’s proprietary software ARISFish (v2.6.3). First, raw image files were background subtracted, which removed static objects from the image so that only objects in motion are shown. Then, an echogram was created, which transformed the image into a graph of distance (y-axis) and time (x-axis), so that objects in motion appeared as white “tracks.” The echogram enabled the data reviewer to quickly navigate to parts of the image file that contained objects that could be migrating fish. These tracks were then manually viewed alongside the raw image file to determine if the object was a fish to be further investigated. 

Fish greater or equal to 45 centimeters (cm) were measured, marked, and counted using the ARISFish software. Forty-five cm was determined to be the minimum length of a potential steelhead, based on captures of steelhead during sampling in the Dungeness River 2014, 2015, and 2017 by the Jamestown S’Klallam Tribe (JSK) (unpublished data, C. Burns). Only fish that completely moved through the SONAR beams were counted; fish that nosed in and out or did not completely move from one side of the beams to the other were not counted. For each fish counted the following data were recorded:
* Date
* Hour of the 30-minute image file (e.g., 14:00, 14:30)
* Time 
* Frame
* Direction of travel (upstream or downstream)
* Range (distance from the SONAR)
* Length of the fish in cm
* Data reviewer confidence (1= extremely confident that the object counted is a fish ≥ 45 cm, 2= somewhat confident that the object is a fish ≥ 45 cm, 3= object of interest)

If no fish were observed in the 30-minute image file, a line of data with “NO FISH” was recorded to indicate that the file was reviewed for fish, but no fish ≥ 45 cm were present. Marked fish were automatically saved within the image file for later error checking; data were also recorded within an Excel spreadsheet for data summarization and analysis. 


## Analysis


### Excluding Bull Trout

Bull trout are swimming past the sonar unit as well as steelhead, and we need to parse which fish identified by the sonar are steelhead, and exclude any bull trout. The largest bull trout sampled in any species composition data was 67 cm, so we are assuming any fish larger than 67 cm detected on the sonar is a steelhead. It then remains to filter the fish equal to or less than 67 cm long from the sonar and determine what proportion of those are steelhead, and what are bull trout. 

We only have species composition data for one year, 2021. It was collected weekly, using tangle nets just upstream of the sonar location. For every fish caught, we know the date and fork length of that fish. Based on this data, we have also determined that the steelhead run on the Dungeness is over by June 1. Therefore, we have only made predictions for fish detected prior to June 1st. 

We have several options to model the probability of any particular fish, less than or equal to 67 cm, being a steelhead. we could model that probability as a factor of date (perhaps with a quadratic term to capture non-linearity), or as a factor of fork length, or both. If we use date, we can interpret the probability of being a steelhead as the proportion of all fish detected on that date that are steelhead. If we use fork length (or date and fork length), we can assign a probability to every fish detected by sonar, and assume that all fish with a probability greater than some threshold (probably 50%) are steelhead. 

<!-- Currently, we are only using fork length, because although the date of capture is probably available, it is not in the current data set. From the species composition netting, there are `r nrow(spp_fl)` fish caught with fork lengths. These can be seen in Figure \@ref(fig:fl-hist). Since we only care about differentiating steelhead, we grouped resident rainbows with bull trout, and then fit a binomial GLM with a logit link, using the fork length to predict the probability of a fish being a steelhead. We did not restrict the dataset to fish with fork lengths less than 67 cm, because larger fish have information about the shape of the logistic curve.  -->

<!-- After fitting this GLM, we predicted the probability of being a steelhead for all fish observed on the sonar that were smaller than or equal to 67 cm, based on their length. Any fish with a probability of 50% or greater we assigned to be a steelhead. We then applied the same model (Section \@ref(expanding-30-min-to-60-min)) to expand 30 minute counts for small fish to full hour counts. We added counts or estimates of large fish to small fish for each time period to estimate total net steelhead moving upstream for each time period. Estimates of large and small fish within the same time period were assumed to be independent when calculating the standard error. -->

We chose to use fork length and the Juilan day of capture. From the species composition netting, there are `r nrow(spp_fl)` fish to use in this model. These can be seen in Figures \@ref(fig:fl-hist) and \@ref(fig:fl-date). Since we only care about differentiating steelhead, we grouped resident rainbows with bull trout, and then fit a binomial GAM with a logit link, using splines of fork length and Julian day to predict the probability of a fish being a steelhead. We did not restrict the dataset to fish with fork lengths less than 67 cm, because larger fish have information about the shape of the logistic curve. 

After fitting this GAM, we predicted the probability of being a steelhead for all fish observed on the sonar that were smaller than or equal to 67 cm, based on their length and Julian day of observation. Any fish with a probability of 50% or greater we assigned to be a steelhead. We then applied the same model (Section \@ref(expanding-30-min-to-60-min)) to expand 30 minute counts for small fish to full hour counts. We added counts or estimates of large fish to small fish for each time period to estimate total net steelhead moving upstream for each time period. Estimates of large and small fish within the same time period were assumed to be independent when calculating the standard error.

```{r fl-hist, fig.cap = "Histogram of forklengths, colored by species."}
spp_fl %>%
  ggplot(aes(x = fork_length_mm,
             fill = species)) +
  geom_histogram(position = "dodge",
                 binwidth = 40) +
  scale_fill_brewer(palette = "Set1",
                    name = "Species") +
  labs(x = "Fork Length (mm)")
```

```{r fl-date, fig.cap = "Scatterplot of date of capture and forklength, colored by species."}
spp_fl %>%
  ggplot(aes(x = jday,
             y = fork_length_mm,
             color = species)) +
  geom_point() +
  scale_color_brewer(palette = "Set1",
                     name = "Species") +
  labs(x = "Julian Day",
       y = "Fork Length (mm)")
```


```{r fl-gam, eval = T}
fl_mod <- spp_fl %>%
  gam(spp_fct ~ s(fl_z, k = 5, m = 2, bs = "tp") + 
        s(jday, k = 5, m = 2, bs = "tp"),
      data = .,
      family = binomial)
# glm(spp_fct ~ fl_z + jday + I(jday^2),
#     data = .,
#     family = binomial)

# what is the fork length when 50% of being a steelhead?
p_pred = 0.5
pred_tab <- crossing(length = seq(300, 800, by = 10),
                     survey_date = seq(ymd(20210203),
                                       ymd(20210601),
                                       by = "1 days")) %>%
  mutate(fl_z = (length - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd),
         jday = yday(survey_date)) %>%
  bind_cols(predict(fl_mod,
                    newdata = .,
                    type = "response",
                    se.fit = T) %>%
              as_tibble() %>%
              select(prob_sthd = fit,
                     prob_se = se.fit))

```


```{r fl-glm, eval = F}
fl_mod <- spp_fl %>%
  glm(spp_fct ~ fl_z,
      data = .,
      family = binomial)

# summary(fl_mod)
# # probability of being steelhead at average forklength (~ 55 cm)
# boot::inv.logit(coef(fl_mod)[1])
# # if fork length increases by 1 standard deviation (~ 13 cm), 
# # odds of being a steelhead increase by a factor of:
# exp(coef(fl_mod)[2])

# what is the fork length when 50% of being a steelhead?
p_pred = 0.5
cut_pt <- boot::logit(p_pred) %>%
  subtract(coef(fl_mod)[1]) %>%
  divide_by(coef(fl_mod)[2]) %>%
  multiply_by(unique(spp_fl$fl_sd)) %>%
  add(unique(spp_fl$fl_mean)) %>%
  as.numeric()
```


```{r pred-small-fish, eval = F}
# predict species of all fish <= 67 cm
small_fish <- sonar_fish %>%
  filter(confidence == 1,
         !sthd_length) %>%
  select(year:hour, time, date_time,
         direction,
         length,
         frame:range) %>%
  mutate(fork_length_mm = length * 10,
         fl_z = (fork_length_mm - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd)) %>%
  mutate(jday = yday(date_time)) %>%
  bind_cols(predict(fl_mod,
                    newdata = .,
                    type = "response",
                    se.fit = T) %>%
              as_tibble() %>%
              select(prob_sthd = fit,
                     prob_se = se.fit)) %>%
  mutate(p_alpha = prob_sthd^2 * ((1 - prob_sthd) / prob_se^2 - prob_sthd^-1),
         p_beta = p_alpha * (prob_sthd^-1 - 1))

# filter out fish deemed not steelhead and
# combine with some other information
small_sthd_cnt <- small_fish %>%
  filter(prob_sthd >= 0.5) %>%
  group_by(year,
           date,
           time = hour,
           date_time) %>%
  count(direction,
        name = "n_fish") %>%
  ungroup() %>%
  filter(direction %in% c("upstream",
                          "downstream")) %>%
  mutate(across(direction,
                ~ recode(.,
                         "upstream" = "up",
                         "downstream" = "down"))) %>%
  left_join(hr_periods) %>%
  arrange(date_time,
          direction) %>%
  # left_join(hr_periods %>%
  #              filter(reviewed) %>%
  #              select(year,
  #                     date, time,
  #                     hour,
  #                     hr_fct,
  #                     data_reviewed,
  #                     )) %>%
  # left_join(full_hrs %>%
  #             select(year, date, hour) %>%
  #             distinct() %>%
  #             mutate(full_hour = T)) %>%
  # mutate(across(full_hour,
  #               replace_na,
  #               F)) %>%
  # arrange(date_time) %>%
  mutate(across(data_reviewed,
                recode,
                "First 30" = "first",
                "Second 30" = "second")) %>%
  pivot_wider(names_from = direction,
              values_from = n_fish,
              values_fill = 0) %>%
  mutate(net = up - down) %>%
  pivot_longer(cols = c(up, down, net),
               names_to = "direction",
               values_to = "n_fish") %>%
  select(-time,
         -date_time) %>%
  pivot_wider(names_from = data_reviewed,
              values_from = n_fish) %>%
  mutate(across(c(first,
                  second),
                ~ if_else(is.na(.) & full_hr,
                          as.integer(0), .))) %>%
  mutate(date_time = date + hour) %>%
  select(year, date, hour, hr_fct, date_time,
         full_hr,
         direction,
         first,
         second)

# start with small steelhead counted during whole hour operations 
cnt_small <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     full_sm_sthd = list(small_sthd_cnt %>%
                                           filter(full_hr) %>%
                                           mutate(total = first + second,
                                                  se = 0),
                                         
                                         small_sthd_cnt %>%
                                           filter(full_hr) %>%
                                           mutate(total = first + second) %>%
                                           mutate(fct_group = date + hours(hr_fct * 24 / max(hrs_fct_grp)),
                                                  across(fct_group,
                                                         ~ factor(as.character(.),
                                                                  labels = 1:n_distinct(.)))) %>%
                                           group_by(fct_group,
                                                    year,
                                                    date, 
                                                    direction,
                                                    hr_fct) %>%
                                           summarize(across(date_time,
                                                            min),
                                                     across(c(first,
                                                              second,
                                                              total),
                                                            sum),
                                                     .groups = "drop") %>%
                                           left_join(hrs_fct_grp %>%
                                                       enframe(name = "hour",
                                                               value = "hr_fct") %>%
                                                       mutate(across(hour,
                                                                     as.numeric)) %>%
                                                       group_by(hr_fct) %>%
                                                       summarize(across(hour,
                                                                        min)),
                                                     by = "hr_fct") %>%
                                           mutate(across(hour,
                                                         as.period,
                                                         unit = "hour"),
                                                  date_time = date + hour) %>%
                                           mutate(se = 0),
                                         
                                         small_sthd_cnt %>%
                                           filter(full_hr) %>%
                                           mutate(total = first + second) %>%
                                           group_by(year,
                                                    date, 
                                                    direction) %>%
                                           summarize(across(c(first,
                                                              second,
                                                              total),
                                                            sum),
                                                     .groups = "drop") %>%
                                           mutate(hr_fct = as.integer(1),
                                                  hour = as.period(0, unit = "hour"),
                                                  date_time = date + hour) %>%
                                           mutate(se = 0))) %>%
  unnest(full_sm_sthd) %>%
  mutate(across(direction,
                factor,
                levels = c("up",
                           "down",
                           "net"))) %>%
  # mutate(year = year(date),
  #        hour = hour(date_time),
  #        hr_fct = hrs_fct_grp[as.character(hour)],
  #        across(hour,
  #               as.period,
  #               unit = "hours")) %>%
  inner_join(per_fit_df %>%
              select(time_scale,
                     direction),
             by = join_by(time_scale,
                          direction)) %>%
  select(time_scale,
         direction,
         year,
         date,
         hour,
         hr_fct,
         everything()) %>%
  select(-fct_group) %>%
  # mutate(across(sthd_length,
  #               replace_na,
  #               F)) %>%
  arrange(time_scale,
          direction,
          date_time) %>%
  add_column(est_type = "Full Hour Census",
             .before = "first")


# next predict small steelhead from those counted during half hour operations 
pred_small <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     pred_df = list(small_sthd_cnt %>%
                                      filter(!full_hr) %>%
                                      select(-full_hr),
                                    
                                    small_sthd_cnt %>%
                                      filter(!full_hr) %>%
                                      mutate(fct_group = date + hours(hr_fct * 24 / max(hrs_fct_grp)),
                                             across(fct_group,
                                                    ~ factor(as.character(.),
                                                             labels = 1:n_distinct(.)))) %>%
                                      group_by(fct_group,
                                               year,
                                               date, 
                                               direction,
                                               hr_fct) %>%
                                      summarize(across(date_time,
                                                       min),
                                                across(c(first,
                                                         second),
                                                       sum),
                                                .groups = "drop") %>%
                                      left_join(hrs_fct_grp %>%
                                                  enframe(name = "hour",
                                                          value = "hr_fct") %>%
                                                  mutate(across(hour,
                                                                as.numeric)) %>%
                                                  group_by(hr_fct) %>%
                                                  summarize(across(hour,
                                                                   min)),
                                                by = "hr_fct") %>%
                                      mutate(across(hour,
                                                    as.period,
                                                    unit = "hour"),
                                             date_time = date + hour),
                                         
                                         small_sthd_cnt %>%
                                           filter(!full_hr) %>%
                                           group_by(year,
                                                    date, 
                                                    direction) %>%
                                           summarize(across(c(first,
                                                              second),
                                                            sum),
                                                     .groups = "drop") %>%
                                           mutate(hr_fct = as.integer(1),
                                                  hour = as.period(0, unit = "hour"),
                                                  date_time = date + hour))) %>%
  unnest(pred_df) %>%
  select(-fct_group) %>%
  nest(pred_df = -c(time_scale,
                    direction)) %>%
  inner_join(per_fit_df %>%
              select(time_scale,
                     direction,
                     mod),
            by = c("time_scale",
                   "direction")) %>%
  mutate(preds = map2(mod,
                      pred_df,
                      .f = function(x, y) {
                        predict(x,
                                newdata = y,
                                se.fit = T)
                      }),
         total = map(preds,
                   "fit"),
         se = map(preds,
                  "se.fit"),
         resid_scale = map(preds,
                           "residual.scale")) %>%
  mutate(pred_ci = map2(mod,
                        pred_df,
                        .f = function(x, y) {
                          predict(x,
                                  newdata = y,
                                  interval = "prediction") %>%
                            as_tibble() %>%
                            select(lci = lwr,
                                   uci = upr)
                        })) %>%
  select(-mod, -preds) %>%
  unnest(cols = c(pred_df, total, 
                  se, resid_scale,
                  pred_ci)) %>%
  # assume first 30 min is exactly 1/2 of full hour
  mutate(total = 2 * first,
         # for prediction intervals (instead of confidence, adjust the standard error)
         # se = sqrt(se^2 + resid_scale^2),
         lci = qnorm(0.025, total, se),
         uci = qnorm(0.975, total, se)) |> 
  select(-resid_scale) |> 
  select(time_scale,
         direction,
         year,
         date,
         hour,
         hr_fct,
         everything()) %>%
  add_column(est_type = "Half Hour Exp.",
             .before = "first")

small_sthd <- pred_small %>%
  bind_rows(cnt_small) %>%
  mutate(across(where(is.factor),
                fct_drop)) %>%
  arrange(time_scale,
          date_time,
          direction)

```

```{r combine-big-small}
# get prediction intervals when no fish are counted in first half-hour
zero_pred = per_fit_df |> 
  select(time_scale,
         direction,
         mod) |> 
  mutate(pred_ci = map(mod,
                       .f = function(x) {
                         predict(x,
                                 newdata = tibble(first = 0),
                                 # interval = "prediction",
                                 interval = "confidence",
                                 type = "response") |> 
                           as_tibble()
                       })) |> 
  unnest(pred_ci) |> 
  select(time_scale,
         direction,
         lci0 = lwr,
         uci0 = upr)

fish_est <- big_sthd %>%
  add_column(size = "big",
             .after = "direction") %>%
  bind_rows(small_sthd %>%
              select(any_of(names(big_sthd))) |> 
              add_column(size = "small",
                         .after = "direction")) %>%
  pivot_wider(names_from = size,
              values_from = c(first, second, total, se, lci, uci)) %>%
  mutate(across(c(starts_with("first"),
                  starts_with("total")),
                replace_na,
                0)) %>%
  left_join(zero_pred) %>%
  mutate(second_big = if_else(est_type == "Full Hour" & 
                                is.na(second_big) &
                                !is.na(second_small),
                              as.integer(0),
                              second_big),
         second_small = if_else(est_type == "Full Hour" & 
                                  is.na(second_small) &
                                  !is.na(second_big),
                                as.integer(0),
                                second_small)) %>%
  rowwise() %>%
  mutate(lci_big = if_else(est_type == "Half Hour Exp." &
                             first_big == 0 &
                             is.na(lci_big),
                           lci0,
                           lci_big),
         lci_small = if_else(est_type == "Half Hour Exp." &
                             first_small == 0 &
                             is.na(lci_small),
                           lci0,
                           lci_small),
         uci_big = if_else(est_type == "Half Hour Exp." &
                             first_big == 0 &
                             is.na(uci_big),
                           uci0,
                           uci_big),
         uci_small = if_else(est_type == "Half Hour Exp." &
                             first_small == 0 &
                             is.na(uci_small),
                           uci0,
                           uci_small)) %>%
  mutate(total = total_big + total_small,
         se = sqrt(sum(c(se_big, se_small)^2, na.rm = T)),
         lci = lci_small + lci_big,
         uci = uci_small + uci_big) %>%
  ungroup() %>%
  select(-lci0, 
         -uci0)

```

```{r}
# deal with some mismatches between small/big fish (duplicate hour groups / days)
dup_est <- fish_est %>%
  unite(id,
        time_scale, direction, date, hour, hr_fct,
        remove = F) %>%
  filter(id %in% id[duplicated(id)]) %>%
  arrange(time_scale,
          id,
          est_type)

if(nrow(dup_est) > 0) {
  fish_est <- fish_est %>%
    anti_join(dup_est) %>%
    bind_rows(dup_est %>%
                group_by(time_scale,
                         year, 
                         date, 
                         hr_fct,
                         hour) %>%
                summarize(across(c(first_big:total_small,
                                   total),
                                 sum,
                                 na.rm = T),
                          .groups = "drop") %>%
                left_join(dup_est %>%
                            group_by(time_scale,
                                     year, 
                                     date, 
                                     hr_fct,
                                     hour) %>%
                            summarize(across(c(contains("se"),
                                               -contains("second")),
                                             ~ sqrt(sum(.^2, na.rm = T))),
                                      .groups = "drop")) %>%
                mutate(est_type = "Mixed") %>%
                select(any_of(names(dup_est)))) %>%
    arrange(time_scale,
            date,
            hour,
            hr_fct)
}

```

```{r big-small-corr}
# are the estimates of big and small steelhead correlated?
cor_tab <- fish_est %>%
  nest(data = -c(time_scale, year)) %>%
  mutate(r = map_dbl(data,
                     .f = function(x) {
                       cor(x$total_big,
                           x$total_small,
                           use = "pairwise.complete.obs")
                     }),
         cor_test = map(data,
                        .f = function(x) {
                          cor.test(x$total_big,
                                   x$total_small)
                        }),
         p_value = map_dbl(cor_test,
                           "p.value"))
# cor_tab
```


```{r}
# expand estimates by percent sonar was operational in that period
fish_cnts <- fish_est %>%
  select(-contains("big"),
         -contains("small")) %>%
  full_join(ops_df %>%
              filter(time_scale %in% unique(fish_est$time_scale)) %>%
              unnest(ops) %>%
              select(-c(tot_pers,
                        op_pers,
                        hour,
                        hr_fct)) %>%
              crossing(direction = unique(fish_est$direction))) %>%
  # mutate(date_time = if_else(time_scale == "Hour",
  #                            date + hour,
  #                            if_else(time_scale == "Day",
  #                                    as.POSIXct(date),
  #                                    date + hours((hr_fct - 1) * (24 / max(hrs_fct_grp)))))) %>%
  relocate(date_time, 
           .after = hour) %>%
  arrange(time_scale,
          date_time) %>%
  mutate(across(total,
                ~ . / op_perc)) %>%
  mutate(est_type = if_else(is.na(est_type),
                            if_else(op_perc == 0,
                                    "Missing Data",
                                    if_else(op_perc == 1,
                                            "Zero Counts",
                                            "Partial Count")),
                            est_type)) %>%
  mutate(across(c(total, se),
                ~ if_else(is.na(.) & op_perc > 0,
                          0,
                          .)))

# # fish_est |>
# fish_cnts |>
#   group_by(time_scale,
#            direction) |> 
#   filter(date_time %in% date_time[duplicated(date_time)])

```

### Expanding 30 min to 60 min

```{r mod-data}
# period comparison for all hours with first and second half hours fully recorded
comp_30_df <- analysis_grps %>%
  filter(size == "sthd",
         !missing) %>%
  unnest(data) %>%
  filter(full_hr) %>%
  select(-date_time,
         -time) %>%
  mutate(date_time = date + hour,
         hr_grp = factor(date_time,
                         labels = 1:n_distinct(date_time))) %>%
  group_by(hr_grp) %>%
  mutate(tot_fish = sum(n_fish, na.rm = T)) %>%
  ungroup() %>%
  filter(tot_fish > 0) %>%
  select(year,
         date_time,
         date,
         hour,
         hr_fct,
         data_reviewed,
         direction,
         n_fish) %>%
  mutate(across(data_reviewed,
                recode,
                "First 30" = "first",
                "Second 30" = "second")) %>%
  pivot_wider(names_from = data_reviewed,
              values_from = n_fish) %>%
  arrange(date_time,
          direction)

# group data by different time-scales
per_fit_df <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     model_df = list(comp_30_df %>%
                                       mutate(fct_group = date + hour,
                                              across(fct_group,
                                                     ~ factor(as.character(.),
                                                              labels = 1:n_distinct(.)))) %>%
                                       group_by(date, 
                                                direction, 
                                                fct_group) %>%
                                       summarize(across(date_time,
                                                        min),
                                                 across(c(first, 
                                                          second),
                                                        sum),
                                                 .groups = "drop"),
                                     
                                     comp_30_df %>%
                                       mutate(fct_group = date + (hr_fct * 24 / max(hrs_fct_grp)),
                                              across(fct_group,
                                                     ~ factor(as.character(.),
                                                              labels = 1:n_distinct(.)))) %>%
                                       group_by(date, 
                                                direction,
                                                hr_fct,
                                                fct_group) %>%
                                       summarize(across(date_time,
                                                        min),
                                                 across(c(first, 
                                                          second),
                                                        sum),
                                                 .groups = "drop") %>%
                                       left_join(hrs_fct_grp %>%
                                                   enframe(name = "hour",
                                                           value = "hr_fct") %>%
                                                   mutate(across(hour,
                                                                 as.numeric)) %>%
                                                   group_by(hr_fct) %>%
                                                   summarize(across(hour,
                                                                    min),
                                                             .groups = "drop"),
                                                 by = join_by(hr_fct)) %>%
                                       mutate(across(hour,
                                                     as.period,
                                                     unit = "hour"),
                                              date_time = date + hour),
                                     
                                     comp_30_df %>%
                                       mutate(fct_group = date,
                                              across(fct_group,
                                                     ~ factor(as.character(.),
                                                              labels = 1:n_distinct(.)))) %>%
                                       group_by(date, 
                                                direction, 
                                                fct_group) %>%
                                       summarize(across(c(date_time),
                                                        min),
                                                 across(c(first, 
                                                          second),
                                                        sum),
                                                 .groups = "drop") %>%
                                       mutate(hour = as.period(0, unit = "hour"),
                                              hr_fct = 1) %>%
                                       mutate(across(date_time,
                                                     floor_date,
                                                     unit = "days")))) %>%
  unnest(model_df) %>%
  mutate(total = first + second) %>%
  mutate(across(direction,
                factor,
                levels = c("up",
                           "down",
                           "net"))) %>%
  arrange(time_scale,
          direction) %>%
  nest(model_df = -c(time_scale,
                     direction))

#-----------------------------------
# settled on modeling up and down separately, on daily scale
per_fit_df %<>%
  filter(time_scale == "Day",
         direction %in% c("up",
                          "down"))

```


We started by examining the counts during the first 30 minutes and the entire hour. First, we filtered for records with a confidence level of 1 (extremely confident) and a length greater than 67 cm to ensure we were only comparing records of steelhead. For each hour with both halves recorded and for each half hour, we summed the fish determined to be moving upstream, and those moving downstream, and calculated the number of net upstream fish (upstream - downstream). We then added the two half hours together to provide a number of the net upstream fish for that hour. 

We compared the first half hour with the entire hour at several temporal scales. We started with a single hour, then also summed net upstream fish in `r 24 / max(hrs_fct_grp)` hour blocks, and finally summed the net upstream fish by date. The data was structured such that hours where both half hours were examined were usually consecutive at least up to the day scale, meaning each `r 24 / max(hrs_fct_grp)` hour block and each day had nearly identical amounts of time with two half hours to other periods at the same temporal scale.

For each temporal scale grouping, we fit a linear model with the counts of net upstream fish in the first 30 minutes as the independent variable and the total net upstream fish for the hour as the dependent variable. In each model, we fixed the intercept at 0, to ensure that if no fish were counted in the first half hour, we would expand that to zero fish for the entire hour. We focused on the estimated slope, hypothesizing that it should be 2. 

```{r fit-models}
per_fit_df %<>%
  mutate(mod = map(model_df,
                   .f = function(x) {
                     lm(total ~ first - 1,
                        data = x)
                   }),
         coefs = map(mod,
                     .f = broom::tidy),
         ci = map(mod,
                  .f = function(x) {
                    ci = confint(x)
                    if(class(ci)[1] == "matrix") {
                      ci %<>%
                        as_tibble()
                    } else {
                      ci %<>%
                        enframe() %>%
                        pivot_wider()
                    }
                    return(ci)
                  })) %>%
  mutate(slope = map_dbl(coefs,
                         "estimate"),
         se = map_dbl(coefs,
                      "std.error"),
         sigma = map_dbl(mod,
                         .f = function(x) {
                           summary(x)$sigma
                         }),
         R2 = map_dbl(mod,
                      .f = function(x) {
                        summary(x)$r.squared
                      }),
         adj_R2 = map_dbl(mod,
                          .f = function(x) {
                            summary(x)$adj.r.squared
                          })) %>%
  unnest(ci) %>%
  relocate(ends_with("%"),
           .after = "se")

```

```{r make-predictions-big-fish}
# what fish were observed outside the full hour periods?
big_other_fish <- analysis_grps %>%
  filter(size == "sthd",
         !missing) %>%
  unnest(data) %>%
  filter(!full_hr) %>%
  select(any_of(names(comp_30_df)),
         data_reviewed,
         first = n_fish)


# predict number of steelhead-sized fish going by during non-review periods
big_other_pred <- tibble(time_scale = as_factor(c('Hour',
                                                  paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                                  'Day')),
                         pred_df = list(big_other_fish %>%
                                          mutate(fct_group = date + hour,
                                                 across(fct_group,
                                                        ~ factor(as.character(.),
                                                                 labels = 1:n_distinct(.)))) %>%
                                          group_by(date, 
                                                   direction, 
                                                   fct_group) %>%
                                          summarize(across(date_time,
                                                           min),
                                                    across(c(first),
                                                           sum),
                                                    .groups = "drop") |> 
                                          arrange(date_time,
                                                  direction),
                                        
                                        big_other_fish %>%
                                          mutate(fct_group = date + hours(hr_fct * 24 / max(hrs_fct_grp)),
                                                 across(fct_group,
                                                        ~ factor(as.character(.),
                                                                 labels = 1:n_distinct(.)))) %>%
                                          group_by(date, 
                                                   direction, 
                                                   hr_fct,
                                                   fct_group) %>%
                                          summarize(across(date_time,
                                                           min),
                                                    across(c(first),
                                                           sum),
                                                    .groups = "drop") %>%
                                          left_join(hrs_fct_grp %>%
                                                      enframe(name = "hour",
                                                              value = "hr_fct") %>%
                                                      mutate(across(hour,
                                                                    as.numeric)) %>%
                                                      group_by(hr_fct) %>%
                                                      summarize(across(hour,
                                                                       min)),
                                                    by = "hr_fct") %>%
                                          mutate(across(hour,
                                                        as.period,
                                                        unit = "hour"),
                                                 date_time = date + hour) |> 
                                          arrange(date_time,
                                                  direction),
                                        
                                        big_other_fish %>%
                                          mutate(fct_group = date,
                                                 across(fct_group,
                                                        ~ factor(as.character(.),
                                                                 labels = 1:n_distinct(.)))) %>%
                                          group_by(date, 
                                                   direction, 
                                                   fct_group) %>%
                                          summarize(across(c(first),
                                                           sum),
                                                    .groups = "drop") %>%
                                          mutate(date_time = date,
                                                 hr_fct = as.integer(1)) |> 
                                          arrange(date_time,
                                                  direction))) %>%
  unnest(pred_df) %>%
  nest(pred_df = -c(time_scale,
                    direction)) %>%
  inner_join(per_fit_df %>%
               select(time_scale,
                      direction,
                      mod),
             by = c("time_scale",
                    "direction")) %>%
  mutate(preds = map2(mod,
                      pred_df,
                      .f = function(x, y) {
                        predict(x,
                                newdata = y,
                                se.fit = T)
                      }),
         total = map(preds,
                     "fit"),
         se = map(preds,
                  "se.fit"),
         resid_scale = map(preds,
                           "residual.scale")) %>%
  mutate(pred_ci = map2(mod,
                        pred_df,
                        .f = function(x, y) {
                          predict(x,
                                  newdata = y,
                                  interval = "prediction") %>%
                            as_tibble() %>%
                            select(lci = lwr,
                                   uci = upr)
                        })) %>%
  select(-mod, -preds) %>%
  unnest(cols = c(pred_df, total, 
                  se, resid_scale, 
                  pred_ci)) %>%
  # assume first 30 min is exactly 1/2 of full hour
  mutate(total = 2 * first,
         # for prediction intervals (instead of confidence, adjust the standard error)
         # se = sqrt(se^2 + resid_scale^2),
         lci = qnorm(0.025, total, se),
         uci = qnorm(0.975, total, se)) |> 
  select(-resid_scale) |> 
  mutate(year = year(date),
         hour = hour(date_time),
         hr_fct = hrs_fct_grp[as.character(hour)],
         across(hour,
                as.period,
                unit = "hours")) %>%
  select(time_scale,
         direction,
         year,
         date,
         hour,
         hr_fct,
         everything()) %>%
  select(-fct_group) %>%
  add_column(est_type = "Half Hour Exp.",
             .before = "first")

```

```{r}
# put all steelhead-sized fish together (reviewed and predicted)
big_sthd = per_fit_df |> 
  select(time_scale,
         direction, 
         model_df) |> 
  unnest(model_df) |> 
  select(#-hour,
         #-hr_fct,
         -fct_group) |> 
  mutate(se = 0,
         year = year(date_time)) |> 
  relocate(year,
           .after = "date_time") |> 
  add_column(est_type = "Full Hour Census",
             .before = "first") |> 
  select(any_of(names(big_other_pred)),
         everything()) |> 
  bind_rows(big_other_pred) |> 
  arrange(time_scale,
          date_time,
          direction)
```


```{r}
# save for other analyses
save(fish_cnts, 
     fish_est,
     big_sthd,
     small_fish,
     hrs_fct_grp,
     ops_df,
     file = here("analysis/data/derived_data",
                 "est_fish_cnts.rda"))
```


### Missing Data

There are periods when the sonar was not functioning, for a variety of reasons. Rather than ignore those time periods, and assume that no steelhead were passing then, we would prefer to impute net upstream fish for those missing values.

The first step is to expand the estimates of net upstream fish for periods when the sonar only partially operated (e.g. 14 hours out of a 24 hour day). We did this by dividing the estimate for that period by the percent of time the sonar was operational in that period. This assumes that fish are behaving similarly for that entire period. 

The next step is to deal with those periods when the sonar was not operating at all, where we have truly missing data. Table \@ref(tab:miss-data) shows how much data was missing for each year, depending on how the periods were constructed (e.g. hourly, hourly blocks, daily).

```{r create-time-series}
library(zoo)
library(forecast)
library(imputeTS)

ts_est <- fish_cnts %>%
  select(time_scale,
         direction,
         year,
         date_time,
         est_type,
         total, se) %>%
  nest(data = -c(time_scale,
                 direction,
                 year)) %>%
  mutate(n_periods = map_dbl(data,
                             .f = function(x) {
                               nrow(x)
                             }),
         n_NA = map_dbl(data,
                        .f = function(x) {
                          sum(is.na(x$total))
                        }),
         perc_NA = n_NA / n_periods,
         ts_zoo = map(data,
                      .f = function(x) {
                        zoo(x$total,
                            x$date_time)
                      }),
         ts = map(ts_zoo,
                  as.ts)) %>%
  # fit some ARIMA models
  mutate(auto_arima = map(ts_zoo,
                          .f = auto.arima,
                          seasonal = F,
                          allowdrift = F,
                          ic = "aicc"),
         order = map_chr(auto_arima,
                         .f = function(x) {
                           arimaorder(x) %>%
                             as.vector() %>%
                             paste(collapse = " ")
                         }),
         sigma2 = map_dbl(auto_arima,
                          "sigma2"),
         se = sqrt(sigma2))

```

```{r impute-missing}
ts_est %<>%
  # make predictions based on best ARIMA model
  mutate(kalman_preds = map(ts_zoo,
                            .f = function(x) {
                              na_kalman(x,
                                        model = "auto.arima",
                                        smooth = T) %>%
                                as_tibble() %>%
                                rename(kalman_pred = value)
                            }),
         # predict based on linear interpolation
         lin_preds = map(ts_zoo,
                         .f = function(x) {
                           na_interpolation(x,
                                            option = "linear") %>%
                             as_tibble() %>%
                             rename(lin_pred = value)
                         }),
         # predict based on moving average
         ma_preds = map(ts_zoo,
                        .f = function(x) {
                          na_ma(x,
                                k = 4,
                          ) %>%
                            as_tibble() %>%
                            rename(ma_pred = value)
                        }))

# combine all predictions back with existing counts
all_preds <- fish_cnts %>%
  mutate(hour = if_else(time_scale == "Day",
                        as.period(NA),
                        hour),
         hr_fct = if_else(time_scale == "Day",
                        NA_real_,
                        hr_fct)) %>%
  full_join(ts_est %>%
              select(time_scale,
                     direction,
                     year,
                     data,
                     pred_se = se,
                     ends_with("preds")) %>%
              unnest(cols = c(data, ends_with("preds"))) %>%
              mutate(across(contains("pred"),
                            ~ if_else(est_type != "Missing Data",
                                      NA_real_,
                                      .))) %>%
              filter(est_type == "Missing Data") %>%
              select(-total,
                     -se) %>%
              pivot_longer(ends_with("pred"),
                           names_to = "model",
                           values_to = "est") %>%
              mutate(across(model,
                            str_remove,
                            "_pred$")) %>%
              mutate(lci_ts = qnorm(0.025, est, pred_se),
                     uci_ts = qnorm(0.975, est, pred_se)) %>%
              mutate(date = floor_date(date_time,
                                       unit = "days"),
                     hour = if_else(time_scale == "Hour",
                                    hms(paste(hour(date_time),
                                              minute(date_time),
                                              second(date_time))),
                                    as.period(NA)),
                     hr_fct = if_else(str_detect(time_scale,
                                                 "Block"),
                                      hour(date_time) / (24 / max(hrs_fct_grp)) + 1,
                                      NA_real_)),
            multiple = "all") %>%
  select(time_scale:hour,
         date_time,
         everything()) %>%
  mutate(model = if_else(is.na(model),
                         if_else(est_type == "Half Hour Exp.",
                                 "expansion",
                                 "none"),
                         model)) %>%
  mutate(est = if_else(is.na(est),
                       total,
                       est),
         lci = if_else(is.na(lci),
                       lci_ts,
                       lci),
         uci = if_else(is.na(uci),
                       uci_ts,
                       uci),
         se = if_else(is.na(se),
                      pred_se,
                      se)) %>%
  select(time_scale:date_time,
         op_perc,
         est_type,
         total,
         model,
         est, se, lci, uci) |> 
  arrange(time_scale,
          date_time,
          direction)

all_preds %>%
  filter(!model %in% c("lin",
                      "ma")) %>%
  group_by(time_scale,
           year) %>%
  summarize(est = sum(est),
            se = sqrt(sum(se^2)),
            lci = sum(lci),
            uci = sum(uci)) %>%
  add_column(model = "kalman",
             .before = 1) %>%
  mutate(lci_v2 = qnorm(0.025, est, se),
         uci_v2 = qnorm(0.975, est, se))


```


```{r miss-data}
ts_df %>%
  select(time_scale, year,
         n_periods:perc_NA) %>%
  mutate(across(perc_NA,
                ~ . * 100)) %>%
  arrange(year,
          time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "n Periods",
                      "n NAs",
                      "% NA"),
        digits = c(rep(0, 4), 1),
        caption = "Table showing how many periods are in each group of data, and how many of those periods are NAs (missing values).") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```

To interpolate across those periods of missing data, we employed time-series models. Using the `forecast` package in R, we fit an ARIMA (auto-regressive integrated moving average) model, and let the `auto.arima` function determine the model with the best order (number of auto-regressive, moving average and difference steps) for each year and time-scale combination. We used the uncertainty from this model ($\sigma^2$) for all predictions. 

We examined several forms of interpolation across the missing data, including a Kalman filter, linear regression and moving average. The Kalman filter uses the ARIMA structure to estimate the missing data. A linear regression essentially draws a straight line from the data point prior to the first missing data and the data point after the last missing data point for each gap in the time series. A moving average approach uses two non-missing values prior to the missing data point, and two non-missing values after, weights them exponentially by their distance from the missing data point, and calculates the weighted mean. 

\newpage
# Results

## Expanding 30 min to 60 min

The expansion factor (i.e. slope) changes depending on the temporal scale that data is summarized on. Figure \@ref(fig:period-comp-fig) shows the various regressions, comparing them with the 1-1 line and the expected slope of two. None of the temporal scales produced a slope of two, but the longer the temporal scale the closer it got to that expected value. 

```{r period-comp-fig, fig.cap = "Scatter plots showing the net counts of fish moving upstream, using hours with both the first 30 minutes and second 30 minutes. The counts are summarized by hour, six hour blocks and entire day (24 hours) in the different facets. The dashed red line is has a slope of 2 (expected value), the dotted grey line has a slope of 1, and the blue line is the linear regression fit to that data, with 95% confidence intervals."}
per_fit_df %>%
  select(time_scale,
         model_df) %>%
  unnest(model_df) %>%
  ggplot(aes(x = first,
             y = total)) +
  geom_abline(slope = 2,
              linetype = 2,
              color = "red") +
  geom_abline(slope = 1,
              linetype = 3,
              color = "gray30") +
  geom_smooth(method = "lm",
              formula = y ~ x - 1) +
  # geom_point() +
  geom_point(position = position_jitter(0.2, 0.2,
                                        seed = 4)) +
  facet_wrap(~ time_scale,
             nrow = 1,
             scales = "free") +
  labs(x = "Only first 30 minutes",
       y = "Entire hour")
```


Table \@ref(tab:lin-coef-tab) shows the summary of linear models fit to data summarized at various time scales. We summarized the estimated number of steelhead larger than 67 cm at the day scale (summing estimates at smaller temporal scales) and plotted the time-series in Figure \@ref(fig:ts-est) to show the differences caused by summarizing data at different time scales. 

```{r lin-coef-tab}
per_fit_df %>%
  mutate(across(ends_with("%"),
                round,
                2)) %>%
  mutate(CI = paste(`2.5 %`, `97.5 %`, sep = "-")) %>%
  select(time_scale,
         slope,
         se,
         CI,
         adj_R2) %>%
  kable(col.names = c("Time Scale",
                      "Slope",
                      "SE",
                      "95% CI",
                      "R2"),
        digits = 2,
        booktabs = T,
        linesep = "",
        caption = "Results of fitting linear models with total net upstream fish as the response and the net upstream fish in the first 30 minutes as the covariate with no intercept.") %>%
  kable_styling()

```

```{r big-est}
big_sthd %>%
  group_by(time_scale,
           year) %>%
  summarize(total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  arrange(year, time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "Estimate",
                      "SE"),
        digits = c(0, 0, 0, 2),
        caption = "Estimates of total net upstream fish larger than 67 cm, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```

```{r ts-est, fig.height = 7, fig.cap = "Time-series of estimates based on available data, using only extremely confident observations of fish greater than 67 cm, faceted by year. Colors correspond to which regression model was used to expand the 30 minutes observations. Any uncertainty shown is derived from the linear regression model."}
big_sthd %>%
  full_join(ops_df %>%
              unnest(ops)) %>%
  group_by(time_scale,
           year,
           date) %>%
  summarize(avg_op = mean(op_perc),
            n_full_hr = sum(total > 0 & se == 0, na.rm = T),
            n_est = n(),
            n_nas = sum(is.na(total)),
            total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(across(c(total, se),
                ~ if_else(avg_op == 0,
                          NA_real_,
                          .))) %>%
  ggplot(aes(x = date,
             y = total,
             color = time_scale,
             fill = time_scale)) +
  geom_hline(yintercept = 0,
             linetype = 2,
             color = "gray40") +
  geom_ribbon(aes(ymin = qnorm(0.025, total, se),
                  ymax = qnorm(0.975, total, se)),
              alpha = 0.2,
              color = NA) +
  geom_line() +
  # scale_color_brewer(palette = "Set1",
  #                    name = "Model") +
  # scale_fill_brewer(palette = "Set1",
  #                    name = "Model") +
  scale_color_viridis_d(name = "Model",
                        end = 0.8) +
  scale_fill_viridis_d(name = "Model",
                       end = 0.8) +
  facet_wrap(~ year,
             ncol = 1,
             scales = "free") +
  labs(x = "Date",
       y = "Net Upstream Fish")

```

```{r est-corr}
cor_tab <- fish_cnts %>%
  group_by(time_scale,
           year,
           date) %>%
  summarize(n_full_hr = sum(is.na(se)),
            total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  select(-n_full_hr,
         -se) %>%
  pivot_wider(names_from = time_scale,
              values_from = total) %>%
  select(-year,
         -date) %>%
  corrr::correlate(use = "pairwise.complete.obs",
                   method = "pearson",
                   quiet = T)

```


## Excluding Bull Trout

<!-- Figure \@ref(fig:fig-glm) shows the fitted GLM that predicts the probability of being a steelhead based on a fish's length. Note that a 67 cm long fish would have a  round(predict(fl_mod, newdata = tibble(fl_z = (67 - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd)), type = "response"), 3) * 100% of being a steelhead with this model. -->

```{r fig-glm, eval = F, fig.cap = "Points show the fork length of steelhead (along the top) and non-steelhead (along the bottom), with the fitted binomial GLM in red. The dashed line shows where fish greater than that would have a greater than 50% probability of being a steelhead. The dotted line shows the 67 cm threshold for which fish we will be applying this model to."}
spp_fl %>%
  ggplot(aes(fork_length_mm,
             spp_fct)) +
  geom_point() +
  geom_vline(xintercept = cut_pt,
             linetype = 2) +
  geom_vline(xintercept = 67,
             linetype = 3) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "cauchit")),
  #             aes(color = "cauchit",
  #                 fill = "cauchit"),
  #             alpha = 0.2) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "probit")),
  #             aes(color = "probit",
  #                 fill = "probit"),
#             alpha = 0.2) +
# geom_smooth(method = glm,
#             formula = y ~ x,
#             method.args = list(family = binomial(link = "cloglog")),
#             aes(color = "cloglog",
#                 fill = "cloglog"),
#             alpha = 0.2) +
geom_smooth(method = glm,
            formula = y ~ x,
            method.args = list(family = binomial),
            aes(color = "logit",
                fill = "logit"),
            alpha = 0.2) +
  scale_color_brewer(palette = "Set1",
                     name = "Link Fnc.") +
  scale_fill_brewer(palette = "Set1",
                    name = "Link Fnc.") +
  theme(legend.position = "none") +
  labs(y = "Probability of Being a Steelhead",
       x = "Fork Length (cm)")

```

Figure \@ref(fig:fig-gam) shows the fitted GAM that predicts the probability of being a steelhead based on a fish's length and date of capture. Note that the mean probability of being a steelhead for a 67 cm long fish, averaged across the entire season, would be `r round(mean(pred_tab$prob_sthd[pred_tab$length == 67]), 3) * 100`% of being a steelhead with this model. Also note that a fish 60 cm long would not be considered a steelhead if observed at the very beginning of the season or after the beginning of May, but would if observed in late March or April. 

```{r fig-gam, fig.cap = "The color depicts the probability of fish being a steelhead given the date of capture and fork length. Fish above the black line would have a greater than 50% probability of being a steelhead. The dotted line shows the 67 cm threshold for which fish we will be applying this model to."}
pred_tab %>%
  ggplot(aes(x = survey_date,
             y = length,
             fill = prob_sthd)) +
  geom_tile() +
  theme(panel.border = element_blank(),
        panel.grid = element_blank(),
        legend.position = "bottom") +
  scale_fill_viridis_c(name = "Probability of\nBeing a Steelhead") +
  geom_line(data = pred_tab %>%
              filter(prob_sthd >= p_pred) %>%
              group_by(survey_date) %>%
              filter(length == min(length)) %>%
              arrange(survey_date, length),
            color = "black") +
  geom_hline(yintercept = 670,
             linetype = 2,
             color = 'darkgray') +
  labs(x = "Date",
       y = "Fork Length (mm)")

```

Applying this model and rule-set to all the observed fish smaller than or equal to 67 cm, including the predictive model to expand 30 minute counts to full hour counts, a number of additional steelhead are added to our estimate each year (Table \@ref(tab:sm-est)). The total estimates (including all fish larger than 67 cm, as well as fish less than or equal to 67 cm that are predicted to be steelhead) are shown in Table \@ref(tab:tot-est). 

```{r sm-est}
small_fish %>%
  group_by(time_scale, 
           year) %>%
  summarize(total = sum(total),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_val = paste0(round(total), " (",
                           round(se, 1),
                           ")")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = year,
              values_from = prnt_val) %>%
  rename(`Time Scale` = time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        digits = c(0, 0, 0, 2),
        caption = "Estimates (SE) of total net upstream steelhead smaller than 67 cm, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling()

```

```{r tot-est}
fish_est %>%
  group_by(time_scale, 
           year) %>%
  summarize(total = sum(total),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_val = paste0(round(total), " (",
                           round(se, 1),
                           ")")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = year,
              values_from = prnt_val) %>%
  rename(`Time Scale` = time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        digits = c(0, 0, 0, 2),
        caption = "Estimates (SE) of total net upstream steelhead, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling()

```


## Missing Data

Figure \@ref(fig:oper-fig) shows the periods when the sonar array was not operating, and Figure \@ref(fig:miss-ts) shows how that impacts the time-series of fish counts. Note the large period in 2020 when the sonar was shut down due to COVID-19.

As the temporal scale on which counts are aggregated increases, the amount of missing data decreases. For example, if three hours are missing within a day, we can expand the rest of the day's counts by the percent of time the sonar was operational, so that day will not be "missing" at the day time-scale, although those three hours still are if we are operating on an hour time-scale. 

```{r oper-fig, fig.cap = "Purple depicts hours when the sonar was working, while yellow indicates the sonar was not functioning."}
hr_periods %>%
  mutate(hr = as.numeric(hour) / (60*60)) %>%
  mutate(date = as.Date(paste(month(date), mday(date)), format = "%m %d")) %>%
  ggplot(aes(x = hr,
             y = date,
             color = operational,
             fill = operational)) +
  geom_tile() +
  scale_fill_viridis_d(direction = -1,
                       name = "Sonar\nOperational") +
  scale_color_viridis_d(direction = -1,
                        name = "Sonar\nOperational") +
  facet_wrap(~ year,
             scales = "fixed") +
  scale_y_date(breaks = breaks_pretty(7)) +
  labs(y = "Date",
       x = "Hour") +
  theme(legend.position = "bottom")
```

```{r miss-ts, fig.height = 6, fig.cap = "Time series of upstream fish in blue, with missing data highlighted in red."}
p_list = vector("list",
                length = nrow(ts_df))
for(i in seq_along(ts_df$time_scale)) {
  p_list[[i]] <- ggplot_na_distribution(ts_df$ts_zoo[[i]]) +
    labs(title = paste(ts_df$time_scale[i], "in", ts_df$year[i]),
         subtitle = element_blank()) +
    theme(axis.title.y = element_blank())
}
ggarrange(plotlist = p_list,
          ncol = 3,
          nrow = 3,
          common.legend = T,
          legend = "bottom")
```

After interpolating across the missing data, Table \@ref(tab:missing-tab) displays how many fish were added to each year's estimate, based on the temporal scale and the interpolation method. Table \@ref(tab:abund-est-missing) provides final estimates of total net upstream steelhead each year, including fish smaller than 67 cm and periods of missing data, split out by the temporal scale the data was summarized on. 

```{r missing-tab}
all_preds %>%
  filter(is.na(total)) %>%
  group_by(time_scale,
           year,
           model) %>%
  summarize(pred_tot = sum(est, na.rm = T),
            pred_se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_value = paste0(round(pred_tot), " (", round(pred_se, 1), ")")) %>%
  select(-starts_with("pred_")) %>%
  pivot_wider(names_from = "model",
              values_from = prnt_value) %>%
  right_join(expand(all_preds,
                    time_scale, year)) %>%
  arrange(year,
          time_scale) %>%
  kable(col.names = c("Time Scale",
                      "Year",
                      "Kalman",
                      "Linear",
                      "MA"),
        booktabs = T,
        linesep = "",
        caption = "Estimates (SE) of how many net upstream steelhead are added to the totals from periods with wholly missing data. Interpolation methods include the Kalman filter, linear model, and moving average. There are three estimates per year, corresponding to the three different temporal scales. Blank cells indicate no interpolation was necessary for that year / temporal scale combination.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```


```{r abund-est-missing}
all_preds %>%
  filter(!is.na(total)) %>%
  group_by(time_scale,
           year) %>%
  summarize(obs_tot = sum(total, na.rm = T),
            obs_se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  full_join(all_preds %>%
              filter(is.na(total)) %>%
              group_by(time_scale,
                       year,
                       model) %>%
              summarize(pred_tot = sum(est, na.rm = T),
                        pred_se = sqrt(sum(se^2, na.rm = T)),
                        .groups = "drop")) %>%
  mutate(across(model,
                replace_na,
                "none"),
         across(starts_with("pred"),
                replace_na,
                0)) %>%
  mutate(total = obs_tot + pred_tot,
         se = sqrt(obs_se^2 + pred_se^2)) %>%
  mutate(None = paste0(round(obs_tot), " (", round(obs_se, 1), ")"),
         prnt_value = paste0(round(total), " (", round(se, 1), ")")) %>%
  select(-starts_with("obs"),
         -starts_with("pred")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = "model",
              values_from = prnt_value) %>%
  select(-none) %>%
  arrange(year,
          time_scale) %>%
  kable(col.names = c("Time Scale",
                      "Year",
                      "None",
                      "Kalman",
                      "Linear",
                      "MA"),
        booktabs = T,
        linesep = "",
        caption = "Estimates (SE) of total net upstream steelhead, using only extremely confident observations, and after interpolating counts for periods of missing data. Interpolation methods include no interpolation, the Kalman filter, linear model, and moving average. There are three estimates per year, corresponding to the three different temporal scales.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)

```

\newpage
# Discussion Points

* What to do with rows where `data_recorded ` is "Partial"? This includes `r sum(sonar_raw$data_recorded  == "Partial")` rows, or `r round(sum(sonar_raw$data_recorded  == "Partial") / nrow(sonar_raw) * 100, 1)`% of the data. Exclude and treat as missing data? Or is there a better way to parse this? Currently I've filtered it out and treated it as missing.
* What should we do with observations with confidence of 2 or 3? They are currently excluded completely.
* What to do about kelts?

## Expanding 30 min to 60 min

* The regression between counts in the first hour and the entire hour shows a consistent expectation that the counts in the second part of the hour will be less than counts in the first part. This holds regardless of whether we aggregate data by hour, day or something in between. 
* Currently I'm calculating the net upstream totals for each half hour period before running the regressions. If there's a compelling reason to either run separate regressions for downstream and upstream moving fish, or treat downstream and upstream fish from the same hour as two distinct data points, let's talk about that.
* We could run separate regressions for each year, but if the methodology is the same year-to-year, I don't see why we'd expect different results.
* Three years is not enough to use year as a random effect, but perhaps in a few more years we could explore this as an option.

## Excluding Bull Trout

* Should we assume any fish smaller than 40 cm is a bull trout, since that was the smallest steelhead length recorded? Under the current method, this doesn't matter because any fish smaller than `r round(min(pred_tab$length[pred_tab$prob_sthd >= 0.5]), 1)` cm is excluded from the steelhead counts regardless of date of capture.

<!-- * Incorporating some kind of spline or curve to account for steelhead run timing could improve this model. For example a 55 cm long fish might be considered a non-steelhead early or late in the season, but could be predicted to be a steelhead in the middle of the run, because it may be more likely that any fish is a steelhead then. To fit this model, I'll need the date when each fish's length was taken as part of the species composition data. -->

* I updated this model to include both the Julian day of capture and the fish length, using a spline for both covariates. The fish length spline turned out to be pretty close to a straight line, with larger fish being more likely to be a steelhead. The Julian day of capture had a peak probability of being a steelhead occurring in mid- to late-March, and tapering off on either side. Given there were a number of bull trout caught in the very beginning and towards the end of the sampling period, this shape makes sense. However, it should be noted that including Julian day of capture reduced the estimated number of small steelhead (less than 67 cm) in all years. It was a small reduction in 2020 and 2021, but a substantial one in 2019. This appears to be because there were a large number of small fish detected late in the run in 2019. Because they were so late, virtually
none of those fish were predicted to be steelhead using this updated model.

## Missing Data

* Any of the interpolation models we tested (Kalman filter, linear regression or moving average) resulted in larger estimates of steelhead moving upstream (Table \@ref(tab:abund-est-missing)), but differed in which one provide the biggest increase depending on the time-scale and year.
* The uncertainty (e.g. standard error) grew when incorporating those missing data, which is appropriate. The uncertainty grew substantially in 2020, when there was a large period of missing data due to COVID restrictions.
* Depending on how big the missing data gaps are, and the time-scale we are aggregating data on, there were some years and time-scales with no missing data (e.g. 24 hour scale in 2021). The lack of missing data relies on using the percentage of hours when sonar was operational within each time-step to increase the estimates for any time-steps when the operational time was less than 100%.
* The alternative to expanding time-steps when the sonar was partially operational is to remove all data from those time-steps and treat them as missing data. 
* It's unclear to me whether that would have a substantial impact on the overall estimates or uncertainty.

