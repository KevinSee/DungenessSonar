---
title: "Estimates of Steelhead in Dungeness River"
subtitle: "Using Sonar"
author:
  - Kevin See:
      email: Kevin.See@dfw.wa.gov
      institute: [wdfw]
      correspondence: true
  # - Bethany Craig:
  #     email: Bethany.Craig@dfw.wa.gov
  #     institute: [wdfw]
  #     correspondence: false
institute:
  - wdfw: Washington Department of Fish & Wildlife
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
    wdfwTemplates::wdfw_html_format2:
      fig_caption: yes
      fig_height: 4
      fig_width: 6
      toc: yes
      toc_depth: 3
      toc_float:
        collapsed: yes
        smooth_scroll: yes
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::pdf_document2:
      fig_caption: yes
      fig_height: 5
      fig_width: 6
      toc: yes
      includes:
        in_header: ../templates/header_WDFW.tex
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks2.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::word_document2:
      fig_caption: yes
      fig_height: 4
      fig_width: 6
      toc: yes
      reference_docx: "../templates/ReportTemplate.docx" # Insert path for the DOCX file
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
bibliography:
  # - AUC.bib
  - references.bib
csl: "../templates/american-fisheries-society.csl" # Insert path for the bib-style
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/",
  dpi = 300
)
```

```{r packages}
# load these packages
library(tidyverse)
library(here)
library(magrittr)
library(janitor)
library(lubridate)
library(ggfortify)
library(ggpubr)
library(scales)
library(kableExtra)

theme_set(theme_bw())

# knitr options
options(knitr.kable.NA = '-')

# when knitting to Word, use this
# what kind of document is being created?
doc.type <- knitr::opts_knit$get('rmarkdown.pandoc.to')

if(doc.type == 'docx') {
  options(knitr.table.format = "pandoc")
}

```

\newpage
# Goals

To estimate the number of adult steelhead spawners in the Dungeness river, with appropriate uncertainty, based upon a sonar system set up in the lower river. This involves several sub-tasks:

1. Expand 30 min observations into 60 min estimates. Most hours only the first 30 min of video was analyzed, but for a subset of hours the entire hour was analyzed. Develop a crosswalk between the first 30 min and the entire hour, and then predict the total counts from all the hours when only the first 30 min are available, with uncertainty from this crosswalk.
1. Exclude bull trout from images based on species composition data. 
1. Account for outages. There are periods when the sonar unit was not functioning for a variety of reasons. Develop a method to interpolate across those periods of missing data.


# Methods


```{r read-data}
sonar_raw <- read_csv(here("analysis/data/raw_data",
                           "2019 sonar.csv")) %>%
  mutate(across(Hour,
                hms)) %>%
  bind_rows(read_csv(here("analysis/data/raw_data",
                          "2020 sonar.csv")) %>%
              mutate(across(Hour,
                            str_pad,
                            width = 5,
                            side = "left",
                            pad = "0")) %>%
              mutate(across(Hour,
                            hm)) %>%
              rename(comments = `Comments/Notes`) %>%
              clean_names("upper_camel")) %>%
  bind_rows(read_csv(here("analysis/data/raw_data",
                          "2021 sonar.csv")) %>%
              mutate(across(Hour,
                            ~ hm(paste(str_sub(., 1,2),
                                       str_sub(., 3, 4),
                                       sep = ":")))) %>%
              rename(comments = `Comments/Notes`) %>%
              clean_names("upper_camel")) %>%
  filter(!is.na(Year)) %>%
  mutate(across(Date,
                mdy)) %>%
  mutate(across(DataReviewed,
                recode,
                "No Data" = "No data")) %>%
  mutate(across(DataRecorded,
                recode,
                "No Data" = "No data")) %>%
  mutate(across(Time,
                recode,
                "No Data" = "No data",
                "no fish" = "No fish",
                "No Fish" = "No fish"),
         Time = if_else(str_detect(Comments, "No data"),
                        "No data",
                        Time),
         across(Direction,
                recode,
                "Downstream" = "downstream",
                "Upstream" = "upstream")) %>%
  clean_names() %>%
  mutate(date_time = date + hour) %>%
  arrange(date_time)


# pull out records of fish detections
sonar_fish <- sonar_raw %>%
  filter(data_recorded != "Partial") %>%
  filter(!is.na(frame)) %>%
  mutate(sthd_length = if_else(length > 67, T, F)) %>%
  mutate(across(time,
                hms))

# determine operational times
hrs_summ <- sonar_raw %>%
  group_by(year) %>%
  summarize(across(date_time,
                   list(min = min,
                        max = max),
                   na.rm = T,
                   .names = "{.fn}")) %>%
  mutate(across(c(min, max),
                floor_date,
                unit = "hour")) %>%
  mutate(n_hrs = difftime(max, min, units = "hours"),
         across(n_hrs,
                as.numeric)) %>%
  rowwise() %>%
  mutate(hrs = list(min + dhours(0:n_hrs)),
         ints = list(interval(start = hrs,
                              end = lead(hrs) - dseconds(1))))

hr_periods <- hrs_summ %>%
  select(year, ints) %>%
  unnest(ints) %>%
  mutate(start_int = int_start(ints),
         end_int = int_end(ints),
         end_int = if_else(is.na(end_int),
                           start_int,
                           end_int)) %>%
  mutate(ints = interval(start = start_int,
                         end = end_int)) %>%
  select(-start_int,
         -end_int) %>%
  rowwise() %>%
  mutate(operational = if_else(sum(sonar_raw$date_time[sonar_raw$data_recorded != "Partial"] %within% ints, na.rm = T) > 0,
                               T, F)) %>%
  ungroup() %>%
  mutate(date = floor_date(int_start(ints), "day"),
         date_time = floor_date(int_start(ints), "hour"),
         hour = difftime(date_time, date,
                         units = "hours"),
         across(hour,
                as.period)) %>%
  select(year, date, hour, everything()) %>%
  arrange(date_time)


spp_comp <- read_csv(here("analysis/data/raw_data",
                          "species comp.csv")) %>%
  clean_names() %>%
  mutate(across(date,
                mdy)) %>%
    select(date:sthd, notes)

spp_fl <- readxl::read_excel(here("analysis/data/raw_data",
                                  "Species Comp ALL.xlsx"),
                             "2021",
                             range = "S3:U38") %>%
  mutate(across(everything(),
                ~ if_else(. > 100,
                          . / 10, .))) %>%
  pivot_longer(everything(),
               names_to = "spp",
               values_to = "fork_length",
               values_drop_na = T) %>%
  mutate(across(spp,
                as_factor)) %>%
  mutate(fl_mean = mean(fork_length),
         fl_sd = sd(fork_length),
         fl_z = (fork_length - fl_mean) / fl_sd)
```


```{r}
#--------------------------------------------------
# which hours do we want to group together?
#--------------------------------------------------

# # 12 hour blocks
# hrs_fct_grp <- rep(1:2, each = 12) |>
#   set_names(0:23)

# # 8 hour blocks
# hrs_fct_grp <- rep(1:3, each = 8) |>
#   set_names(0:23)

# 6 hour blocks
hrs_fct_grp <- rep(1:4, each = 6) |>
  set_names(0:23)

# # 4 hour blocks
# hrs_fct_grp <- rep(1:6, each = 4) |>
#   set_names(0:23)

# # 2 hour blocks
# hrs_fct_grp <- rep(1:12, each = 2) |>
#   set_names(0:23)

# generate percent of each time step sonar was operational
hr_op <- hr_periods %>%
  group_by(year, date,
           hour) %>%
  summarize(tot_hrs = n(),
            op_hrs = sum(operational),
            op_perc = op_hrs / tot_hrs,
            .groups = "drop")

hrs_op <- hr_periods %>%
  mutate(hr_fct = hrs_fct_grp[as.character(hour(date_time))]) %>%
  group_by(year, date,
           hr_fct) %>%
  summarize(tot_hrs = n(),
            op_hrs = sum(operational),
            op_perc = op_hrs / tot_hrs,
            .groups = "drop")

day_op <- hr_periods %>%
  group_by(year, date) %>%
  summarize(tot_hrs = n(),
            op_hrs = sum(operational),
            op_perc = op_hrs / tot_hrs,
            .groups = "drop")

# wk_op <- hr_periods %>%
#   mutate(wk_grp = week(date_time)) %>%
#   group_by(year, wk_grp) %>%
#   summarize(first_day = min(date),
#             tot_hrs = n(),
#             op_hrs = sum(operational),
#             op_perc = op_hrs / tot_hrs,
#             .groups = "drop")


ops_df <- tibble(time_scale = as_factor(c('Hour',
                                          paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                          'Day')),
                 ops = list(hr_op,
                            hrs_op,
                            day_op))

```

## Expanding 30 min to 60 min

```{r mod-data}
# which hours had the 2nd 30 min reviewed?
full_hrs <- sonar_raw %>%
  filter(data_reviewed == "Second 30") %>%
  select(year, date, hour) %>%
  mutate(hour = hour - minutes(30)) %>%
  left_join(sonar_raw) %>%
  bind_rows(sonar_raw %>%
              filter(data_reviewed == "Second 30")) %>%
  arrange(date, hour) %>%
  filter(data_recorded == "Full") %>%
  select(year, date, hour, data_recorded, data_reviewed) %>%
  distinct() %>%
  mutate(date_time = date + hour,
         hr_group = floor_date(date_time, "hours"),
         day_group = floor_date(date_time, "days"),
         # wk_group = floor_date(date_time, "weeks"),
         operational = TRUE) %>%
  mutate(hr_fct = hrs_fct_grp[as.character(hour(date_time))],
         hrs_group = paste(date, hr_fct)) %>%
  mutate(across(ends_with("_group"),
                ~ as.factor(as.numeric(as.factor(.))))) %>%
  relocate(operational,
           .before = "hr_group")


# period comparison for all hours with first and second half hours fully recorded
per_comp <- full_hrs %>%
  filter(operational) %>%
  select(year:hour,
         hr_fct,
         time_period = data_reviewed,
         ends_with("_group")) %>%
  distinct() %>%
  mutate(across(time_period,
                str_remove,
                " 30$")) %>%
  left_join(sonar_fish %>%
              filter(confidence == 1,
                     sthd_length))


# group data by different time-scales
per_fit_df <- tibble(time_scale = as_factor(c('Hour',
                                          paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                          'Day')),
                 model_df = list(per_comp %>%
                                   rename(fct_group = hr_group) %>%
                                   mutate(hour = if_else(time_period == "Second",
                                                         hour - minutes(30),
                                                         hour)) %>%
                                   select(-day_group,
                                          -hrs_group),
                                 per_comp %>%
                                   rename(fct_group = hrs_group) %>%
                                   select(-day_group,
                                          -hr_group),
                                 per_comp %>%
                                   rename(fct_group = day_group) %>%
                                   select(-hr_group,
                                          -hrs_group))) %>%
  mutate(model_df = map(model_df,
                        .f = function(x) {
                          x %>%
                            group_by(year, date,
                                     fct_group,
                                     time_period,
                                     direction) %>%
                            summarize(n_fish = sum(!is.na(frame), na.rm = T),
                                      .groups = "drop") %>%
                            filter(!is.na(direction)) %>%
                            pivot_wider(names_from = direction,
                                        values_from = n_fish,
                                        values_fill = 0) %>%
                            mutate(tot_fish = upstream - downstream) %>%
                            select(-any_of(c("downstream",
                                             "upstream"))) %>%
                            pivot_wider(names_from = time_period,
                                        values_from = tot_fish,
                                        names_sort = T,
                                        values_fill = 0) %>%
                            mutate(Total = First + Second) %>%
                            clean_names()
                          }))

```

We started by examining the counts during the first 30 minutes and the entire hour. First, we filtered for records with a confidence level of 1 (extremely confident) and a length greater than 67 cm to ensure we were only comparing records of steelhead. For each hour with both halves recorded and for each half hour, we summed the fish determined to be moving upstream, and those moving downstream, and calculated the number of net upstream fish (upstream - downstream). We then added the two half hours together to provide a number of the net upstream fish for that hour. 

We compared the first half hour with the entire hour at several temporal scales. We started with a single hour, then also summed net upstream fish in `r 24 / max(hrs_fct_grp)` hour blocks, and finally summed the net upstream fish by date. The data was structured such that hours where both half hours were examined were usually consecutive at least up to the day scale, meaning each `r 24 / max(hrs_fct_grp)` hour block and each day had nearly identical amounts of time with two half hours to other periods at the same temporal scale.

For each temporal scale grouping, we fit a linear model with the counts of net upstream fish in the first 30 minutes as the independent variable and the total net upstream fish for the hour as the dependent variable. In each model, we fixed the intercept at 0, to ensure that if no fish were counted in the first half hour, we would expand that to zero fish for the entire hour. We focused on the estimated slope, hypothesizing that it should be 2. 

```{r fit-models}
per_fit_df %<>%
  mutate(mod = map(model_df,
                   .f = function(x) {
                     lm(total ~ first - 1,
                        data = x)
                   }),
         coefs = map(mod,
                     .f = broom::tidy),
         ci = map(mod,
                  .f = function(x) {
                    confint(x) %>%
                      as_tibble()
                  })) %>%
  mutate(slope = map_dbl(coefs,
                         "estimate"),
         se = map_dbl(coefs,
                         "std.error"),
         R2 = map_dbl(mod,
                     .f = function(x) {
                       summary(x)$r.squared
                     }),
         adj_R2 = map_dbl(mod,
                          .f = function(x) {
                            summary(x)$adj.r.squared
                          })) %>%
  unnest(ci) %>%
  relocate(ends_with("%"),
           .after = "se")

```

```{r make-predictions-big-fish}
# what fish were observed outside the full hour periods?
other_fish <- sonar_fish %>%
  anti_join(full_hrs %>%
              select(year, date, hour)) %>%
  filter(confidence == 1,
         sthd_length) %>%
  mutate(hr_group = floor_date(date_time, "hours")) %>%
  mutate(hr_fct = hrs_fct_grp[as.character(hour(date_time))],
         hrs_group = paste(date, hr_fct)) %>%
  mutate(day_group = floor_date(date_time, "days")) %>%
  mutate(across(ends_with("_group"),
                ~ as.factor(as.numeric(as.factor(.)))))

pred_other <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     pred_df = list(other_fish %>%
                                      rename(fct_group = hr_group),
                                    other_fish %>%
                                      rename(fct_group = hrs_group),
                                    other_fish %>%
                                      rename(fct_group = day_group))) %>%
  mutate(pred_df = map(pred_df,
                       .f = function(x) {
                         x %>%
                           group_by(year, date,
                                    fct_group,
                                    time_period = data_reviewed,
                                    direction) %>%
                           summarize(n_fish = sum(!is.na(frame), na.rm = T),
                                     .groups = "drop") %>%
                           filter(direction %in% c("upstream",
                                                   "downstream")) %>%
                           pivot_wider(names_from = direction,
                                       values_from = n_fish,
                                       values_fill = 0) %>%
                           mutate(first = upstream - downstream) %>%
                           select(-any_of(c("downstream",
                                            "upstream"))) %>%
                           clean_names()
                       })) %>%
  left_join(per_fit_df %>%
              select(time_scale,
                     mod),
            by = "time_scale") %>%
  mutate(preds = map2(mod,
                      pred_df,
                      .f = function(x, y) {
                        predict(x,
                                newdata = y,
                                se.fit = T)
                      }),
         total = map(preds,
                   "fit"),
         se = map(preds,
                  "se.fit")) %>%
  select(-mod, -preds) %>%
  unnest(cols = c(pred_df, total, se)) %>%
  left_join(tibble(time_scale = as_factor(c('Hour',
                                            paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                            'Day')),
                   time_df = list(other_fish %>%
                                    mutate(fct_group = hr_group) %>%
                                    select(year, date, hour, 
                                           fct_group) %>%
                                    distinct(),
                                  other_fish %>%
                                    mutate(fct_group = hrs_group) %>%
                                    select(year, date, hr_fct, 
                                           fct_group) %>%
                                    distinct(),
                                  other_fish %>%
                                    mutate(fct_group = day_group) %>%
                                    select(year, date, 
                                           fct_group) %>%
                                    distinct())) %>%
              unnest(time_df)) %>%
  select(time_scale,
         year,
         date,
         hour,
         hr_fct,
         everything()) %>%
  select(-fct_group,
         -time_period) %>%
  add_column(est_type = "Half Hour",
             .before = "first")
```

```{r}
fish_big <- per_fit_df %>%
  select(time_scale, 
         model_df) %>%
  unnest(model_df) %>%
  left_join(tibble(time_scale = as_factor(c('Hour',
                                            paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                            'Day')),
                   time_df = list(per_comp %>%
                                    mutate(fct_group = hr_group) %>%
                                    mutate(hour = if_else(time_period == "Second",
                                                          hour - minutes(30),
                                                          hour)) %>%
                                    select(year, date, hour, 
                                           fct_group) %>%
                                    distinct(),
                                  per_comp %>%
                                    mutate(fct_group = hrs_group) %>%
                                    select(year, date, hr_fct, 
                                           fct_group) %>%
                                    distinct(),
                                  per_comp %>%
                                    mutate(fct_group = day_group) %>%
                                    select(year, date, 
                                           fct_group) %>%
                                    distinct())) %>%
              unnest(time_df)) %>%
  relocate(hour, hr_fct,
           .after = date) %>%
  select(-fct_group) %>%
  mutate(est_type = "Full Hour") %>%
  relocate(est_type, 
           .before = first) %>%
  mutate(se = 0) %>%
  bind_rows(pred_other) %>%
  relocate(hr_fct,
           .after = date) %>%
  arrange(time_scale,
          date,
          hr_fct,
          hour)

```

## Excluding Bull Trout

Bull trout are swimming past the sonar unit as well as steelhead, and we need to parse which fish identified by the sonar are steelhead, and exclude any bull trout. The largest bull trout sampled in any species composition data was 67 cm, so we are assuming any fish larger than 67 cm detected on the sonar is a steelhead. It then remains to filter the fish equal to or less than 67 cm long from the sonar and determine what proportion of those are steelhead, and what are bull trout. 

We only have species composition data for one year, 2021. It was collected weekly, using tangle nets just upstream of the sonar location. For every fish caught, we know the date and fork length of that fish. 

We have several options to model the probability of any particular fish, less than or equal to 67 cm, being a steelhead. we could model that probability as a factor of date (perhaps with a quadratic term to capture non-linearity), or as a factor of fork length, or both. If we use date, we can interpret the probability of being a steelhead as the proportion of all fish detected on that date that are steelhead. If we use fork length (or date and fork length), we can assign a probability to every fish detected by sonar, and assume that all fish with a probability greater than some threshold (probably 50%) are steelhead. 

Currently, we are only using fork length, because although the date of capture is probably available, it is not in the current data set. From the species composition netting, there are `r nrow(spp_fl)` fish caught with fork lengths. These can be seen in Figure \@ref(fig:fl-hist). Since we only care about differentiating steelhead, we grouped resident rainbows with bull trout, and then fit a binomial GLM with a logit link, using the fork length to predict the probability of a fish being a steelhead. We did not restrict the dataset to fish with fork lengths less than 67 cm, because larger fish have information about the shape of the logistic curve. 

After fitting this GLM, we predicted the probability of being a steelhead for all fish observed on the sonar that were smaller than or equal to 67 cm, based on their length. Any fish with a probability of 50% or greater we assigned to be a steelhead. We then applied the same model (Section \@ref(expanding-30-min-to-60-min)) to expand 30 minute counts for small fish to full hour counts. We added counts or estimates of large fish to small fish for each time period to estimate total netsteelhead moving upstream for each time period. Estimates of large and small fish within the same time period were assumed to be independent when calculating the standard error.

```{r fl-hist, fig.cap = "Histogram of forklengths, colored by species."}
spp_fl %>%
  ggplot(aes(x = fork_length,
             fill = spp)) +
  geom_histogram(position = "dodge",
                 binwidth = 4) +
  scale_fill_brewer(palette = "Set1",
                     name = "Species") +
  labs(x = "Fork Length (cm)")
```


```{r fl-glm}
fl_mod <- spp_fl %>%
  mutate(spp = recode(spp,
                      "Resident RB" = "Bull Trout"),
         across(spp,
                fct_drop)) %>%
  mutate(spp_fct = as.numeric(spp) - 1) %>%
  glm(spp_fct ~ fl_z,
      data = .,
      family = binomial)

# summary(fl_mod)
# # probability of being steelhead at average forklength (~ 55 cm)
# boot::inv.logit(coef(fl_mod)[1])
# # if fork length increases by 1 standard deviation (~ 13 cm), 
# # odds of being a steelhead increase by a factor of:
# exp(coef(fl_mod)[2])

# what is the fork length when 50% of being a steelhead?
p_pred = 0.5
cut_pt <- boot::logit(p_pred) %>%
  subtract(coef(fl_mod)[1]) %>%
  divide_by(coef(fl_mod)[2]) %>%
  multiply_by(unique(spp_fl$fl_sd)) %>%
  add(unique(spp_fl$fl_mean)) %>%
  as.numeric()

# predict species of all fish <= 67 cm
small_fish <- sonar_fish %>%
  filter(confidence == 1,
         !sthd_length) %>%
  select(year:hour, time, direction,
         length,
         frame:range) %>%
  mutate(fl_z = (length - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd)) %>%
  bind_cols(predict(fl_mod,
                    newdata = .,
                    type = "response",
                    se.fit = T) %>%
              as_tibble() %>%
              select(prob_sthd = fit,
                     prob_se = se.fit)) %>%
  mutate(p_alpha = prob_sthd^2 * ((1 - prob_sthd) / prob_se^2 - prob_sthd^-1),
         p_beta = p_alpha * (prob_sthd^-1 - 1))

```


```{r pred-small-fish}
# filter out fish deemed not steelhead and
# combine with some other information
small_sthd <- small_fish %>%
  filter(prob_sthd >= 0.5) %>%
  left_join(sonar_fish) %>%
  left_join(full_hrs %>%
              select(year, date, hour) %>%
              distinct() %>%
              mutate(full_hour = T)) %>%
  mutate(across(full_hour,
                replace_na,
                F)) %>%
  mutate(time_period = data_reviewed) %>%
  mutate(across(time_period,
                str_remove,
                " 30$")) %>%
  mutate(hr_group = floor_date(date_time, "hours")) %>%
  mutate(hr_fct = hrs_fct_grp[as.character(hour(date_time))],
         hrs_group = paste(date, hr_fct)) %>%
  mutate(day_group = floor_date(date_time, "days")) %>%
  mutate(across(ends_with("_group"),
                ~ as.factor(as.numeric(as.factor(.)))))


# start with small steelhead counted during whole hour operations 
calc_small <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     full_sm_sthd = list(small_sthd %>%
                                      filter(full_hour) %>%
                                      rename(fct_group = hr_group) %>%
                                      mutate(hour = if_else(time_period == "Second",
                                                         hour - minutes(30),
                                                         hour)),
                                    small_sthd %>%
                                      filter(full_hour) %>%
                                      rename(fct_group = hrs_group),
                                    small_sthd %>%
                                      filter(full_hour) %>%
                                      rename(fct_group = day_group))) %>%
  mutate(full_sm_sthd = map(full_sm_sthd,
                       .f = function(x) {
                         x %>%
                           group_by(year, date,
                                    fct_group,
                                    time_period,
                                    direction) %>%
                           summarize(n_fish = sum(!is.na(frame), na.rm = T),
                                     .groups = "drop") %>%
                           filter(direction %in% c("upstream",
                                                   "downstream")) %>%
                           pivot_wider(names_from = direction,
                                       values_from = n_fish,
                                       values_fill = 0) %>%
                            mutate(tot_fish = upstream - downstream) %>%
                           select(-any_of(c("downstream",
                                            "upstream"))) %>%
                           pivot_wider(names_from = time_period,
                                       values_from = tot_fish,
                                       values_fill = 0,
                                       names_sort = T) %>%
                           mutate(Total = First + Second) %>%
                           clean_names() %>%
                           mutate(se = 0)
                       })) %>%
  unnest(full_sm_sthd) %>%
  arrange(time_scale,
          date,
          fct_group) %>%
  add_column(est_type = "Full Hour",
             .before = "first")


# next predict small steelhead from those counted during half hour operations 
pred_small <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     pred_df = list(small_sthd %>%
                                      filter(!full_hour) %>%
                                      rename(fct_group = hr_group),
                                    small_sthd %>%
                                      filter(!full_hour) %>%
                                      rename(fct_group = hrs_group),
                                    small_sthd %>%
                                      filter(!full_hour) %>%
                                      rename(fct_group = day_group))) %>%
  mutate(pred_df = map(pred_df,
                       .f = function(x) {
                         x %>%
                           group_by(year, date,
                                    fct_group,
                                    time_period,
                                    direction) %>%
                           summarize(n_fish = sum(!is.na(frame), na.rm = T),
                                     .groups = "drop") %>%
                           filter(direction %in% c("upstream",
                                                   "downstream")) %>%
                           pivot_wider(names_from = direction,
                                       values_from = n_fish,
                                       values_fill = 0) %>%
                           mutate(first = upstream - downstream) %>%
                           select(-any_of(c("downstream",
                                            "upstream"))) %>%
                           clean_names()
                       })) %>%
  left_join(per_fit_df %>%
              select(time_scale,
                     mod),
            by = "time_scale") %>%
  mutate(preds = map2(mod,
                      pred_df,
                      .f = function(x, y) {
                        predict(x,
                                newdata = y,
                                se.fit = T)
                      }),
         total = map(preds,
                   "fit"),
         se = map(preds,
                  "se.fit")) %>%
  select(-mod, -preds) %>%
  unnest(cols = c(pred_df, total, se)) %>%
  select(-time_period) %>%
  add_column(est_type = "Half Hour",
             .before = "first")

fish_small <- calc_small %>%
  bind_rows(pred_small) %>%
  arrange(time_scale, 
          date,
          fct_group) %>%
  left_join(tibble(time_scale = as_factor(c('Hour',
                                            paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                            'Day')),
                   time_df = list(small_sthd %>%
                                    mutate(fct_group = hr_group) %>%
                                    mutate(hour = if_else(time_period == "Second",
                                                         hour - minutes(30),
                                                         hour)) %>%
                                    select(year, date, hour, 
                                           fct_group) %>%
                                    distinct(),
                                  small_sthd %>%
                                    mutate(fct_group = hrs_group) %>%
                                    select(year, date, hr_fct, 
                                           fct_group) %>%
                                    distinct(),
                                  small_sthd %>%
                                    mutate(fct_group = day_group) %>%
                                    select(year, date, 
                                           fct_group) %>%
                                    distinct())) %>%
              unnest(time_df)) %>%
  select(time_scale,
         year,
         date,
         hour,
         hr_fct,
         everything()) %>%
  select(-fct_group)

# fish_cnts %>%
#   left_join(small_cnts %>%
#               rename(sm_est_type = est_type) %>%
#               rename(small_tot = total,
#                      small_se = se)) %>%
#   filter(!is.na(small_tot),
#          est_type != sm_est_type) %>%
#   tabyl(est_type, sm_est_type)
#   mutate(date_time = ymd_hms(paste(date, hour))) %>%
  

```

```{r combine-big-small}
fish_est <- fish_big %>%
  mutate(size = "big") %>%
  bind_rows(fish_small %>%
              mutate(size = "small")) %>%
  pivot_wider(names_from = size,
              values_from = c(first, second, total, se)) %>%
  mutate(across(c(starts_with("first"),
                  starts_with("total")),
                replace_na,
                0)) %>%
  mutate(second_big = if_else(est_type == "Full Hour" & 
                                is.na(second_big) &
                                !is.na(second_small),
                              as.integer(0),
                              second_big),
         second_small = if_else(est_type == "Full Hour" & 
                                  is.na(second_small) &
                                  !is.na(second_big),
                                as.integer(0),
                                second_small)) %>%
  rowwise() %>%
  mutate(total = total_big + total_small,
         se = sqrt(sum(c(se_big, se_small)^2, na.rm = T))) %>%
  ungroup()
```

```{r}
# deal with some mismatches between small/big fish (duplicate hour groups / days)
dup_est <- fish_est %>%
  unite(id,
        time_scale, date, hour, hr_fct,
        remove = F) %>%
  filter(id %in% id[duplicated(id)]) %>%
  arrange(time_scale,
          id,
          est_type)

fish_est <- fish_est %>%
  anti_join(dup_est) %>%
  bind_rows(dup_est %>%
              group_by(time_scale,
                       year, 
                       date, 
                       hr_fct,
                       hour) %>%
              summarize(across(c(first_big:total_small,
                                 total),
                               sum,
                               na.rm = T),
                        .groups = "drop") %>%
              left_join(dup_est %>%
                          group_by(time_scale,
                                   year, 
                                   date, 
                                   hr_fct,
                                   hour) %>%
                          summarize(across(c(contains("se"),
                                             -contains("second")),
                                           ~ sqrt(sum(.^2, na.rm = T))),
                                    .groups = "drop")) %>%
              mutate(est_type = "Mixed") %>%
              select(any_of(names(dup_est)))) %>%
  arrange(time_scale,
          date,
          hour,
          hr_fct)

```

```{r big-small-corr}
# are the estimates of big and small steelhead correlated?
cor_tab <- fish_est %>%
  group_by(time_scale,
           year) %>%
  nest() %>%
  ungroup() %>%
  mutate(r = map_dbl(data,
                     .f = function(x) {
                       cor(x$total_big,
                           x$total_small,
                           use = "pairwise.complete.obs")
                     }),
         cor_test = map(data,
                        .f = function(x) {
                          cor.test(x$total_big,
                                   x$total_small)
                        }),
         p_value = map_dbl(cor_test,
                           "p.value"))
# cor_tab
```


```{r}
# expand estimates by percent sonar was operational in that period
fish_cnts <- fish_est %>%
  select(-contains("big"),
         -contains("small")) %>%
  full_join(ops_df %>%
              unnest(ops) %>%
              select(-tot_hrs,
                     -op_hrs)) %>%
  mutate(date_time = if_else(time_scale == "Hour",
                             date + hour,
                             if_else(time_scale == "Day",
                                     as.POSIXct(date),
                                     date + hours((hr_fct - 1) * (24 / max(hrs_fct_grp)))))) %>%
  relocate(date_time, 
           .after = hour) %>%
  arrange(time_scale,
          date_time) %>%
  mutate(across(total,
                ~ . / op_perc)) %>%
  mutate(est_type = if_else(is.na(est_type),
                            if_else(op_perc == 0,
                                    "Missing Data",
                                    if_else(op_perc == 1,
                                            "Zero Counts",
                                            "Partial Count")),
                            est_type)) %>%
  mutate(across(c(total, se),
                ~ if_else(is.na(.) & op_perc > 0,
                          0,
                          .)))
```

```{r}
# save for other analyses
save(fish_cnts, 
     fish_est,
     fish_big,
     fish_small,
     hrs_fct_grp,
     ops_df,
     file = here("analysis/data/derived_data",
                 "est_fish_cnts.rda"))
```


## Missing Data

There are periods when the sonar was not functioning, for a variety of reasons. Rather than ignore those time periods, and assume that no steelhead were passing then, we would prefer to impute net upstream fish for those missing values.

The first step is to expand the estimates of net upstream fish for periods when the sonar only partially operated (e.g. 14 hours out of a 24 hour day). We did this by dividing the estimate for that period by the percent of time the sonar was operational in that period. This assumes that fish are behaving similarly for that entire period. 

The next step is to deal with those periods when the sonar was not operating at all, where we have truly missing data. Table \@ref(tab:miss-data) shows how much data was missing for each year, depending on how the periods were constructed (e.g. hourly, hourly blocks, daily).

```{r create-time-series}
library(zoo)
library(forecast)
library(imputeTS)

ts_df <- fish_cnts %>%
  select(time_scale,
         year,
         date_time,
         est_type,
         total, se) %>%
  group_by(time_scale,
           year) %>%
  nest() %>%
  ungroup() %>%
  mutate(n_periods = map_dbl(data,
                             .f = function(x) {
                               nrow(x)
                             }),
         n_NA = map_dbl(data,
                        .f = function(x) {
                          sum(is.na(x$total))
                        }),
         perc_NA = n_NA / n_periods,
         ts_zoo = map(data,
                      .f = function(x) {
                        zoo(x$total,
                            x$date_time)
                      }),
         ts = map(ts_zoo,
                  as.ts)) %>%
  # fit some ARIMA models
  mutate(auto_arima = map(ts_zoo,
                          .f = auto.arima,
                          seasonal = F,
                          allowdrift = F,
                          ic = "aicc"),
         order = map_chr(auto_arima,
                         .f = function(x) {
                           arimaorder(x) %>%
                             as.vector() %>%
                             paste(collapse = " ")
                         }),
         sigma2 = map_dbl(auto_arima,
                          "sigma2"),
         se = sqrt(sigma2))

```

```{r impute-missing}
ts_df %<>%
  # make predictions based on best ARIMA model
  mutate(kalman_preds = map(ts_zoo,
                           .f = function(x) {
                             na_kalman(x,
                                       model = "auto.arima",
                                       smooth = T) %>%
                               as_tibble() %>%
                               rename(kalman_pred = value)
                             }),
         # predict based on linear interpolation
         lin_preds = map(ts_zoo,
                        .f = function(x) {
                          na_interpolation(x,
                                           option = "linear") %>%
                            as_tibble() %>%
                            rename(lin_pred = value)
                          }),
         # predict based on moving average
         ma_preds = map(ts_zoo,
                         .f = function(x) {
                           na_ma(x,
                                 k = 4,
                                 ) %>%
                             as_tibble() %>%
                             rename(ma_pred = value)
                         }))

# combine all predictions back with existing counts
all_preds <- fish_cnts %>%
  full_join(ts_df %>%
              select(time_scale,
                     year,
                     data,
                     pred_se = se,
                     ends_with("preds")) %>%
              unnest(cols = c(data, ends_with("preds"))) %>%
              mutate(across(contains("pred"),
                            ~ if_else(est_type != "Missing Data",
                                      NA_real_,
                                      .))) %>%
              filter(est_type == "Missing Data") %>%
              select(-total,
                     -se) %>%
              pivot_longer(ends_with("pred"),
                           names_to = "model",
                           values_to = "est") %>%
              mutate(across(model,
                            str_remove,
                            "_pred$")) %>%
              mutate(lci = qnorm(0.025, est, pred_se),
                     uci = qnorm(0.975, est, pred_se)) %>%
              mutate(date = floor_date(date_time,
                                       unit = "days"),
                     hour = if_else(time_scale == "Hour",
                                    hms(paste(hour(date_time),
                                              minute(date_time),
                                              second(date_time))),
                                    as.period(NA)),
                     hr_fct = if_else(str_detect(time_scale,
                                                 "Block"),
                                      hour(date_time) / (24 / max(hrs_fct_grp)) + 1,
                                      NA_real_))) %>%
  select(time_scale:hour,
         date_time,
         everything()) %>%
  mutate(model = if_else(is.na(model),
                         if_else(est_type == "Half Hour",
                                 "expansion",
                                 "none"),
                         model)) %>%
  mutate(est = if_else(is.na(est),
                       total,
                       est),
         lci = if_else(is.na(lci),
                       qnorm(0.025, total, se),
                       lci),
         uci = if_else(is.na(uci),
                       qnorm(0.975, total, se),
                       uci),
         se = if_else(is.na(se),
                      pred_se,
                      se)) %>%
  select(time_scale:date_time,
         op_perc,
         est_type,
         total,
         model,
         est, se, lci, uci)

# all_preds %>%
#   filter(!model %in% c("lin",
#                       "ma")) %>%
#   group_by(time_scale,
#            year) %>%
#   summarize(est = sum(est),
#             se = sqrt(sum(se^2)),
#             lci = sum(lci),
#             uci = sum(uci)) %>%
#   add_column(model = "kalman",
#              .before = 1) %>%
#   mutate(lci_v2 = qnorm(0.025, est, se),
#          uci_v2 = qnorm(0.975, est, se))


```


```{r miss-data}
ts_df %>%
  select(time_scale, year,
         n_periods:perc_NA) %>%
  mutate(across(perc_NA,
                ~ . * 100)) %>%
  arrange(year,
          time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "n Periods",
                      "n NAs",
                      "% NA"),
        digits = c(rep(0, 4), 1),
        caption = "Table showing how many periods are in each group of data, and how many of those periods are NAs (missing values).") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```

To interpolate across those periods of missing data, we employed time-series models. Using the `forecast` package in R, we fit an ARIMA (auto-regressive integrated moving average) model, and let the `auto.arima` function determine the model with the best order (number of auto-regressive, moving average and difference steps) for each year and time-scale combination. We used the uncertainty from this model ($\sigma^2$) for all predictions. 

We examined several forms of interpolation across the missing data, including a Kalman filter, linear regression and moving average. The Kalman filter uses the ARIMA structure to estimate the missing data. A linear regression essentially draws a straight line from the data point prior to the first missing data and the data point after the last missing data point for each gap in the time series. A moving average approach uses two non-missing values prior to the missing data point, and two non-missing values after, weights them exponentially by their distance from the missing data point, and calculates the weighted mean. 

\newpage
# Results

## Expanding 30 min to 60 min

The expansion factor (i.e. slope) changes depending on the temporal scale that data is summarized on. Figure \@ref(fig:period-comp-fig) shows the various regressions, comparing them with the 1-1 line and the expected slope of two. None of the temporal scales produced a slope of two, but the longer the temporal scale the closer it got to that expected value. 

```{r period-comp-fig, fig.cap = "Scatter plots showing the net counts of fish moving upstream, using hours with both the first 30 minutes and second 30 minutes. The counts are summarized by hour, six hour blocks and entire day (24 hours) in the different facets. The dashed line is has a slope of 2 (expected value), the dotted line has a slope of 1, and the blue line is the linear regression fit to that data, with 95% confidence intervals."}
per_fit_df %>%
  select(time_scale,
         model_df) %>%
  unnest(model_df) %>%
  ggplot(aes(x = first,
             y = total)) +
  geom_abline(slope = 2,
              linetype = 2,
              color = "gray30") +
  geom_abline(slope = 1,
              linetype = 3,
              color = "gray30") +
  geom_smooth(method = "lm",
              formula = y ~ x - 1) +
  # geom_point() +
  geom_point(position = position_jitter(0.2, 0.2,
                                        seed = 4)) +
  facet_wrap(~ time_scale,
             nrow = 1,
             scales = "free") +
  labs(x = "Only first 30 minutes",
       y = "Entire hour")
```


Table \@ref(tab:lin-coef-tab) shows the summary of linear models fit to data summarized at various time scales. We summarized the estimated number of steelhead larger than 67 cm at the day scale (summing estimates at smaller temporal scales) and plotted the time-series in Figure \@ref(fig:ts-est) to show the differences caused by summarizing data at different time scales. 

```{r lin-coef-tab}
per_fit_df %>%
  mutate(across(ends_with("%"),
                round,
                2)) %>%
  mutate(CI = paste(`2.5 %`, `97.5 %`, sep = "-")) %>%
  select(time_scale,
         slope,
         se,
         CI,
         adj_R2) %>%
  kable(col.names = c("Time Scale",
                      "Slope",
                      "SE",
                      "95% CI",
                      "R2"),
        digits = 2,
        booktabs = T,
        linesep = "",
        caption = "Results of fitting linear models with total net upstream fish as the response and the net upstream fish in the first 30 minutes as the covariate with no intercept.") %>%
  kable_styling()

```

```{r big-est}
fish_big %>%
  group_by(time_scale,
           year) %>%
  summarize(total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  arrange(year, time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "Estimate",
                      "SE"),
        digits = c(0, 0, 0, 2),
        caption = "Estimates of total net upstream fish larger than 67 cm, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```

```{r ts-est, fig.height = 7, fig.cap = "Time-series of estimates based on available data, using only extremely confident observations of fish greater than 67 cm, faceted by year. Colors correspond to which regression model was used to expand the 30 minutes observations. Any uncertainty shown is derived from the linear regression model."}
fish_big %>%
  full_join(ops_df %>%
              unnest(ops)) %>%
  group_by(time_scale,
           year,
           date) %>%
  summarize(avg_op = mean(op_perc),
            n_full_hr = sum(total > 0 & se == 0, na.rm = T),
            n_est = n(),
            n_nas = sum(is.na(total)),
            total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(across(c(total, se),
                ~ if_else(avg_op == 0,
                          NA_real_,
                          .))) %>%
  ggplot(aes(x = date,
             y = total,
             color = time_scale,
             fill = time_scale)) +
  geom_hline(yintercept = 0,
             linetype = 2,
             color = "gray40") +
  geom_ribbon(aes(ymin = qnorm(0.025, total, se),
                  ymax = qnorm(0.975, total, se)),
              alpha = 0.2,
              color = NA) +
  geom_line() +
  # scale_color_brewer(palette = "Set1",
  #                    name = "Model") +
  # scale_fill_brewer(palette = "Set1",
  #                    name = "Model") +
  scale_color_viridis_d(name = "Model",
                        end = 0.8) +
  scale_fill_viridis_d(name = "Model",
                        end = 0.8) +
  facet_wrap(~ year,
             ncol = 1,
             scales = "free") +
  labs(x = "Date",
       y = "Net Upstream Fish")

```

```{r est-corr}
cor_tab <- fish_cnts %>%
  group_by(time_scale,
           year,
           date) %>%
  summarize(n_full_hr = sum(is.na(se)),
            total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  select(-n_full_hr,
         -se) %>%
  pivot_wider(names_from = time_scale,
              values_from = total) %>%
  select(-year,
         -date) %>%
  corrr::correlate(use = "pairwise.complete.obs",
                   method = "pearson",
                   quiet = T)

```


## Excluding Bull Trout

Figure \@ref(fig:fig-glm) shows the fitted GLM that predicts the probability of being a steelhead based on a fish's length. Note that a 67 cm long fish would have a `r round(predict(fl_mod, newdata = tibble(fl_z = (67 - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd)), type = "response"), 3) * 100`% of being a steelhead with this model.

```{r fig-glm, fig.cap = "Points show the fork length of steelhead (along the top) and non-steelhead (along the bottom), with the fitted binomial GLM in red. The dashed line shows where fish greater than that would have a greater than 50% probability of being a steelhead. The dotted line shows the 67 cm threshold for which fish we will be applying this model to."}
spp_fl %>%
  mutate(spp = recode(spp,
                      "Resident RB" = "Bull Trout"),
         across(spp,
                fct_drop)) %>%
  mutate(spp_fct = as.numeric(spp) - 1) %>%
  ggplot(aes(fork_length,
             spp_fct)) +
  geom_point() +
  geom_vline(xintercept = cut_pt,
             linetype = 2) +
  geom_vline(xintercept = 67,
             linetype = 3) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "cauchit")),
  #             aes(color = "cauchit",
  #                 fill = "cauchit"),
  #             alpha = 0.2) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "probit")),
  #             aes(color = "probit",
  #                 fill = "probit"),
  #             alpha = 0.2) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "cloglog")),
  #             aes(color = "cloglog",
  #                 fill = "cloglog"),
  #             alpha = 0.2) +
  geom_smooth(method = glm,
              formula = y ~ x,
              method.args = list(family = binomial),
              aes(color = "logit",
                  fill = "logit"),
              alpha = 0.2) +
  scale_color_brewer(palette = "Set1",
                     name = "Link Fnc.") +
  scale_fill_brewer(palette = "Set1",
                    name = "Link Fnc.") +
  theme(legend.position = "none") +
  labs(y = "Probability of Being a Steelhead",
       x = "Fork Length (cm)")

```

Applying this model and rule-set to all the observed fish smaller than or equal to 67 cm, including the predictive model to expand 30 minute counts to full hour counts, a number of additional steelhead are added to our estimate each year (Table \@ref(tab:sm-est)). The total estimates (including all fish larger than 67 cm, as well as fish less than or equal to 67 cm that are predicted to be steelhead) are shown in Table \@ref(tab:tot-est). 

```{r sm-est}
fish_small %>%
  group_by(time_scale, 
           year) %>%
  summarize(total = sum(total),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_val = paste0(round(total), " (",
                           round(se, 1),
                           ")")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = year,
              values_from = prnt_val) %>%
  rename(`Time Scale` = time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        digits = c(0, 0, 0, 2),
        caption = "Estimates (SE) of total net upstream steelhead smaller than 67 cm, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling()

```

```{r tot-est}
fish_est %>%
  group_by(time_scale, 
           year) %>%
  summarize(total = sum(total),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_val = paste0(round(total), " (",
                           round(se, 1),
                           ")")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = year,
              values_from = prnt_val) %>%
  rename(`Time Scale` = time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        digits = c(0, 0, 0, 2),
        caption = "Estimates (SE) of total net upstream steelhead, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling()

```


## Missing Data

Figure \@ref(fig:oper-fig) shows the periods when the sonar array was not operating, and Figure \@ref(fig:miss-ts) shows how that impacts the time-series of fish counts. Note the large period in 2020 when the sonar was shut down due to COVID-19.

As the temporal scale on which counts are aggregated increases, the amount of missing data decreases. For example, if three hours are missing within a day, we can expand the rest of the day's counts by the percent of time the sonar was operational, so that day will not be "missing" at the day time-scale, although those three hours still are if we are operating on an hour time-scale. 

```{r oper-fig, fig.cap = "Purple depicts hours when the sonar was working, while yellow indicates the sonar was not functioning."}
hr_periods %>%
  mutate(hr = as.numeric(hour) / (60*60)) %>%
  mutate(date = as.Date(paste(month(date), mday(date)), format = "%m %d")) %>%
  ggplot(aes(x = hr,
             y = date,
             color = operational,
             fill = operational)) +
  geom_tile() +
  scale_fill_viridis_d(direction = -1,
                       name = "Sonar\nOperational") +
  scale_color_viridis_d(direction = -1,
                        name = "Sonar\nOperational") +
  facet_wrap(~ year,
             scales = "fixed") +
  scale_y_date(breaks = breaks_pretty(7)) +
  labs(y = "Date",
       x = "Hour") +
  theme(legend.position = "bottom")
```

```{r miss-ts, fig.height = 6, fig.cap = "Time series of upstream fish in blue, with missing data highlighted in red."}
p_list = vector("list",
                length = nrow(ts_df))
for(i in seq_along(ts_df$time_scale)) {
  p_list[[i]] <- ggplot_na_distribution(ts_df$ts_zoo[[i]]) +
    labs(title = paste(ts_df$time_scale[i], "in", ts_df$year[i]),
         subtitle = element_blank()) +
    theme(axis.title.y = element_blank())
}
ggarrange(plotlist = p_list,
          ncol = 3,
          nrow = 3,
          common.legend = T,
          legend = "bottom")
```

After interpolating across the missing data, Table \@ref(tab:missing-tab) displays how many fish were added to each year's estimate, based on the temporal scale and the interpolation method. Table \@ref(tab:abund-est-missing) provides final estimates of total net upstream steelhead each year, including fish smaller than 67 cm and periods of missing data, split out by the temporal scale the data was summarized on. 

```{r missing-tab}
all_preds %>%
  filter(is.na(total)) %>%
  group_by(time_scale,
           year,
           model) %>%
  summarize(pred_tot = sum(est, na.rm = T),
            pred_se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_value = paste0(round(pred_tot), " (", round(pred_se, 1), ")")) %>%
  select(-starts_with("pred_")) %>%
  pivot_wider(names_from = "model",
              values_from = prnt_value) %>%
  right_join(expand(all_preds,
                    time_scale, year)) %>%
  arrange(year,
          time_scale) %>%
  kable(col.names = c("Time Scale",
                      "Year",
                      "Kalman",
                      "Linear",
                      "MA"),
        booktabs = T,
        linesep = "",
        caption = "Estimates (SE) of how many net upstream steelhead are added to the totals from periods with wholly missing data. Interpolation methods include the Kalman filter, linear model, and moving average. There are three estimates per year, corresponding to the three different temporal scales. Blank cells indicate no interpolation was necessary for that year / temporal scale combination.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```


```{r abund-est-missing}
all_preds %>%
  filter(!is.na(total)) %>%
  group_by(time_scale,
           year) %>%
  summarize(obs_tot = sum(total, na.rm = T),
            obs_se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  full_join(all_preds %>%
              filter(is.na(total)) %>%
              group_by(time_scale,
                       year,
                       model) %>%
              summarize(pred_tot = sum(est, na.rm = T),
                        pred_se = sqrt(sum(se^2, na.rm = T)),
                        .groups = "drop")) %>%
  mutate(across(model,
                replace_na,
                "none"),
         across(starts_with("pred"),
                replace_na,
                0)) %>%
  mutate(total = obs_tot + pred_tot,
         se = sqrt(obs_se^2 + pred_se^2)) %>%
  mutate(None = paste0(round(obs_tot), " (", round(obs_se, 1), ")"),
         prnt_value = paste0(round(total), " (", round(se, 1), ")")) %>%
  select(-starts_with("obs"),
         -starts_with("pred")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = "model",
              values_from = prnt_value) %>%
  select(-none) %>%
  arrange(year,
          time_scale) %>%
  kable(col.names = c("Time Scale",
                      "Year",
                      "None",
                      "Kalman",
                      "Linear",
                      "MA"),
        booktabs = T,
        linesep = "",
        caption = "Estimates (SE) of total net upstream steelhead, using only extremely confident observations, and after interpolating counts for periods of missing data. Interpolation methods include no interpolation, the Kalman filter, linear model, and moving average. There are three estimates per year, corresponding to the three different temporal scales.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)

```

\newpage
# Discussion Points

* What to do with rows where `data_recorded ` is "Partial"? This includes `r sum(sonar_raw$data_recorded  == "Partial")` rows, or `r round(sum(sonar_raw$data_recorded  == "Partial") / nrow(sonar_raw) * 100, 1)`% of the data. Exclude and treat as missing data? Or is there a better way to parse this? Currently I've filtered it out and treated it as missing.
* What should we do with observations with confidence of 2 or 3? They are currently excluded completely.


## Expanding 30 min to 60 min

* The regression between counts in the first hour and the entire hour shows a consistent expectation that the counts in the second part of the hour will be less than counts in the first part. This holds regardless of whether we aggregate data by hour, day or something in between. 
* Currently I'm calculating the net upstream totals for each half hour period before running the regressions. If there's a compelling reason to either run separate regressions for downstream and upstream moving fish, or treat downstream and upstream fish from the same hour as two distinct data points, let's talk about that.
  * We could run separate regressions for each year, but if the methodology is the same year-to-year, I don't see why we'd expect different results.
  * Three years is not enough to use year as a random effect, but perhaps in a few more years we could explore this as an option.
  
## Excluding Bull Trout

* Should we assume any fish smaller than 40 cm is a bull trout, since that was the smallest steelhead length recorded? Under the current method, this doesn't matter because any fish smaller than `r round(cut_pt, 1)` cm is excluded from the steelhead counts.

* Incorporating some kind of spline or curve to account for steelhead run timing could improve this model. For example a 55 cm long fish might be considered a non-steelhead early or late in the season, but could be predicted to be a steelhead in the middle of the run, because it may be more likely that any fish is a steelhead then. To fit this model, I'll need the date when each fish's length was taken as part of the species composition data.


## Missing Data

* Any of the interpolation models we tested (Kalman filter, linear regression or moving average) resulted in larger estimates of steelhead moving upstream (Table \@ref(tab:abund-est-missing)), but differed in which one provide the biggest increase depending on the time-scale and year.
* The uncertainty (e.g. standard error) grew when incorporating those missing data, which is appropriate. The uncertainty grew substantially in 2020, when there was a large period of missing data due to COVID restrictions.
* Depending on how big the missing data gaps are, and the time-scale we are aggregating data on, there were some years and time-scales with no missing data (e.g. 24 hour scale in 2021). The lack of missing data relies on using the percentage of hours when sonar was operational within each time-step to increase the estimates for any time-steps when the operational time was less than 100%.
  * The alternative to expanding time-steps when the sonar was partially operational is to remove all data from those time-steps and treat them as missing data. 
  * It's unclear to me whether that would have a substantial impact on the overall estimates or uncertainty.

