---
title: "Estimates of Steelhead in Dungeness River"
subtitle: "Using Sonar"
author:
  - Kevin See:
      email: Kevin.See@dfw.wa.gov
      institute: [wdfw]
      correspondence: true
  # - Bethany Craig:
  #     email: Bethany.Craig@dfw.wa.gov
  #     institute: [wdfw]
  #     correspondence: false
institute:
  - wdfw: Washington Department of Fish & Wildlife
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
    wdfwTemplates::wdfw_html_format2:
      fig_caption: yes
      fig_height: 4
      fig_width: 6
      toc: yes
      toc_depth: 3
      toc_float:
        collapsed: yes
        smooth_scroll: yes
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::pdf_document2:
      fig_caption: yes
      fig_height: 5
      fig_width: 6
      toc: yes
      includes:
        in_header: ../templates/header_WDFW.tex
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks2.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::word_document2:
      fig_caption: yes
      fig_height: 4
      fig_width: 6
      toc: yes
      reference_docx: "../templates/ReportTemplate.docx" # Insert path for the DOCX file
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
bibliography:
  # - AUC.bib
  - references.bib
csl: "../templates/american-fisheries-society.csl" # Insert path for the bib-style
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/",
  dpi = 300
)
```

```{r packages}
# load these packages
library(tidyverse)
library(here)
library(magrittr)
library(janitor)
library(lubridate)
library(splines)
library(mgcv)
library(ggfortify)
library(ggpubr)
library(scales)
library(kableExtra)

theme_set(theme_bw())

# knitr options
options(knitr.kable.NA = '-')

# when knitting to Word, use this
# what kind of document is being created?
doc.type <- knitr::opts_knit$get('rmarkdown.pandoc.to')

if(doc.type == 'docx') {
  options(knitr.table.format = "pandoc")
}

```

\newpage
# Goals

To estimate the number of adult steelhead spawners in the Dungeness river, with appropriate uncertainty, based upon a sonar system set up in the lower river. This involves several sub-tasks:

1. Expand 30 min observations into 60 min estimates. Most hours only the first 30 min of video was analyzed, but for a subset of hours the entire hour was analyzed. Develop a crosswalk between the first 30 min and the entire hour, and then predict the total counts from all the hours when only the first 30 min are available, with uncertainty from this crosswalk.
1. Exclude bull trout from images based on species composition data. 
1. Account for outages. There are periods when the sonar unit was not functioning for a variety of reasons. Develop a method to interpolate across those periods of missing data.


# Methods


```{r read-data}
sonar_raw <- read_csv(here("analysis/data/raw_data",
                           "2019 sonar.csv")) %>%
  mutate(across(Hour,
                hms)) %>%
  bind_rows(read_csv(here("analysis/data/raw_data",
                          "2020 sonar.csv")) %>%
              mutate(across(Hour,
                            str_pad,
                            width = 5,
                            side = "left",
                            pad = "0")) %>%
              mutate(across(Hour,
                            hm)) %>%
              rename(comments = `Comments/Notes`) %>%
              clean_names("upper_camel")) %>%
  bind_rows(read_csv(here("analysis/data/raw_data",
                          "2021 sonar.csv")) %>%
              mutate(across(Hour,
                            ~ hm(paste(str_sub(., 1,2),
                                       str_sub(., 3, 4),
                                       sep = ":")))) %>%
              rename(comments = `Comments/Notes`) %>%
              clean_names("upper_camel")) %>%
  filter(!is.na(Year)) %>%
  mutate(across(Date,
                mdy)) %>%
  mutate(across(DataReviewed,
                recode,
                "No Data" = "No data")) %>%
  mutate(across(DataRecorded,
                recode,
                "No Data" = "No data")) %>%
  mutate(across(Time,
                recode,
                "No Data" = "No data",
                "no fish" = "No fish",
                "No Fish" = "No fish"),
         Time = if_else(str_detect(Comments, "No data"),
                        "No data",
                        Time),
         across(Direction,
                recode,
                "Downstream" = "downstream",
                "Upstream" = "upstream")) %>%
  clean_names() %>%
  mutate(date_time = date + hour) %>%
  arrange(date_time) %>%
  # drop any detections after June 1
  filter(month(date) < 6)

# drop a couple records because they don't match
# one upstream and one downstream fish recorded during the 2nd half hour,
# but no data in teh first half hour, at all
sonar_raw %<>%
  filter(!(date == ymd(20200322) &
             hour == "30M 0S"))

sonar_raw %<>%
  filter(!(date == ymd(20200301) &
             (str_detect(hour, "^0S") |
                str_detect(hour, "^30M") |
                str_detect(hour, "^1H"))))


# add one missing row (assuming no fish detected)
sonar_raw %<>%
  bind_rows(sonar_raw %>%
              filter(date == ymd(20190320),
                     str_detect(hour, "15H")) %>%
              mutate(hour = hour + minutes(30),
                     date_time = date + hour,
                     data_reviewed = "Second 30",
                     across(c(direction),
                            ~ NA),
                     across(c(length:confidence),
                            ~ NA_real_))) %>%
  arrange(date_time)

# pull out records of fish detections
sonar_fish <- sonar_raw %>%
  filter(data_recorded != "Partial") %>%
  filter(!is.na(frame)) %>%
  mutate(sthd_length = if_else(length > 67, T, F)) %>%
  mutate(across(time,
                hms))

#--------------------------------------------------
# which hours do we want to group together?
#--------------------------------------------------

# # 12 hour blocks
# hrs_fct_grp <- rep(1:2, each = 12) |>
#   set_names(0:23)

# # 8 hour blocks
# hrs_fct_grp <- rep(1:3, each = 8) |>
#   set_names(0:23)

# 6 hour blocks
hrs_fct_grp <- rep(1:4, each = 6) |>
  set_names(0:23)

# # 4 hour blocks
# hrs_fct_grp <- rep(1:6, each = 4) |>
#   set_names(0:23)

# # 2 hour blocks
# hrs_fct_grp <- rep(1:12, each = 2) |>
#   set_names(0:23)

hr_periods <- sonar_raw %>%
  group_by(year) %>%
  summarize(across(date_time,
                   list(min = min,
                        max = max),
                   na.rm = T,
                   .names = "{.fn}"),
            .groups = "drop") %>%
  mutate(date_time = map2(min,
                     max,
                     .f = function(x, y) seq(x, y, by = 30*60))) %>%
  select(-min, -max) %>%
  unnest(date_time) %>%
  mutate(date = floor_date(date_time,
                           unit = "day"),
         time = difftime(date_time, date,
                         units = "mins"),
         across(time,
                as.period)) %>%
  left_join(sonar_raw %>%
              filter(data_recorded != "Partial") %>%
              select(year, date, time = hour,
                     data_recorded,
                     data_reviewed) %>%
              distinct() %>%
              mutate(reviewed = TRUE)) %>%
  mutate(across(reviewed,
                replace_na,
                FALSE)) %>%
  mutate(reviewed = if_else(!is.na(data_reviewed) & data_reviewed == "No data",
                            F, reviewed)) %>%
  mutate(operational = if_else(reviewed |
                                 (is.na(data_recorded) & lag(data_recorded) %in% c("Full")),
                               T, F)) %>%
  mutate(dt = floor_date(date_time,
                         unit = "hours"),
         hour = difftime(dt, date,
                         units = "hours"),
         across(hour,
                as.period)) %>%
  select(-dt) %>%
  mutate(hr_fct = hrs_fct_grp[as.character(hour(date_time))]) %>%
  relocate(hour, hr_fct, .after = "time") %>%
  # fix one data_reviewed entry
  mutate(data_reviewed = if_else(str_detect(time, "30M") & 
                                   data_recorded == "Full" &
                                   data_reviewed == "First 30",
                                 "Second 30",
                                 data_reviewed))

#--------------------------------------------------
# species composition data
#--------------------------------------------------
spp_comp <- read_csv(here("analysis/data/raw_data",
                          "species comp.csv")) %>%
  clean_names() %>%
  mutate(across(date,
                mdy)) %>%
    select(date:sthd, notes)

# # older version
# spp_fl <- readxl::read_excel(here("analysis/data/raw_data",
#                                   "Species Comp ALL.xlsx"),
#                              "2021",
#                              range = "S3:U38") %>%
#   mutate(across(everything(),
#                 ~ if_else(. > 100,
#                           . / 10, .))) %>%
#   pivot_longer(everything(),
#                names_to = "spp",
#                values_to = "fork_length",
#                values_drop_na = T) %>%
#   mutate(across(spp,
#                 as_factor)) %>%
#   mutate(fl_mean = mean(fork_length),
#          fl_sd = sd(fork_length),
#          fl_z = (fork_length - fl_mean) / fl_sd)

spp_fl <- readxl::read_excel(here("analysis/data/raw_data",
                                  "Species Comp ALL.xlsx"),
                             "2021 lengths") %>%
  clean_names() %>%
  filter(str_detect(survey_type,
                    "Gill Net")) %>%
  mutate(spp = recode(species,
                      "Resident rainbow" = "Resident RB")) %>%
  select(spp,
         survey_date,
         fork_length) %>%
  mutate(across(spp,
                as_factor)) %>%
  mutate(fl_mean = mean(fork_length),
         fl_sd = sd(fork_length),
         fl_z = (fork_length - fl_mean) / fl_sd)
```


```{r}
# generate percent of each time step sonar was operational
half_hr_op = hr_periods %>%
  group_by(year, 
           date,
           time) %>%
  summarize(across(date_time,
                   min),
            tot_pers = n(),
            op_pers = sum(operational),
            op_perc = op_pers / tot_pers,
            .groups = "drop")

hr_op <- hr_periods %>%
  group_by(year, 
           date,
           hour,
           hr_fct) %>%
  summarize(across(date_time,
                   min),
            n_hrs = n_distinct(hour),
            tot_pers = n(),
            op_pers = sum(operational),
            op_perc = op_pers / tot_pers,
            .groups = "drop")

hrs_op <- hr_periods %>%
  group_by(year, 
           date,
           hr_fct) %>%
  summarize(across(date_time,
                   min),
            n_hrs = n_distinct(hour),
            tot_pers = n(),
            op_pers = sum(operational),
            op_perc = op_pers / tot_pers,
            .groups = "drop") %>%
  left_join(hrs_fct_grp %>%
              enframe(name = "hour",
                      value = "hr_fct") %>%
              mutate(across(hour,
                            as.numeric)) %>%
              group_by(hr_fct) %>%
              summarize(across(hour,
                               min))) %>%
  mutate(across(hour,
                as.period,
                unit = "hour"),
         date_time = date + hour) %>%
  relocate(hour,
           .after = "date")

day_op <- hr_periods %>%
  group_by(year, date) %>%
  summarize(across(date_time,
                   min),
            n_hrs = n_distinct(hour),
            tot_pers = n(),
            op_pers = sum(operational),
            op_perc = op_pers / tot_pers,
            .groups = "drop") %>%
  mutate(hour = 0,
         across(hour,
                as.period,
                unit = "hour"),
         hr_fct = as.integer(1)) %>%
  relocate(hour, hr_fct,
           .after = "date")

# wk_op <- hr_periods %>%
#   mutate(wk_grp = week(date_time)) %>%
#   group_by(year, wk_grp) %>%
#   summarize(first_day = min(date),
#             tot_hrs = n(),
#             op_hrs = sum(operational),
#             op_perc = op_hrs / tot_hrs,
#             .groups = "drop")


ops_df <- tibble(time_scale = as_factor(c('Hour',
                                          paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                          'Day')),
                 ops = list(hr_op,
                            hrs_op,
                            day_op))

```

## Expanding 30 min to 60 min

```{r mod-data}
# add counts of upstream, downstream and net large fish to hr_periods
ts_df <- hr_periods %>%
  left_join(sonar_fish %>%
              filter(confidence == 1) %>%
              group_by(year,
                       date,
                       time = hour,
                       sthd_length) %>%
              count(direction) %>%
              ungroup() %>%
              filter(direction %in% c("upstream",
                                      "downstream")) %>%
              mutate(across(direction,
                            recode,
                            "upstream" = "up",
                            "downstream" = "down")) %>%
              pivot_wider(names_from = direction,
                          values_from = n,
                          values_fill = 0)) %>%
  mutate(across(c(up, down),
                ~ if_else(is.na(.) & reviewed, as.integer(0), .))) %>%
  mutate(net = up - down)

# which hours had the 2nd 30 min reviewed?
full_hrs <- hr_periods %>%
  filter(data_reviewed %in% c("First 30",
                              "Second 30")) %>%
  group_by(date, hour) %>%
  mutate(n_pers = n(),
         n_first = sum(str_detect(data_reviewed, "First")),
         n_second = sum(str_detect(data_reviewed, "Second"))) %>%
  filter(n_pers == 2) %>%
  arrange(date_time) %>%
  ungroup() %>%
  select(all_of(names(hr_periods)))

# period comparison for all hours with first and second half hours fully recorded
# comp_30_df <- ts_df %>%
#   filter(sthd_length) %>%
#   filter(data_reviewed == "Second 30") %>%
#   select(date_time) %>%
#   mutate(across(date_time,
#                 ~ . + dminutes(-30))) %>%
#   left_join(ts_df %>%
#               filter(sthd_length)) %>%
#   bind_rows(ts_df %>%
#               filter(sthd_length) %>%
#               filter(data_reviewed == "Second 30")) %>%
#   filter(reviewed) %>%
#   filter(data_reviewed %in% c("First 30",
#                               "Second 30")) %>%
#   arrange(date_time) %>% 
#   mutate(date_time = date + hour) %>%
#   select(year, 
#          date_time,
#          date, 
#          hour,
#          hr_fct,
#          sthd_length,
#          data_reviewed,
#          up, down,
#          net) %>%
#   mutate(across(data_reviewed,
#                 recode,
#                 "First 30" = "first",
#                 "Second 30" = "second")) %>%
#   pivot_longer(cols = c(up, down, net),
#                names_to = "direction",
#                values_to = "n_fish") %>%
#   pivot_wider(names_from = data_reviewed,
#               names_sort = T,
#               values_from = n_fish,
#               values_fill = 0)

comp_30_df <- full_hrs %>%
  left_join(ts_df %>%
              filter(sthd_length)) %>%
  mutate(across(c(up, down, net),
                replace_na,
                0)) %>%
  mutate(dt = date + hour,
         hr_grp = factor(dt,
                         labels = 1:n_distinct(dt))) %>%
  group_by(hr_grp) %>%
  mutate(tot_fish = sum(up, down, na.rm = T)) %>%
  ungroup() %>%
  filter(tot_fish > 0) %>%
  mutate(date_time = date + hour) %>%
  select(year, 
         date_time,
         date, 
         hour,
         hr_fct,
         data_reviewed,
         up, down,
         net) %>%
  mutate(across(data_reviewed,
                recode,
                "First 30" = "first",
                "Second 30" = "second")) %>%
  pivot_longer(cols = c(up, down, net),
               names_to = "direction",
               values_to = "n_fish") %>%
  pivot_wider(names_from = data_reviewed,
              values_from = n_fish)

# group data by different time-scales
per_fit_df <- tibble(time_scale = as_factor(c('Hour',
                                          paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                          'Day')),
                 model_df = list(comp_30_df %>%
                                   mutate(fct_group = date + hour,
                                          across(fct_group,
                                                 ~ factor(as.character(.),
                                                          labels = 1:n_distinct(.)))) %>%
                                   group_by(date, 
                                            direction, 
                                            fct_group) %>%
                                   summarize(across(date_time,
                                                       min),
                                             across(c(first, 
                                                      second),
                                                    sum),
                                             .groups = "drop"),
                                 
                                 comp_30_df %>%
                                   mutate(fct_group = date + (hr_fct * 24 / max(hrs_fct_grp)),
                                          across(fct_group,
                                                 ~ factor(as.character(.),
                                                          labels = 1:n_distinct(.)))) %>%
                                   group_by(date, 
                                            direction,
                                            hr_fct,
                                            fct_group) %>%
                                   summarize(across(date_time,
                                                       min),
                                             across(c(first, 
                                                      second),
                                                    sum),
                                             .groups = "drop") %>%
                                   left_join(hrs_fct_grp %>%
                                               enframe(name = "hour",
                                                       value = "hr_fct") %>%
                                               mutate(across(hour,
                                                             as.numeric)) %>%
                                               group_by(hr_fct) %>%
                                               summarize(across(hour,
                                                                min))) %>%
                                   mutate(across(hour,
                                                 as.period,
                                                 unit = "hour"),
                                          date_time = date + hour),
                                 
                                 comp_30_df %>%
                                   mutate(fct_group = date,
                                          across(fct_group,
                                                 ~ factor(as.character(.),
                                                          labels = 1:n_distinct(.)))) %>%
                                   group_by(date, 
                                            direction, 
                                            fct_group) %>%
                                   summarize(across(date_time,
                                                       min),
                                             across(c(first, 
                                                      second),
                                                    sum),
                                             .groups = "drop") %>%
                                   mutate(across(date_time,
                                                 floor_date,
                                                 unit = "days")))) %>%
  unnest(model_df) %>%
  mutate(total = first + second) %>%
  mutate(across(direction,
                factor,
                levels = c("up",
                           "down",
                           "net"))) %>%
  arrange(time_scale,
          direction) %>%
  nest(model_df = -c(time_scale,
                    direction))
```

We started by examining the counts during the first 30 minutes and the entire hour. First, we filtered for records with a confidence level of 1 (extremely confident) and a length greater than 67 cm to ensure we were only comparing records of steelhead. For each hour with both halves recorded and for each half hour, we summed the fish determined to be moving upstream, and those moving downstream, and calculated the number of net upstream fish (upstream - downstream). We then added the two half hours together to provide a number of the net upstream fish for that hour. 

We compared the first half hour with the entire hour at several temporal scales. We started with a single hour, then also summed net upstream fish in `r 24 / max(hrs_fct_grp)` hour blocks, and finally summed the net upstream fish by date. The data was structured such that hours where both half hours were examined were usually consecutive at least up to the day scale, meaning each `r 24 / max(hrs_fct_grp)` hour block and each day had nearly identical amounts of time with two half hours to other periods at the same temporal scale.

For each temporal scale grouping and direction (up, down and net upstream), we fit a linear model with the counts of fish in the first 30 minutes as the independent variable and the total count of fish in the entire hour as the dependent variable. In each model, we fixed the intercept at 0, to ensure that if no fish were counted in the first half hour, we would expand that to zero fish for the entire hour. We focused on the estimated slope, hypothesizing that it should be 2. 

```{r fit-models}
per_fit_df %<>%
  mutate(mod = map(model_df,
                   .f = function(x) {
                     lm(total ~ first - 1,
                        data = x)
                   }),
         coefs = map(mod,
                     .f = broom::tidy),
         ci = map(mod,
                  .f = function(x) {
                    ci = confint(x)
                    if(class(ci)[1] == "matrix") {
                      ci %<>%
                        as_tibble()
                    } else {
                      ci %<>%
                        enframe() %>%
                        pivot_wider()
                    }
                    return(ci)
                  })) %>%
  mutate(slope = map_dbl(coefs,
                         "estimate"),
         se = map_dbl(coefs,
                         "std.error"),
         sigma = map_dbl(mod,
                     .f = function(x) {
                       summary(x)$sigma
                     }),
         R2 = map_dbl(mod,
                     .f = function(x) {
                       summary(x)$r.squared
                     }),
         adj_R2 = map_dbl(mod,
                          .f = function(x) {
                            summary(x)$adj.r.squared
                          })) %>%
  unnest(ci) %>%
  relocate(ends_with("%"),
           .after = "se")

```

```{r k-fold-cv, eval = T}
# how many folds?
k_folds <- 20

# split data into training and test samples by day.
# predict whole day and calculate forecast statistics
cv_res = NULL
for(i in 1:k_folds) {
  
  set.seed(i * 10 + 6)
  train_dates <- sort(sample(unique(comp_30_df$date),
                             0.7 * n_distinct(comp_30_df$date)))
  
  cv_df <- per_fit_df %>%
    mutate(train_df = map(model_df,
                          .f = function(x) {
                            x %>%
                              filter(date %in% train_dates)
                          }),
           test_df = map(model_df,
                         .f = function(x) {
                           x %>%
                             filter(!date %in% train_dates)
                         }),
           mod = map(train_df,
                     .f = function(x) {
                       lm(total ~ first - 1,
                          data = x)
                     }),
           preds = map2(mod,
                        test_df,
                        .f = function(x, y) {
                          y %>%
                            bind_cols(predict(x,
                                              newdata = y,
                                              se.fit = T) %>%
                                        as_tibble() %>%
                                        select(pred = fit,
                                               se = se.fit))
                        }))
  
  res <- cv_df %>%
    select(time_scale, 
           direction,
           preds) %>%
    unnest(preds) %>%
    mutate(year = year(date)) %>%
    group_by(time_scale,
             year,
             date,
             direction) %>%
    summarize(across(c(first, second,
                       total, pred),
                     sum),
              across(se,
                     ~ sum(.^2)),
              .groups = "drop") %>%
    mutate(error = pred - total) %>%
    filter(total != 0) %>%
    group_by(time_scale,
             direction) %>%
    summarize(mean_bias = mean(error),
              median_bias = median(error),
              rmse = sqrt(mean(error^2)),
              # rmse_perc = rmse / mean(total),
              mae = mean(abs(error)),
              # mae_perc = sum(abs(error)) / sum(total),
              mape = mean(abs(error) / total),
              .groups = "drop") %>%
    add_column(fold = i,
               .before = 0)
  
  if(i == 1) {
    cv_res <- res
  } else {
    cv_res %<>%
      bind_rows(res)
  }
}

```

We also performed `r k_folds`-fold cross validation. For each cross-validation, we created a training dataset by sampling all the data from 70% of the days with full hours reviewed, and held out the remaining 30% as a test dataset. We fit the same linear regression to each training dataset, and used it to predict the total net upstream steelhead for each day in the test dataset. We then summarized the mean bias, root mean squared error (RMSE), mean absolute error (MAE) and mean absolute percent error (MAPE) for each temporal scale. To calculate these, first we calculated the error for each day, $e_t$, as the predicted minus the observed.

$$
e_t = pred_t - obs_t
$$
The mean bias is simply the average $e_t$. RMSE is defined as

$$
RMSE = \sqrt{\frac{1}{n}\sum{e_t^2}}
$$
MAE is

$$
MAE = \frac{1}{n}\sum{|e_t|}
$$
MAPE is the average of percent errors, and is defined as:

$$
MAPE = \frac{1}{n}\sum{\frac{|e_t|}{obs_t}}
$$

```{r make-predictions-big-fish}
# what fish were observed outside the full hour periods?
other_big_fish <- hr_periods %>%
  anti_join(comp_30_df %>%
              select(date, hour) %>%
              distinct(),
            by = c("date", "hour")) %>%
  filter(reviewed) %>%
  left_join(ts_df %>%
              filter(sthd_length)) %>%
  filter(sthd_length) %>%
  pivot_longer(cols = c(up,
                        down,
                        net),
               names_to = "direction",
               values_to = "first") %>%
  select(year:hr_fct,
         direction,
         first)
  
pred_big_other <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     pred_df = list(other_big_fish %>%
                                      mutate(fct_group = date + hour,
                                             across(fct_group,
                                                    ~ factor(as.character(.),
                                                             labels = 1:n_distinct(.)))) %>%
                                      group_by(date, 
                                               direction, 
                                               fct_group) %>%
                                      summarize(across(date_time,
                                                       min),
                                                across(c(first),
                                                       sum),
                                                .groups = "drop"),
                                    
                                    other_big_fish %>%
                                      mutate(fct_group = date + hours(hr_fct * 24 / max(hrs_fct_grp)),
                                             across(fct_group,
                                                    ~ factor(as.character(.),
                                                             labels = 1:n_distinct(.)))) %>%
                                      group_by(date, 
                                               direction, 
                                               hr_fct,
                                               fct_group) %>%
                                      summarize(across(date_time,
                                                       min),
                                                across(c(first),
                                                       sum),
                                                .groups = "drop") %>%
                                      left_join(hrs_fct_grp %>%
                                               enframe(name = "hour",
                                                       value = "hr_fct") %>%
                                                 mutate(across(hour,
                                                               as.numeric)) %>%
                                                 group_by(hr_fct) %>%
                                                 summarize(across(hour,
                                                                  min)),
                                               by = "hr_fct") %>%
                                      mutate(across(hour,
                                                    as.period,
                                                    unit = "hour"),
                                             date_time = date + hour),
                                    
                                    other_big_fish %>%
                                      mutate(fct_group = date,
                                             across(fct_group,
                                                    ~ factor(as.character(.),
                                                             labels = 1:n_distinct(.)))) %>%
                                      group_by(date, 
                                               direction, 
                                               fct_group) %>%
                                      summarize(across(date_time,
                                                       min),
                                                across(c(first),
                                                       sum),
                                                .groups = "drop") %>%
                                      mutate(date_time = floor_date(date_time,
                                                                    unit = "days")) %>%
                                      mutate(hr_fct = as.integer(1)))) %>%
  unnest(pred_df) %>%
  nest(pred_df = -c(time_scale,
                    direction)) %>%
  left_join(per_fit_df %>%
              select(time_scale,
                     direction,
                     mod),
            by = c("time_scale",
                   "direction")) %>%
  mutate(preds = map2(mod,
                      pred_df,
                      .f = function(x, y) {
                        predict(x,
                                newdata = y,
                                se.fit = T)
                      }),
         total = map(preds,
                   "fit"),
         se = map(preds,
                  "se.fit")) %>%
  select(-mod, -preds) %>%
  unnest(cols = c(pred_df, total, se)) %>%
  mutate(year = year(date),
         hour = hour(date_time),
         hr_fct = hrs_fct_grp[as.character(hour)],
         across(hour,
                as.period,
                unit = "hours"),
         sthd_length = T) %>%
  select(time_scale,
         direction,
         year,
         date,
         hour,
         hr_fct,
         sthd_length,
         everything()) %>%
  select(-fct_group) %>%
  add_column(est_type = "Half Hour",
             .before = "first")

```

```{r}
fish_big <- per_fit_df %>%
  select(time_scale, 
         direction,
         model_df) %>%
  unnest(model_df) %>%
  select(-fct_group) %>%
  mutate(year = year(date),
         hour = hour(date_time),
         hr_fct = hrs_fct_grp[as.character(hour)],
         across(hour,
                as.period,
                unit = "hours")) %>%
  mutate(hour = if_else(time_scale == "Day",
                        as.period(0, 
                                  unit = "hour"),
                        hour),
         hour = if_else(str_detect(hour, "24H"),
                        as.period(0,
                                  unit = "hour"),
                        hour),
         date_time = date + hour,
         sthd_length = T) %>%
  select(time_scale,
         direction,
         year,
         date,
         hour,
         hr_fct,
         sthd_length,
         everything()) %>%
  ungroup() %>%
  mutate(se = 0) %>%
  add_column(est_type = "Full Hour",
             .before = "first") %>%
  bind_rows(pred_big_other) %>%
  arrange(time_scale,
          direction,
          date_time)

```

## Excluding Bull Trout

Bull trout are swimming past the sonar unit as well as steelhead, and we need to parse which fish identified by the sonar are steelhead, and exclude any bull trout. The largest bull trout sampled in any species composition data was 67 cm, so we are assuming any fish larger than 67 cm detected on the sonar is a steelhead. It then remains to filter the fish equal to or less than 67 cm long from the sonar and determine what proportion of those are steelhead, and what are bull trout. 

We only have species composition data for one year, 2021. It was collected weekly, using tangle nets just upstream of the sonar location. For every fish caught, we know the date and fork length of that fish. Based on this data, we have also determined that the steelhead run on the Dungeness is over by June 1. Therefore, we have only made predictions for fish detected prior to June 1st. 

We have several options to model the probability of any particular fish, less than or equal to 67 cm, being a steelhead. we could model that probability as a factor of date (perhaps with a quadratic term to capture non-linearity), or as a factor of fork length, or both. If we use date, we can interpret the probability of being a steelhead as the proportion of all fish detected on that date that are steelhead. If we use fork length (or date and fork length), we can assign a probability to every fish detected by sonar, and assume that all fish with a probability greater than some threshold (probably 50%) are steelhead. 

<!-- Currently, we are only using fork length, because although the date of capture is probably available, it is not in the current data set. From the species composition netting, there are `r nrow(spp_fl)` fish caught with fork lengths. These can be seen in Figure \@ref(fig:fl-hist). Since we only care about differentiating steelhead, we grouped resident rainbows with bull trout, and then fit a binomial GLM with a logit link, using the fork length to predict the probability of a fish being a steelhead. We did not restrict the dataset to fish with fork lengths less than 67 cm, because larger fish have information about the shape of the logistic curve.  -->

<!-- After fitting this GLM, we predicted the probability of being a steelhead for all fish observed on the sonar that were smaller than or equal to 67 cm, based on their length. Any fish with a probability of 50% or greater we assigned to be a steelhead. We then applied the same model (Section \@ref(expanding-30-min-to-60-min)) to expand 30 minute counts for small fish to full hour counts. We added counts or estimates of large fish to small fish for each time period to estimate total net steelhead moving upstream for each time period. Estimates of large and small fish within the same time period were assumed to be independent when calculating the standard error. -->

We chose to use fork length and the Juilan day of capture. From the species composition netting, there are `r nrow(spp_fl)` fish to use in this model. These can be seen in Figures \@ref(fig:fl-hist) and \@ref(fig:fl-date). Since we only care about differentiating steelhead, we grouped resident rainbows with bull trout, and then fit a binomial GAM with a logit link, using splines of fork length and Julian day to predict the probability of a fish being a steelhead. We did not restrict the dataset to fish with fork lengths less than 67 cm, because larger fish have information about the shape of the logistic curve. 

After fitting this GAM, we predicted the probability of being a steelhead for all fish observed on the sonar that were smaller than or equal to 67 cm, based on their length and Julian day of observation. Any fish with a probability of 50% or greater we assigned to be a steelhead. We then applied the same model (Section \@ref(expanding-30-min-to-60-min)) to expand 30 minute counts for small fish to full hour counts. We added counts or estimates of large fish to small fish for each time period to estimate total net steelhead moving upstream for each time period. Estimates of large and small fish within the same time period were assumed to be independent when calculating the standard error.

```{r fl-hist, fig.cap = "Histogram of forklengths, colored by species."}
spp_fl %>%
  ggplot(aes(x = fork_length,
             fill = spp)) +
  geom_histogram(position = "dodge",
                 binwidth = 4) +
  scale_fill_brewer(palette = "Set1",
                     name = "Species") +
  labs(x = "Fork Length (cm)")
```

```{r fl-date, fig.cap = "Scatterplot of date of capture and forklength, colored by species."}
spp_fl %>%
  ggplot(aes(x = survey_date,
             y = fork_length,
             color = spp)) +
  geom_point() +
  scale_color_brewer(palette = "Set1",
                     name = "Species") +
  labs(x = "Date",
       y = "Fork Length (cm)")
```


```{r fl-gam, eval = T}
fl_mod <- spp_fl %>%
  mutate(spp = recode(spp,
                      "Resident RB" = "Bull Trout"),
         across(spp,
                fct_drop),
         jday = yday(survey_date)) %>%
  mutate(spp_fct = as.numeric(spp) - 1) %>%
  gam(spp_fct ~ s(fl_z, k = 5, m = 2, bs = "tp") + 
        s(jday, k = 5, m = 2, bs = "tp"),
      data = .,
      family = binomial)
  # glm(spp_fct ~ fl_z + jday + I(jday^2),
  #     data = .,
  #     family = binomial)

# what is the fork length when 50% of being a steelhead?
p_pred = 0.5
pred_tab <- crossing(length = seq(20, 80, by = 0.05),
                     survey_date = seq(ymd(20210203),
                                       ymd(20210601),
                                       by = "1 days")) %>%
  mutate(fl_z = (length - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd),
         jday = yday(survey_date)) %>%
  bind_cols(predict(fl_mod,
                    newdata = .,
                    type = "response",
                    se.fit = T) %>%
              as_tibble() %>%
              select(prob_sthd = fit,
                     prob_se = se.fit))

```


```{r fl-glm, eval = F}
fl_mod <- spp_fl %>%
  mutate(spp = recode(spp,
                      "Resident RB" = "Bull Trout"),
         across(spp,
                fct_drop)) %>%
  mutate(spp_fct = as.numeric(spp) - 1) %>%
  glm(spp_fct ~ fl_z,
      data = .,
      family = binomial)

# summary(fl_mod)
# # probability of being steelhead at average forklength (~ 55 cm)
# boot::inv.logit(coef(fl_mod)[1])
# # if fork length increases by 1 standard deviation (~ 13 cm), 
# # odds of being a steelhead increase by a factor of:
# exp(coef(fl_mod)[2])

# what is the fork length when 50% of being a steelhead?
p_pred = 0.5
cut_pt <- boot::logit(p_pred) %>%
  subtract(coef(fl_mod)[1]) %>%
  divide_by(coef(fl_mod)[2]) %>%
  multiply_by(unique(spp_fl$fl_sd)) %>%
  add(unique(spp_fl$fl_mean)) %>%
  as.numeric()
```



```{r pred-small-fish}
# predict species of all fish <= 67 cm
small_fish <- sonar_fish %>%
  filter(confidence == 1,
         !sthd_length) %>%
  select(year:hour, 
         time, 
         date_time,
         direction,
         length,
         frame:range) %>%
  mutate(fl_z = (length - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd)) %>%
  mutate(jday = yday(date_time)) %>%
  bind_cols(predict(fl_mod,
                    newdata = .,
                    type = "response",
                    se.fit = T) %>%
              as_tibble() %>%
              select(prob_sthd = fit,
                     prob_se = se.fit)) %>%
  mutate(p_alpha = prob_sthd^2 * ((1 - prob_sthd) / prob_se^2 - prob_sthd^-1),
         p_beta = p_alpha * (prob_sthd^-1 - 1))

# filter out fish deemed not steelhead and
# combine with some other information
small_sthd <- small_fish %>%
  filter(prob_sthd >= 0.5) %>%
  group_by(year,
           date,
           time = hour,
           date_time) %>%
  count(direction) %>%
  ungroup() %>%
  filter(direction %in% c("upstream",
                          "downstream")) %>%
  mutate(across(direction,
                recode,
                "upstream" = "up",
                "downstream" = "down")) %>%
  left_join(hr_periods %>%
               filter(reviewed) %>%
               select(year,
                      date, time,
                      hour,
                      hr_fct,
                      data_reviewed)) %>%
  left_join(full_hrs %>%
              select(year, date, hour) %>%
              distinct() %>%
              mutate(full_hour = T)) %>%
  mutate(across(full_hour,
                replace_na,
                F)) %>%
  arrange(date_time) %>%
  mutate(across(data_reviewed,
                recode,
                "First 30" = "first",
                "Second 30" = "second")) %>%
  pivot_wider(names_from = direction,
              values_from = n,
              values_fill = 0) %>%
  mutate(net = up - down) %>%
  pivot_longer(cols = c(up, down, net),
               names_to = "direction",
               values_to = "n") %>%
  select(-time,
         -date_time) %>%
  pivot_wider(names_from = data_reviewed,
              values_from = n) %>%
  mutate(across(c(first,
                  second),
                ~ if_else(is.na(.) & full_hour,
                          as.integer(0), .))) %>%
  mutate(date_time = date + hour) %>%
  select(year, date, hour, hr_fct, date_time,
         full_hour,
         direction,
         first,
         second) %>%
  add_column(sthd_length = F,
             .before = "full_hour")

# start with small steelhead counted during whole hour operations 
calc_small <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     full_sm_sthd = list(small_sthd %>%
                                           filter(full_hour) %>%
                                           mutate(total = first + second,
                                                  se = 0),
                                         
                                         small_sthd %>%
                                           filter(full_hour) %>%
                                           mutate(total = first + second) %>%
                                           mutate(fct_group = date + hours(hr_fct * 24 / max(hrs_fct_grp)),
                                                  across(fct_group,
                                                         ~ factor(as.character(.),
                                                                  labels = 1:n_distinct(.)))) %>%
                                           group_by(fct_group,
                                                    year,
                                                    date, 
                                                    direction,
                                                    hr_fct) %>%
                                           summarize(across(date_time,
                                                            min),
                                                     across(c(first,
                                                              second,
                                                              total),
                                                            sum),
                                                     .groups = "drop") %>%
                                           left_join(hrs_fct_grp %>%
                                                       enframe(name = "hour",
                                                               value = "hr_fct") %>%
                                                       mutate(across(hour,
                                                                     as.numeric)) %>%
                                                       group_by(hr_fct) %>%
                                                       summarize(across(hour,
                                                                        min)),
                                                     by = "hr_fct") %>%
                                           mutate(across(hour,
                                                         as.period,
                                                         unit = "hour"),
                                                  date_time = date + hour) %>%
                                           mutate(se = 0),
                                         
                                         small_sthd %>%
                                           filter(full_hour) %>%
                                           mutate(total = first + second) %>%
                                           group_by(year,
                                                    date, 
                                                    direction) %>%
                                           summarize(across(c(first,
                                                              second,
                                                              total),
                                                            sum),
                                                     .groups = "drop") %>%
                                           mutate(hr_fct = as.integer(1),
                                                  hour = as.period(0, unit = "hour"),
                                                  date_time = date + hour) %>%
                                           mutate(se = 0))) %>%
  unnest(full_sm_sthd) %>%
  mutate(across(direction,
                factor,
                levels = c("up",
                           "down",
                           "net"))) %>%
  # mutate(year = year(date),
  #        hour = hour(date_time),
  #        hr_fct = hrs_fct_grp[as.character(hour)],
  #        across(hour,
  #               as.period,
  #               unit = "hours")) %>%
  select(time_scale,
         direction,
         year,
         date,
         hour,
         hr_fct,
         everything()) %>%
  select(-fct_group) %>%
  mutate(across(sthd_length,
                replace_na,
                F)) %>%
  arrange(time_scale,
          direction,
          date_time) %>%
  add_column(est_type = "Full Hour",
             .before = "first")


# next predict small steelhead from those counted during half hour operations 
pred_small <- tibble(time_scale = as_factor(c('Hour',
                                              paste(24 / max(hrs_fct_grp), 'Hour Block'),
                                              'Day')),
                     pred_df = list(small_sthd %>%
                                      filter(!full_hour),
                                    
                                    small_sthd %>%
                                      filter(!full_hour) %>%
                                      mutate(fct_group = date + hours(hr_fct * 24 / max(hrs_fct_grp)),
                                             across(fct_group,
                                                    ~ factor(as.character(.),
                                                             labels = 1:n_distinct(.)))) %>%
                                      group_by(fct_group,
                                               year,
                                               date, 
                                               direction,
                                               hr_fct) %>%
                                      summarize(across(date_time,
                                                       min),
                                                across(c(first,
                                                         second),
                                                       sum),
                                                .groups = "drop") %>%
                                      left_join(hrs_fct_grp %>%
                                                  enframe(name = "hour",
                                                          value = "hr_fct") %>%
                                                  mutate(across(hour,
                                                                as.numeric)) %>%
                                                  group_by(hr_fct) %>%
                                                  summarize(across(hour,
                                                                   min)),
                                                by = "hr_fct") %>%
                                      mutate(across(hour,
                                                    as.period,
                                                    unit = "hour"),
                                             date_time = date + hour),
                                         
                                         small_sthd %>%
                                           filter(!full_hour) %>%
                                           group_by(year,
                                                    date, 
                                                    direction) %>%
                                           summarize(across(c(first,
                                                              second),
                                                            sum),
                                                     .groups = "drop") %>%
                                           mutate(hr_fct = as.integer(1),
                                                  hour = as.period(0, unit = "hour"),
                                                  date_time = date + hour))) %>%
  unnest(pred_df) %>%
  select(-fct_group) %>%
  mutate(across(sthd_length,
                replace_na,
                F)) %>%
  nest(pred_df = -c(time_scale,
                    direction)) %>%
  left_join(per_fit_df %>%
              select(time_scale,
                     direction,
                     mod),
            by = c("time_scale",
                   "direction")) %>%
  mutate(preds = map2(mod,
                      pred_df,
                      .f = function(x, y) {
                        predict(x,
                                newdata = y,
                                se.fit = T)
                      }),
         total = map(preds,
                   "fit"),
         se = map(preds,
                  "se.fit")) %>%
  select(-mod, -preds) %>%
  unnest(cols = c(pred_df, total, se)) %>%
  select(time_scale,
         direction,
         year,
         date,
         hour,
         hr_fct,
         everything()) %>%
  add_column(est_type = "Half Hour",
             .before = "first")

fish_small <- pred_small %>%
  bind_rows(calc_small) %>%
  arrange(time_scale,
          direction,
          date_time)

```

```{r combine-big-small}
fish_est <- fish_big %>%
  mutate(size = "big") %>%
  bind_rows(fish_small %>%
              select(-full_hour) %>%
              select(all_of(names(fish_big))) %>%
              mutate(size = "small")) %>%
  select(-sthd_length) %>%
  mutate(hr_fct = hrs_fct_grp[as.character(hour(date_time))]) %>%
  pivot_wider(names_from = size,
              values_from = c(first, second, total, se)) %>%
  mutate(across(c(starts_with("first"),
                  starts_with("total")),
                replace_na,
                0)) %>%
  mutate(second_big = if_else(est_type == "Full Hour" & 
                                is.na(second_big) &
                                !is.na(second_small),
                              as.integer(0),
                              second_big),
         second_small = if_else(est_type == "Full Hour" & 
                                  is.na(second_small) &
                                  !is.na(second_big),
                                as.integer(0),
                                second_small)) %>%
  rowwise() %>%
  mutate(total = total_big + total_small,
         se = sqrt(sum(c(se_big, se_small)^2, na.rm = T))) %>%
  ungroup()
```

```{r}
# deal with some mismatches between small/big fish (duplicate hour groups / days)
dup_est <- fish_est %>%
  unite(id,
        time_scale, date, hour, hr_fct, direction,
        # time_scale, direction, date_time,
        remove = F) %>%
  filter(id %in% id[duplicated(id)]) %>%
  arrange(time_scale,
          direction,
          date_time,
          est_type)

if(nrow(dup_est) > 0) {
  
  fish_est <- fish_est %>%
    anti_join(dup_est,
              by = c("time_scale",
                     "date",
                     "hour",
                     "hr_fct",
                     "direction")) %>%
    bind_rows(dup_est %>%
                group_by(time_scale,
                         direction,
                         year, 
                         date, 
                         hr_fct,
                         hour,
                         date_time) %>%
                summarize(across(c(first_big:total_small,
                                   total),
                                 sum,
                                 na.rm = T),
                          across(c(contains("se"),
                                   -contains("second")),
                                 ~ sqrt(sum(.^2, na.rm = T))),
                          .groups = "drop") %>%
                mutate(est_type = "Mixed") %>%
                select(any_of(names(dup_est)))) %>%
    arrange(time_scale,
            direction,
            date,
            hour,
            hr_fct)
}

```

```{r big-small-corr}
# are the estimates of big and small steelhead correlated?
cor_tab <- fish_est %>%
  group_by(time_scale,
           direction,
           year) %>%
  nest() %>%
  ungroup() %>%
  mutate(r = map_dbl(data,
                     .f = function(x) {
                       cor(x$total_big,
                           x$total_small,
                           use = "pairwise.complete.obs")
                     }),
         cor_test = map(data,
                        .f = function(x) {
                          cor.test(x$total_big,
                                   x$total_small)
                        }),
         p_value = map_dbl(cor_test,
                           "p.value"))
# cor_tab
```


```{r}
# expand estimates by percent sonar was operational in that period
fish_cnts <- fish_est %>%
  select(-contains("big"),
         -contains("small")) %>%
  full_join(ops_df %>%
              unnest(ops) %>%
              mutate(date_time = date + hour) %>%
              select(-ends_with("_pers"),
                     -n_hrs) %>%
              crossing(direction = unique(fish_est$direction))) %>%
  arrange(time_scale,
          direction,
          date_time) %>%
  mutate(across(total,
                replace_na,
                0)) %>%
  mutate(across(total,
                ~ . / op_perc),
         across(se,
                ~ if_else(op_perc > 0 & op_perc < 1,
                          . / op_perc,
                          .))) %>%
  mutate(est_type = if_else(is.na(est_type),
                            if_else(op_perc == 0,
                                    "Missing Data",
                                    if_else(op_perc == 1 & total == 0,
                                            "Zero Counts",
                                            "Partial Count")),
                            est_type)) %>%
  mutate(across(se,
                ~ if_else(is.na(.) & total == 0 & est_type == "Zero Counts" & op_perc == 1,
                          0, .)))
```

```{r}
# save for other analyses
save(fish_cnts, 
     fish_est,
     fish_big,
     fish_small,
     hrs_fct_grp,
     ops_df,
     file = here("analysis/data/derived_data",
                 "est_fish_cnts.rda"))
```


## Missing Data

There are periods when the sonar was not functioning, for a variety of reasons. Rather than ignore those time periods, and assume that no steelhead were passing then, we would prefer to impute net upstream fish for those missing values.

The first step is to expand the estimates of net upstream fish for periods when the sonar only partially operated (e.g. 14 hours out of a 24 hour day). We did this by dividing the estimate for that period by the percent of time the sonar was operational in that period. This assumes that fish are behaving similarly for that entire period. 

The next step is to deal with those periods when the sonar was not operating at all, where we have truly missing data. Table \@ref(tab:miss-data) shows how much data was missing for each year, depending on how the periods were constructed (e.g. hourly, hourly blocks, daily).

```{r create-time-series}
library(zoo)
library(forecast)
library(imputeTS)

ts_df <- fish_cnts %>%
  select(time_scale,
         direction,
         year,
         date_time,
         est_type,
         total, se) %>%
  group_by(time_scale,
           direction,
           year) %>%
  nest() %>%
  ungroup() %>%
  mutate(n_periods = map_dbl(data,
                             .f = function(x) {
                               nrow(x)
                             }),
         n_NA = map_dbl(data,
                        .f = function(x) {
                          sum(is.na(x$total))
                        }),
         perc_NA = n_NA / n_periods,
         ts_zoo = map(data,
                      .f = function(x) {
                        zoo(x$total,
                            x$date_time)
                      }),
         ts = map(ts_zoo,
                  as.ts)) %>%
  # fit some ARIMA models
  mutate(auto_arima = map(ts_zoo,
                          .f = auto.arima,
                          seasonal = F,
                          allowdrift = F,
                          ic = "aicc"),
         order = map_chr(auto_arima,
                         .f = function(x) {
                           arimaorder(x) %>%
                             as.vector() %>%
                             paste(collapse = " ")
                         }),
         sigma2 = map_dbl(auto_arima,
                          "sigma2"),
         se = sqrt(sigma2))

```

```{r impute-missing}
ts_df %<>%
  # make predictions based on best ARIMA model
  mutate(kalman_preds = map(ts_zoo,
                           .f = function(x) {
                             na_kalman(x,
                                       model = "auto.arima",
                                       smooth = T) %>%
                               as_tibble() %>%
                               rename(kalman_pred = value)
                             }),
         # predict based on linear interpolation
         lin_preds = map(ts_zoo,
                        .f = function(x) {
                          na_interpolation(x,
                                           option = "linear") %>%
                            as_tibble() %>%
                            rename(lin_pred = value)
                          }),
         # predict based on moving average
         ma_preds = map(ts_zoo,
                         .f = function(x) {
                           na_ma(x,
                                 k = 4,
                                 ) %>%
                             as_tibble() %>%
                             rename(ma_pred = value)
                         }))

# combine all predictions back with existing counts
all_preds <- fish_cnts %>%
  full_join(ts_df %>%
              select(time_scale,
                     direction,
                     year,
                     data,
                     pred_se = se,
                     ends_with("preds")) %>%
              unnest(cols = c(data, ends_with("preds"))) %>%
              mutate(across(contains("pred"),
                            ~ if_else(est_type != "Missing Data",
                                      NA_real_,
                                      .))) %>%
              filter(est_type == "Missing Data") %>%
              select(-total,
                     -se) %>%
              pivot_longer(ends_with("pred"),
                           names_to = "model",
                           values_to = "est") %>%
              mutate(across(model,
                            str_remove,
                            "_pred$")) %>%
              mutate(lci = qnorm(0.025, est, pred_se),
                     uci = qnorm(0.975, est, pred_se))) %>%
  select(time_scale:hour,
         hr_fct,
         date_time,
         everything()) %>%
  mutate(model = if_else(is.na(model),
                         if_else(est_type == "Half Hour",
                                 "expansion",
                                 "none"),
                         model)) %>%
  mutate(est = if_else(is.na(est),
                       total,
                       est),
         lci = if_else(is.na(lci),
                       qnorm(0.025, total, se),
                       lci),
         uci = if_else(is.na(uci),
                       qnorm(0.975, total, se),
                       uci),
         se = if_else(is.na(se),
                      pred_se,
                      se)) %>%
  select(time_scale:date_time,
         op_perc,
         est_type,
         total,
         model,
         est, se, lci, uci)

# all_preds %>%
#   filter(!model %in% c("lin",
#                       "ma")) %>%
#   group_by(time_scale,
#            year) %>%
#   summarize(est = sum(est),
#             se = sqrt(sum(se^2)),
#             lci = sum(lci),
#             uci = sum(uci)) %>%
#   add_column(model = "kalman",
#              .before = 1) %>%
#   mutate(lci_v2 = qnorm(0.025, est, se),
#          uci_v2 = qnorm(0.975, est, se))


```


```{r miss-data}
ts_df %>%
  select(time_scale, 
         year,
         n_periods:perc_NA) %>%
  distinct() %>%
  mutate(across(perc_NA,
                ~ . * 100)) %>%
  arrange(year,
          time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "n Periods",
                      "n NAs",
                      "% NA"),
        digits = c(rep(0, 4), 1),
        caption = "Table showing how many periods are in each group of data, and how many of those periods are NAs (missing values).") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```

To interpolate across those periods of missing data, we employed time-series models. Using the `forecast` package in R, we fit an ARIMA (auto-regressive integrated moving average) model, and let the `auto.arima` function determine the model with the best order (number of auto-regressive, moving average and difference steps) for each year and time-scale combination. We used the uncertainty from this model ($\sigma^2$) for all predictions. 

We examined several forms of interpolation across the missing data, including a Kalman filter, linear regression and moving average. The Kalman filter uses the ARIMA structure to estimate the missing data. A linear regression essentially draws a straight line from the data point prior to the first missing data and the data point after the last missing data point for each gap in the time series. A moving average approach uses two non-missing values prior to the missing data point, and two non-missing values after, weights them exponentially by their distance from the missing data point, and calculates the weighted mean. 

\newpage
# Results

## Expanding 30 min to 60 min

The expansion factor (i.e. slope) changes depending on the temporal scale that data is summarized on. Figure \@ref(fig:period-comp-fig) shows the various regressions, comparing them with the 1-1 line and the expected slope of two. None of the temporal scales produced a slope of two, but the longer the temporal scale the closer it got to that expected value. 

```{r period-comp-fig, fig.cap = "Scatter plots showing the net counts of fish moving upstream, using hours with both the first 30 minutes and second 30 minutes. The counts are summarized by hour, six hour blocks and entire day (24 hours) in the different facets. The dashed red line is has a slope of 2 (expected value), the dotted grey line has a slope of 1, and the blue line is the linear regression fit to that data, with 95% confidence intervals."}
per_fit_df %>%
  select(time_scale,
         direction,
         model_df) %>%
  unnest(model_df) %>%
  ggplot(aes(x = first,
             y = total)) +
  geom_abline(slope = 2,
              linetype = 2,
              color = "red") +
  geom_abline(slope = 1,
              linetype = 3,
              color = "gray30") +
  geom_smooth(method = "lm",
              formula = y ~ x - 1) +
  # geom_point() +
  geom_point(position = position_jitter(0.2, 0.2,
                                        seed = 4)) +
  facet_wrap(~ time_scale + direction,
             nrow = 3,
             ncol = 3,
             scales = "free") +
  labs(x = "Only first 30 minutes",
       y = "Entire hour")
```


Table \@ref(tab:lin-coef-tab) shows the summary of linear models fit to data summarized at various time scales. We summarized the estimated number of steelhead larger than 67 cm at the day scale (summing estimates at smaller temporal scales) and plotted the time-series in Figure \@ref(fig:ts-est) to show the differences caused by summarizing data at different time scales. 

```{r lin-coef-tab}
per_fit_df %>%
  mutate(across(ends_with("%"),
                round,
                2)) %>%
  mutate(CI = paste(`2.5 %`, `97.5 %`, sep = "-")) %>%
  select(time_scale,
         direction,
         slope,
         se,
         CI,
         adj_R2) %>%
  kable(col.names = c("Time Scale",
                      "Direction",
                      "Slope",
                      "SE",
                      "95% CI",
                      "R2"),
        digits = 2,
        booktabs = T,
        linesep = "",
        caption = "Results of fitting linear models with up, down and net upstream fish as the response and the number of fish counted in that direction in the first 30 minutes as the covariate with no intercept.") %>%
  kable_styling()

```

```{r k-fold-tab}
cv_tab <- cv_res %>%
  group_by(time_scale,
           direction) %>%
  summarize(across(-c(fold),
                   mean),
            .groups = "drop") %>%
  arrange(direction,
          abs(median_bias)) %>%
          # rmse) %>%
  select(-median_bias)

min_stats = cv_tab %>%
  pivot_longer(cols = -c(time_scale,
                         direction),
               names_to = "metric",
               values_to = "value") %>%
  select(-time_scale) %>%
  group_by(direction,
           metric) %>%
  filter(abs(value) == min(abs(value))) %>%
  pivot_wider(names_from = metric,
              values_from = value) %>%
  select(any_of(names(cv_tab)))

cv_tab %>%
  kbl(col.names = c("Time Scale",
                    "Direction",
                    "Avg. Bias",
                    "RMSE",
                    "MAE",
                    "MAPE"),
      digits = 2,
      escape = T,
      booktabs = T,
      linesep = "",
      caption = "Results of k-fold cross validation. Best result of each performance indicator is in bold.") %>%
  kable_styling() %>%
  row_spec(c(3, 6),
           hline_after = T) %>%
  column_spec(3,
              bold = if_else(1:nrow(cv_tab) %in% which(cv_tab$mean_bias %in% min_stats$mean_bias),
                                         T, F)) %>%
  column_spec(4,
              bold = if_else(1:nrow(cv_tab) %in% which(cv_tab$rmse %in% min_stats$rmse),
                                         T, F)) %>%
  column_spec(5,
              bold = if_else(1:nrow(cv_tab) %in% which(cv_tab$mae %in% min_stats$mae),
                                         T, F)) %>%
  column_spec(6,
              bold = if_else(1:nrow(cv_tab) %in% which(cv_tab$mape %in% min_stats$mape),
                                         T, F))

```


```{r big-est}
fish_big %>%
  group_by(time_scale,
           direction,
           year) %>%
  summarize(total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  pivot_wider(names_from = direction,
              values_from = c(total, se),
              names_glue = "{direction}_{.value}") %>%
  mutate(up_down_total = up_total - down_total,
         up_down_se = sqrt((down_se^2 + up_se^2)),
         diff = up_down_total - net_total) %>%
  select(time_scale, 
         year,
         starts_with("up_down"),
         starts_with("net"),
         diff) %>%
  arrange(year, time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "Up - Down Estimate",
                      "Up - Down SE",
                      "Net Estimate",
                      "Net SE",
                      "Difference"),
        digits = c(0, 0, 
                   0, 2,
                   0, 2,
                   0),
        caption = "Estimates of total net upstream fish larger than 67 cm, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```

```{r ts-est, fig.height = 7, fig.cap = "Time-series of estimates based on available data, using only extremely confident observations of fish greater than 67 cm, faceted by year and direction (upstream, downstream or net upstream). Colors correspond to which regression model was used to expand the 30 minutes observations. Any uncertainty shown is derived from the linear regression model."}
fish_big %>%
  full_join(ops_df %>%
              unnest(ops)) %>%
  filter(!is.na(direction)) %>%
  mutate(across(direction,
                factor,
                levels = c("up",
                           "down",
                           "net"))) %>%
  group_by(time_scale,
           direction,
           year,
           date) %>%
  summarize(avg_op = mean(op_perc),
            n_full_hr = sum(total > 0 & se == 0, na.rm = T),
            n_est = n(),
            n_nas = sum(is.na(total)),
            total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(across(c(total, se),
                ~ if_else(avg_op == 0,
                          NA_real_,
                          .))) %>%
  ggplot(aes(x = date,
             y = total,
             color = time_scale,
             fill = time_scale)) +
  geom_hline(yintercept = 0,
             linetype = 2,
             color = "gray40") +
  geom_ribbon(aes(ymin = qnorm(0.025, total, se),
                  ymax = qnorm(0.975, total, se)),
              alpha = 0.2,
              color = NA) +
  geom_line() +
  # scale_color_brewer(palette = "Set1",
  #                    name = "Model") +
  # scale_fill_brewer(palette = "Set1",
  #                    name = "Model") +
  scale_color_viridis_d(name = "Model",
                        end = 0.8) +
  scale_fill_viridis_d(name = "Model",
                        end = 0.8) +
  facet_grid(direction ~ year,
             scales = "free") +
  theme(legend.position = "bottom") +
  labs(x = "Date",
       y = "Fish")

```

```{r est-corr}
cor_tab <- fish_cnts %>%
  group_by(time_scale,
           direction,
           year,
           date) %>%
  summarize(n_full_hr = sum(is.na(se)),
            total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  select(-n_full_hr,
         -se) %>%
  pivot_wider(names_from = time_scale,
              values_from = total) %>%
  group_by(direction) %>%
  nest() %>%
  mutate(cor_tab = map(data,
                       .f = function(x) {
                         x %>%
                           select(-year,
                                  -date) %>%
                           corrr::correlate(use = "pairwise.complete.obs",
                                            method = "pearson",
                                            quiet = T)
                       })) %>%
  ungroup() %>%
  select(-data) %>%
  unnest(cor_tab)
  

```


## Excluding Bull Trout

<!-- Figure \@ref(fig:fig-glm) shows the fitted GLM that predicts the probability of being a steelhead based on a fish's length. Note that a 67 cm long fish would have a  round(predict(fl_mod, newdata = tibble(fl_z = (67 - unique(spp_fl$fl_mean)) / unique(spp_fl$fl_sd)), type = "response"), 3) * 100% of being a steelhead with this model. -->

```{r fig-glm, eval = F, fig.cap = "Points show the fork length of steelhead (along the top) and non-steelhead (along the bottom), with the fitted binomial GLM in red. The dashed line shows where fish greater than that would have a greater than 50% probability of being a steelhead. The dotted line shows the 67 cm threshold for which fish we will be applying this model to."}
spp_fl %>%
  mutate(spp = recode(spp,
                      "Resident RB" = "Bull Trout"),
         across(spp,
                fct_drop)) %>%
  mutate(spp_fct = as.numeric(spp) - 1) %>%
  ggplot(aes(fork_length,
             spp_fct)) +
  geom_point() +
  geom_vline(xintercept = cut_pt,
             linetype = 2) +
  geom_vline(xintercept = 67,
             linetype = 3) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "cauchit")),
  #             aes(color = "cauchit",
  #                 fill = "cauchit"),
  #             alpha = 0.2) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "probit")),
  #             aes(color = "probit",
  #                 fill = "probit"),
  #             alpha = 0.2) +
  # geom_smooth(method = glm,
  #             formula = y ~ x,
  #             method.args = list(family = binomial(link = "cloglog")),
  #             aes(color = "cloglog",
  #                 fill = "cloglog"),
  #             alpha = 0.2) +
  geom_smooth(method = glm,
              formula = y ~ x,
              method.args = list(family = binomial),
              aes(color = "logit",
                  fill = "logit"),
              alpha = 0.2) +
  scale_color_brewer(palette = "Set1",
                     name = "Link Fnc.") +
  scale_fill_brewer(palette = "Set1",
                    name = "Link Fnc.") +
  theme(legend.position = "none") +
  labs(y = "Probability of Being a Steelhead",
       x = "Fork Length (cm)")

```

Figure \@ref(fig:fig-gam) shows the fitted GAM that predicts the probability of being a steelhead based on a fish's length and date of capture. Note that the mean probability of being a steelhead for a 67 cm long fish, averaged across the entire season, would be `r round(mean(pred_tab$prob_sthd[pred_tab$length == 67]), 3) * 100`% of being a steelhead with this model. Also note that a fish 60 cm long would not be considered a steelhead if observed at the very beginning of the season or after the beginning of May, but would if observed in late March or April. 

```{r fig-gam, fig.cap = "The color depicts the probability of fish being a steelhead given the date of capture and fork length. Fish above the black line would have a greater than 50% probability of being a steelhead. The dotted line shows the 67 cm threshold for which fish we will be applying this model to."}
pred_tab %>%
  ggplot(aes(x = survey_date,
             y = length,
             fill = prob_sthd)) +
  geom_tile() +
  theme(panel.border = element_blank(),
        panel.grid = element_blank(),
        legend.position = "bottom") +
  scale_fill_viridis_c(name = "Probability of\nBeing a Steelhead") +
  geom_line(data = pred_tab %>%
              filter(prob_sthd >= p_pred) %>%
              group_by(survey_date) %>%
              filter(length == min(length)) %>%
              arrange(survey_date, length),
            color = "black") +
  geom_hline(yintercept = 67,
             linetype = 2,
             color = 'darkgray') +
  labs(x = "Date",
       y = "Fork Length (cm)")

```

Applying this model and rule-set to all the observed fish smaller than or equal to 67 cm, including the predictive model to expand 30 minute counts to full hour counts, a number of additional steelhead are added to our estimate each year (Table \@ref(tab:sm-est)). The total estimates (including all fish larger than 67 cm, as well as fish less than or equal to 67 cm that are predicted to be steelhead) are shown in Table \@ref(tab:tot-est). 

```{r sm-est}
fish_small %>%
  mutate(across(direction,
                factor,
                levels = c("up",
                           "down",
                           "net"))) %>%
  group_by(Direction = direction,
           time_scale, 
           year) %>%
  summarize(total = sum(total),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_val = paste0(round(total), " (",
                           round(se, 1),
                           ")")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = year,
              values_from = prnt_val) %>%
  rename(`Time Scale` = time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        digits = c(0, 0, 0, 2),
        caption = "Estimates (SE) of steelhead smaller than 67 cm, using only extremely confident observations, by direction of movement. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling()

```

```{r tot-est}
fish_est %>%
  group_by(time_scale,
           direction,
           year) %>%
  summarize(total = sum(total, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  pivot_wider(names_from = direction,
              values_from = c(total, se),
              names_glue = "{direction}_{.value}") %>%
  mutate(up_down_total = up_total - down_total,
         up_down_se = sqrt((down_se^2 + up_se^2)),
         diff = up_down_total - net_total) %>%
  select(time_scale, 
         year,
         starts_with("up_down"),
         starts_with("net"),
         diff) %>%
  arrange(year, time_scale) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "Up - Down Estimate",
                      "Up - Down SE",
                      "Net Estimate",
                      "Net SE",
                      "Difference"),
        digits = c(0, 0, 
                   0, 2,
                   0, 2,
                   0),
        caption = "Estimates of total steelhead, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)

```


## Missing Data

Figure \@ref(fig:oper-fig) shows the periods when the sonar array was not operating, and Figure \@ref(fig:miss-ts) shows how that impacts the time-series of fish counts. Note the large period in 2020 when the sonar was shut down due to COVID-19.

As the temporal scale on which counts are aggregated increases, the amount of missing data decreases. For example, if three hours are missing within a day, we can expand the rest of the day's counts by the percent of time the sonar was operational, so that day will not be "missing" at the day time-scale, although those three hours still are if we are operating on an hour time-scale. 

```{r oper-fig, fig.cap = "Purple depicts hours when the sonar was working, while yellow indicates the sonar was not functioning."}
hr_periods %>%
  mutate(hr = as.numeric(hour) / (60*60)) %>%
  mutate(date = as.Date(paste(month(date), mday(date)), format = "%m %d")) %>%
  ggplot(aes(x = hr,
             y = date,
             color = operational,
             fill = operational)) +
  geom_tile() +
  scale_fill_viridis_d(direction = -1,
                       name = "Sonar\nOperational") +
  scale_color_viridis_d(direction = -1,
                        name = "Sonar\nOperational") +
  facet_wrap(~ year,
             scales = "fixed") +
  scale_y_date(breaks = breaks_pretty(7)) +
  labs(y = "Date",
       x = "Hour") +
  theme(legend.position = "bottom")
```

```{r miss-ts, fig.height = 6, fig.cap = "Time series of net upstream fish in blue, with missing data highlighted in red."}
p_list = vector("list",
                length = nrow(ts_df))
for(i in seq_along(ts_df$time_scale)) {
  p_list[[i]] <- ggplot_na_distribution(ts_df$ts_zoo[[i]]) +
    labs(title = paste(str_to_title(ts_df$direction[i]),
                       ts_df$time_scale[i], "in", ts_df$year[i]),
         subtitle = element_blank()) +
    theme(axis.title.y = element_blank())
}

# ggarrange(plotlist = p_list[which(ts_df$direction == "up")],
# ggarrange(plotlist = p_list[which(ts_df$direction == "down")],
ggarrange(plotlist = p_list[which(ts_df$direction == "net")],
          ncol = 3,
          nrow = 3,
          common.legend = T,
          legend = "bottom")

```

After interpolating across the missing data, Table \@ref(tab:missing-tab) displays how many fish were added to each year's estimate, based on the temporal scale and the interpolation method. Table \@ref(tab:abund-est-missing) shows estimates of all steelhead, by year, direction and time-scale, split by what interpolation method was used. Table \@ref(tab:all-sthd-est) provides final estimates of total net upstream steelhead each year, including fish smaller than 67 cm and periods of missing data, split out by the temporal scale the data was summarized on and the interpolation model used for missing data. 

```{r missing-tab}
all_preds %>%
  filter(is.na(total)) %>%
  group_by(time_scale,
           direction,
           year,
           model) %>%
  summarize(pred_tot = sum(est, na.rm = T),
            pred_se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  mutate(prnt_value = paste0(round(pred_tot), " (", round(pred_se, 1), ")")) %>%
  select(-starts_with("pred_")) %>%
  pivot_wider(names_from = "model",
              values_from = prnt_value) %>%
  right_join(expand(all_preds,
                    time_scale, direction, year)) %>%
  arrange(year,
          direction,
          time_scale) %>%
  kable(col.names = c("Time Scale",
                      "Direction",
                      "Year",
                      "Kalman",
                      "Linear",
                      "MA"),
        booktabs = T,
        linesep = "",
        caption = "Estimates (SE) of how many steelhead by direction are added to the totals from periods with wholly missing data. Interpolation methods include the Kalman filter, linear model, and moving average. There are three estimates per year, corresponding to the three different temporal scales. Blank cells indicate no interpolation was necessary for that year / temporal scale combination.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```


```{r abund-est-missing}
all_preds %>%
  filter(!is.na(total)) %>%
  group_by(time_scale,
           direction,
           year) %>%
  summarize(obs_tot = sum(total, na.rm = T),
            obs_se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  full_join(all_preds %>%
              filter(is.na(total)) %>%
              group_by(time_scale,
                       direction,
                       year,
                       model) %>%
              summarize(pred_tot = sum(est, na.rm = T),
                        pred_se = sqrt(sum(se^2, na.rm = T)),
                        .groups = "drop")) %>%
  mutate(across(model,
                replace_na,
                "none"),
         across(starts_with("pred"),
                replace_na,
                0)) %>%
  mutate(total = obs_tot + pred_tot,
         se = sqrt(obs_se^2 + pred_se^2)) %>%
  mutate(None = paste0(round(obs_tot), " (", round(obs_se, 1), ")"),
         prnt_value = paste0(round(total), " (", round(se, 1), ")")) %>%
  select(-starts_with("obs"),
         -starts_with("pred")) %>%
  select(-total, -se) %>%
  pivot_wider(names_from = "model",
              values_from = prnt_value) %>%
  select(-none) %>%
  arrange(year,
          direction,
          time_scale) %>%
  kable(col.names = c("Time Scale",
                      "Direction",
                      "Year",
                      "None",
                      "Kalman",
                      "Linear",
                      "MA"),
        booktabs = T,
        linesep = "",
        caption = "Estimates (SE) of total steelhead by direction, using only extremely confident observations, and after interpolating counts for periods of missing data. Interpolation methods include no interpolation, the Kalman filter, linear model, and moving average. There are three estimates per year, corresponding to the three different temporal scales.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)

```

```{r all-sthd-est}
all_preds %>%
  filter(!model %in% c("lin", "ma")) %>%
  group_by(time_scale,
           direction,
           year) %>%
  summarize(total = sum(est, na.rm = T),
            se = sqrt(sum(se^2, na.rm = T)),
            .groups = "drop") %>%
  pivot_wider(names_from = direction,
              values_from = c(total, se),
              names_glue = "{direction}_{.value}") %>%
  mutate(ts_model = "Kalman") %>%
  bind_rows(all_preds %>%
              filter(!model %in% c("kalman", "ma")) %>%
              group_by(time_scale,
                       direction,
                       year) %>%
              summarize(total = sum(est, na.rm = T),
                        se = sqrt(sum(se^2, na.rm = T)),
                        .groups = "drop") %>%
              pivot_wider(names_from = direction,
                          values_from = c(total, se),
                          names_glue = "{direction}_{.value}") %>%
              mutate(ts_model = "Linear")) %>%
  bind_rows(all_preds %>%
              filter(!model %in% c("kalman", "lin")) %>%
              group_by(time_scale,
                       direction,
                       year) %>%
              summarize(total = sum(est, na.rm = T),
                        se = sqrt(sum(se^2, na.rm = T)),
                        .groups = "drop") %>%
              pivot_wider(names_from = direction,
                          values_from = c(total, se),
                          names_glue = "{direction}_{.value}") %>%
              mutate(ts_model = "MA")) %>%
  bind_rows(all_preds %>%
              filter(!is.na(total)) %>%
              group_by(time_scale,
                       direction,
                       year) %>%
              summarize(total = sum(est, na.rm = T),
                        se = sqrt(sum(se^2, na.rm = T)),
                        .groups = "drop") %>%
              pivot_wider(names_from = direction,
                          values_from = c(total, se),
                          names_glue = "{direction}_{.value}") %>%
              mutate(ts_model = "None")) %>%
  mutate(across(ts_model,
                factor,
                levels = c("None",
                           "Kalman",
                           "Linear",
                           "MA"))) %>%
  mutate(up_down_total = up_total - down_total,
         up_down_se = sqrt((down_se^2 + up_se^2)),
         diff = up_down_total - net_total) %>%
  select(time_scale, 
         year,
         ts_model,
         starts_with("up_down"),
         starts_with("net"),
         diff) %>%
  arrange(year, 
          time_scale,
          ts_model) %>%
  kable(booktabs = T,
        linesep = "",
        col.names = c("Time Scale",
                      "Year",
                      "Model",
                      "Up - Down Estimate",
                      "Up - Down SE",
                      "Net Estimate",
                      "Net SE",
                      "Difference"),
        digits = c(0, 0, 0,
                   0, 2,
                   0, 2,
                   0),
        caption = "Estimates of total net upstream fish, using only extremely confident observations. There are three estimates per year, corresponding to the three different regression models for expanding 30 minute observations.") %>%
  kable_styling() %>%
  row_spec(row = c(3, 6),
           hline_after = T)
```



\newpage
# Discussion Points

* What to do with rows where `data_recorded ` is "Partial"? This includes `r sum(sonar_raw$data_recorded  == "Partial")` rows, or `r round(sum(sonar_raw$data_recorded  == "Partial") / nrow(sonar_raw) * 100, 1)`% of the data. Exclude and treat as missing data? Or is there a better way to parse this? Currently I've filtered it out and treated it as missing.
* What should we do with observations with confidence of 2 or 3? They are currently excluded completely.

* What to do about kelts? There is currently one observation from the tangle netting that was identified as a kelt (caught on June 8, 2021). With enough data, we could build another GAM model to describe the probability of a steelhead moving downstream as being a kelt, perhaps based on Julian day. Unfortunately, we don't have nearly enough data at the moment. Alternatively, we could assign a date and assume all downstream fish before that date are ***not*** kelts, and all downstream fish observed after that date ***are*** kelts. Given the paucity of data, we may be relying on expert opinion to determine this date. 

## Expanding 30 min to 60 min

* The regression between counts in the first hour and the entire hour shows a consistent expectation that the counts in the second part of the hour will be less than counts in the first part. This holds regardless of whether we aggregate data by hour, day or something in between. 
* However, if we split the counts into upstream and downstream, the results show closer equality between the first and second half hours, especially if we aggregate counts into 6 hour blocks, or entire days.
  * I don't have a good explanation for why this occurs, but it seems worth utilizing (rather that using total net upstream regressions). 
  * This could provide more control over how we account for kelts in the downstream data as well, providing another justification for this approach.
  * One option is to assume the slope is 2, but use the standard error from these regressions to provide an estimate of uncertainty. Another is to use the estimated slope as well. Given that we have no expectation that the first half hour should have higher counts (in either direction) than the second, I'd be inclined to assume a slope of two. That approach will also mean that our estimates don't change as we gather additional data in the future.

* Aggregating date to the 6 hour block or the day seems to make the most sense. The estimates from either of these are consistently higher than aggregating to the hour scale (due to higher regression slopes). However, the choice between the two will have impacts on our estimates. 
  
## Excluding Bull Trout

* Should we assume any fish smaller than 40 cm is a bull trout, since that was the smallest steelhead length recorded? Under the current method, fish caught towards the end of March are considered a steelhead if they are 
at least `r filter(pred_tab, prob_sthd >= 0.5) %>% filter(length == min(length)) %>% pull(length) %>% round(1)` cm. This does increase the number of small steelhead we're estimating. 

* Since the last iteration, I realized I had forgotten to filter the species length data to exclude all hook and line samples, which I presume came from further upstream in the watershed. Excluding this data is meant to avoid any confounding with steelhead encounters and day of year. However, this does reduce the dataset used to fit the GAM. While the general shape of Figure \@ref(fig:fig-gam) makes biological sense, it does predict most fish will be predicted to be steelhead between March and April, almost regardless of size. This has increased the estimates of small steelhead, but perhaps to a more realistic number.

* The current model includes both the Julian day of capture and the fish length, using a spline for both covariates. The fish length spline turned out to be pretty close to a straight line, with larger fish being more likely to be a steelhead. The Julian day of capture had a peak probability of being a steelhead occurring in mid- to late-March, and tapering off on either side. Given there were a number of bull trout caught in the very beginning and towards the end of the sampling period, this shape makes sense. 
<!-- However, it should be noted that including Julian day of capture reduced the estimated number of small steelhead (less than 67 cm) in all years. It was a small reduction in 2020 and 2021, but a substantial one in 2019. This appears to be because there were a large number of small fish detected late in the run in 2019. Because they were so late, virtually none of those fish were predicted to be steelhead using this updated model. -->

* Incorporating another year of species composition data would make this species probability model more robust, especially for the spline related to the day of capture. 

## Missing Data

* Any of the interpolation models we tested (Kalman filter, linear regression or moving average) resulted in larger estimates of steelhead moving upstream (Table \@ref(tab:abund-est-missing)), but differed in which one provide the biggest increase depending on the time-scale and year.
* The uncertainty (e.g. standard error) grew when incorporating those missing data, which is appropriate. The uncertainty grew substantially in 2020, when there was a large period of missing data due to COVID restrictions.
* Depending on how big the missing data gaps are, and the time-scale we are aggregating data on, there were some years and time-scales with no missing data (e.g. 24 hour scale in 2021). The lack of missing data relies on using the percentage of hours when sonar was operational within each time-step to increase the estimates for any time-steps when the operational time was less than 100%.
  * The alternative to expanding time-steps when the sonar was partially operational is to remove all data from those time-steps and treat them as missing data. 
  * Or develop a threshold of operating percent (e.g. 20%?) below which we exclude that data because it's too small a percentage to feel comfortable expanding it, and instead rely on extrapolations from nearby time periods with full data.
  * It's unclear to me whether that would have a substantial impact on the overall estimates or uncertainty.
  * I would recommend the operating threshold approach, maybe using 20% as a threshold. This would mean that for time-periods (6 hour blocks or days) when the sonar was operating for less than 20%, we would mark the data from that time period as `NA` and treat it as missing. Other time periods when the sonar was operating for more than 20% we would take the estimates for that operating time and expand them by the operating percentage. So for example, if we estimated 30 upstream fish during a time period when the sonar was only operating 60% of the time, we would expand by dividing 30 by 60% and end up with an estimate of 50 upstream fish.  

# Decisions to be Made

* Use separate upstream/downstream regressions, or net upstream?
  * *Recommend:* separate upstream/downstream regressions
* What time-scale shall we aggregate data on?
  * *Recommend:* Day
* Should we use estimated slope for expanding 30 min counts, or assume a slope of 2?
  * *Recommend:* Assume a slope of 2
* Should we impose a minimum length to consider a fish a steelhead?
  * *Recommend:* Yes. The smallest observed steelhead length was `r min(spp_fl$fork_length[spp_fl$spp == "Steelhead"])` cm. I propose we use that as a cut off to override the GAM predictions for small fish, ensuring that every fish smaller than this threshold is considered a non-steelhead.
* What to do with data with confidence 2 or 3?
  * *Recommend:* delete / drop it
* What time-series interpolation model should we use?
  * *Recommend:* Kalman filter
* What to do with time-periods when sonar only partially operated?
  * *Recommend:* If sonar operated for less than a threshold (e.g. 20%), treat that data as missing and use time-series interpolation model. For operational percentages greater than this threshold, expand the estimates by that percentage. 
* Include 2022 data?
  * *Recommend:* Yes. Write up a report that covers 4 years of data: 2019-2022. 
* How to account for kelts?
  * *Recommend:* **???**
